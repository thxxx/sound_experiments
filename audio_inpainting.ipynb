{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bbe7497-eb30-4101-ba66-f39b40a88907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.1.0+cu121 with CUDA 1201 (you have 2.1.0+cu118)\n",
      "    Python  3.10.13 (you have 3.10.12)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import sys\n",
    "import copy\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from audiotools import AudioSignal\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import wandb\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from transformers import get_scheduler\n",
    "\n",
    "from beats.BEATs import BEATsConfig, BEATs\n",
    "\n",
    "from config import Config\n",
    "from audiomodel_inpainting import AudioProcessing\n",
    "from audiocraft.modules.conditioners import JointEmbedCondition, SegmentWithAttributes, WavCondition, ConditioningAttributes\n",
    "\n",
    "def make_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def wandb_init(cfg):\n",
    "    wandb.init(\n",
    "            # set the wandb project where this run will be logged\n",
    "            project=cfg.wandb_project_name,\n",
    "            \n",
    "            # track hyperparameters and run metadata\n",
    "            config={\n",
    "            \"learning_rate\": cfg.learning_rate,\n",
    "            \"epochs\": cfg.num_train_epochs,\n",
    "            \"batch_size\": cfg.batch_size,\n",
    "            }\n",
    "    )\n",
    "    \n",
    "def save_checkpoint(cfg, model, result, best_loss, epoch=0):\n",
    "    save_checkpoint = False\n",
    "    with open(\"{}/summary.jsonl\".format(cfg.output_dir), \"a\") as f:\n",
    "        f.write(json.dumps(result) + \"\\n\\n\")\n",
    "        \n",
    "    if result[\"train_loss\"] < best_loss:\n",
    "      best_loss = result[\"train_loss\"]\n",
    "      save_checkpoint = True\n",
    "      \n",
    "    # 모델 상태 저장\n",
    "    if save_checkpoint and cfg.checkpointing_steps == \"best\":\n",
    "        torch.save(model.state_dict(), os.path.join(cfg.output_dir, \"best.pth\"))\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(cfg.output_dir, \"last.pth\"))\n",
    "    torch.save(model.state_dict(), os.path.join(cfg.output_dir, f\"epoch_{epoch}.pth\"))\n",
    "\n",
    "    return best_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04b688a0-b9d8-49f4-8aeb-f8393c443473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(cfg):\n",
    "        from audiocraft.models.loaders import load_compression_model, load_lm_model\n",
    "        \"\"\"Instantiate models and optimizer.\"\"\"     \n",
    "        compression_model = load_compression_model('facebook/audiogen-medium', device=cfg.device)\n",
    "        lm = load_lm_model('facebook/audiogen-medium', custom_cfg=cfg)\n",
    "        return compression_model, lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "136ecf4f-0a20-4c54-b085-8a08ab263244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_tokenizer(wav, compression_model):\n",
    "        with torch.no_grad():\n",
    "            audio_tokens, scale = compression_model.encode(wav)\n",
    "        return audio_tokens\n",
    "\n",
    "def post_process_audio_tokenizer(audio_tokens, audio_lengths=None, compression_model=None, lm=None, cfg=None):\n",
    "    padding_mask = torch.ones_like(audio_tokens, dtype=torch.bool, device=audio_tokens.device)\n",
    "    audio_tokens = audio_tokens.clone()\n",
    "    padding_mask = padding_mask.clone()\n",
    "    token_sample_rate = compression_model.frame_rate\n",
    "    B, K, T_s = audio_tokens.shape\n",
    "    \n",
    "    for i in range(B):\n",
    "        valid_tokens = math.floor(audio_lengths[i] / cfg.sample_rate * token_sample_rate)\n",
    "        audio_tokens[i, :, valid_tokens:] = lm.special_token_id\n",
    "        padding_mask[i, :, valid_tokens:] = 0\n",
    "\n",
    "    return audio_tokens, padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e57ba89f-a582-4ffe-b742-04a5bf8b6221",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, cfg):\n",
    "        \n",
    "        self.target_sample_rate = cfg.sample_rate\n",
    "        self.duration = cfg.duration\n",
    "        self.device = cfg.device\n",
    "        self.audio_paths = cfg.eval_data_path\n",
    "\n",
    "        self.df = pd.read_csv(self.audio_paths)[:20]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = self.df.iloc[idx] #self.audio_files_list[idx]\n",
    "        \n",
    "        import random\n",
    "        audio_path = data['audio_path']\n",
    "        total_duration = data['duration']\n",
    "        description = \"<RANDOM>\"\n",
    "        \n",
    "        # Set duration\n",
    "        duration = self.duration if total_duration >= 3 else total_duration  # Duration is 3 seconds or total_duration if less than 3\n",
    "        \n",
    "        offset = 0.0   \n",
    "        # Load audio signal file\n",
    "        wav = AudioSignal(audio_path, offset=offset, duration=duration)\n",
    "        length = wav.signal_length\n",
    "\n",
    "        # Encode audio signal as one long file\n",
    "        wav.to_mono()\n",
    "        wav.resample(self.target_sample_rate)\n",
    "\n",
    "        if wav.duration < self.duration:\n",
    "          pad_len = int(self.duration * self.target_sample_rate) - wav.signal_length\n",
    "          wav.zero_pad(0, pad_len)\n",
    "        elif wav.duration > self.duration:\n",
    "          wav.truncate_samples(self.duration * self.target_sample_rate)\n",
    "\n",
    "\n",
    "        return wav.audio_data.squeeze(1), description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbd10db4-062f-4456-9051-0e6233ea25cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, cfg, train=True):\n",
    "        self.train = train\n",
    "        \n",
    "        self.target_sample_rate = cfg.sample_rate\n",
    "        self.duration = cfg.duration\n",
    "        self.device = cfg.device\n",
    "\n",
    "        if self.train:\n",
    "            self.audio_paths = cfg.train_data_path\n",
    "        else:\n",
    "            self.audio_paths = cfg.eval_data_path\n",
    "\n",
    "        self.df = pd.read_csv(self.audio_paths)[:100]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        data = self.df.iloc[idx] #self.audio_files_list[idx]\n",
    "        \n",
    "        import random\n",
    "        audio_path = data['audio_path']\n",
    "        total_duration = data['duration']\n",
    "        description = \"<RANDOM>\"\n",
    "        \n",
    "        # Set duration\n",
    "        duration = self.duration if total_duration >= 3 else total_duration  # Duration is 3 seconds or total_duration if less than 3\n",
    "        \n",
    "        # Set offset based on conditions\n",
    "        if total_duration < self.duration or self.train == False:\n",
    "            offset = 0.0 \n",
    "        else:\n",
    "            max_offset = total_duration - duration  # Calculate the maximum possible offset\n",
    "            offset = random.uniform(0, max_offset)  # Choose a random offset within the possible range\n",
    "        \n",
    "        # Load audio signal file\n",
    "        wav = AudioSignal(audio_path, offset=offset, duration=duration)\n",
    "        length = wav.signal_length\n",
    "\n",
    "        # Encode audio signal as one long file\n",
    "        wav.to_mono()\n",
    "        wav.resample(self.target_sample_rate)\n",
    "\n",
    "        if wav.duration < self.duration:\n",
    "          pad_len = int(self.duration * self.target_sample_rate) - wav.signal_length\n",
    "          wav.zero_pad(0, pad_len)\n",
    "        elif wav.duration > self.duration:\n",
    "          wav.truncate_samples(self.duration * self.target_sample_rate)\n",
    "\n",
    "        return wav.audio_data.squeeze(1), description, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "53b4d4e7-ab6e-4ab4-b179-26df95bb19bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SoundConditioner 클래스 정의\n",
    "class SoundConditionerEncodec(nn.Module):\n",
    "    def __init__(self, compression_model):\n",
    "        super(SoundConditionerEncodec, self).__init__()\n",
    "\n",
    "        # 비트 모델 로드\n",
    "       \n",
    "        self.device = cfg.device\n",
    "        self.compression_model = compression_model\n",
    "\n",
    "    def forward(self, wav):\n",
    "        # 오디오 토큰 길이 설정\n",
    "        emb = self.compression_model.encoder(wav)\n",
    "\n",
    "        return emb.permute(0,2,1), None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c68000-8dfc-4053-9313-e02e28222bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SoundConditioner 클래스 정의\n",
    "class SoundConditioner(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(SoundConditioner, self).__init__()\n",
    "\n",
    "         beats_ckpt = \"beats/weights.pt\"\n",
    "        self.device = cfg.device\n",
    "        self.beats_model = self.load_beats(beats_ckpt)\n",
    "        self.beats_model.eval()\n",
    "\n",
    "    def forward(self, wav):\n",
    "        # 오디오 토큰 길이 설정\n",
    "        audio_token_length = 180\n",
    "        # 오디오 임베딩 처리\n",
    "        audio_embeds = self.process_audio_embedding(wav.squeeze(1).to(self.device), self.beats_model, audio_token_length, self.device)\n",
    "        return audio_embeds, None\n",
    "\n",
    "    def load_beats(self, beats_ckpt):\n",
    "        beats_checkpoint = torch.load(beats_ckpt, map_location='cpu')\n",
    "        beats_cfg = BEATsConfig(beats_checkpoint['cfg'])\n",
    "        beats = BEATs(beats_cfg)\n",
    "        beats.load_state_dict(beats_checkpoint['model'])\n",
    "        for name, param in beats.named_parameters():\n",
    "            param.requires_grad = False\n",
    "        return beats\n",
    "\n",
    "    def process_audio_embedding(self, wav, beats, audio_token_length, device):\n",
    "        # 오디오 패딩 마스크 생성\n",
    "        audio_padding_mask = torch.zeros(wav.shape, device=wav.device).bool()\n",
    "    \n",
    "        # 오디오 특징 추출\n",
    "        audio_embeds, _ = beats.extract_features(wav, padding_mask=audio_padding_mask, feature_only=True)\n",
    "    \n",
    "        # 현재 길이 확인\n",
    "        current_length = audio_embeds.size(1)\n",
    "    \n",
    "        if current_length > audio_token_length:\n",
    "            # 오디오 임베딩 자르기\n",
    "            audio_embeds = audio_embeds.narrow(1, 0, audio_token_length)\n",
    "        elif current_length < audio_token_length:\n",
    "            # 필요한 패딩 길이 계산 및 적용\n",
    "            padding_length = audio_token_length - current_length\n",
    "            audio_embeds = F.pad(audio_embeds, (0, 0, 0, padding_length))\n",
    "    \n",
    "        return audio_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e63adc15-23f3-4385-8dfd-617ca0179eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    cfg = Config()\n",
    "    accelerator = Accelerator(gradient_accumulation_steps=cfg.gradient_accumulation_steps)\n",
    "    device = accelerator.device\n",
    "    cfg.update(device=accelerator.device)\n",
    "    make_dir(cfg.output_dir)\n",
    "    make_dir(cfg.generated_dir)\n",
    "    \n",
    "    base_path = \"./csv_files/\"\n",
    "    train_data_path = f\"{base_path}/eval_epidemic_dataset.csv\"\n",
    "    eval_data_path = f\"{base_path}/eval_epidemic_dataset.csv\"\n",
    "    cfg.update(train_data_path=train_data_path, eval_data_path=eval_data_path)\n",
    "    \n",
    "    # 'sound'를 'cross' 키의 리스트에 추가\n",
    "    cfg.fuser['cross'].append('sound')\n",
    "    if accelerator.is_main_process: \n",
    "        wandb_init(cfg)\n",
    "    \n",
    "    with accelerator.main_process_first():  \n",
    "        compression_model, lm = build_model(cfg)\n",
    "        model = AudioProcessing(cfg, lm)  \n",
    "        t5conditioner = copy.deepcopy(lm.condition_provider.conditioners.description)\n",
    "        soundconditioner = SoundConditioner(cfg)\n",
    "        audio_dataset = AudioDataset(cfg, train=True) \n",
    "        eval_dataset = AudioDataset(cfg, train=False)\n",
    "    test_dataset = TestDataset(cfg)\n",
    "    \n",
    "    audio_dataloader = DataLoader(audio_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=8)\n",
    "    eval_dataloader = DataLoader(eval_dataset, batch_size=cfg.eval_batch_size, shuffle=False, num_workers=4)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "        \n",
    "    optimizer_parameters = [param for param in model.parameters() if param.requires_grad]\n",
    "        \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        optimizer_parameters, lr=cfg.learning_rate,\n",
    "        betas=(cfg.adam_beta1, cfg.adam_beta2),\n",
    "        weight_decay=cfg.adam_weight_decay,\n",
    "        eps=cfg.adam_epsilon,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    num_update_steps_per_epoch = math.ceil(len(audio_dataloader) / cfg.gradient_accumulation_steps)\n",
    "    if cfg.max_train_steps is None:\n",
    "      cfg.max_train_steps = cfg.num_train_epochs * num_update_steps_per_epoch\n",
    "    \n",
    "    lr_scheduler = get_scheduler(\n",
    "          name=cfg.lr_scheduler_type,\n",
    "          optimizer=optimizer,\n",
    "          num_warmup_steps=cfg.num_warmup_steps * cfg.gradient_accumulation_steps,\n",
    "          num_training_steps=cfg.max_train_steps * cfg.gradient_accumulation_steps,\n",
    "      )\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        if cfg.resume_from_checkpoint is not None:\n",
    "            accelerator.print(f\"Resumed from local checkpoint: {cfg.resume_from_checkpoint}\")\n",
    "            model.load_state_dict(torch.load(cfg.resume_from_checkpoint, map_location=accelerator.device))\n",
    "            #accelerator.load_state(cfg.resume_from_checkpoint)\n",
    "\n",
    "    audio_dataloader, eval_dataloader, model, compression_model, t5conditioner, soundconditioner, optimizer, lr_scheduler = accelerator.prepare(\n",
    "audio_dataloader, eval_dataloader, model, compression_model, t5conditioner, soundconditioner, optimizer, lr_scheduler\n",
    ")\n",
    "\n",
    "    starting_epoch, completed_steps, best_loss, save_epoch = 0, 0, np.inf, 0\n",
    "    progress_bar = tqdm(range(cfg.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "\n",
    "    \n",
    "    for epoch in range(starting_epoch, cfg.num_train_epochs):\n",
    "        accelerator.print(f\"-------------------EPOCH{epoch}-------------------------\" )\n",
    "        total_loss, total_val_loss = 0, 0\n",
    "        model.eval()\n",
    "        for batch_idx, (wav, descriptions, lengths) in enumerate(audio_dataloader):\n",
    "            with accelerator.accumulate(model):\n",
    "                with torch.no_grad():\n",
    "                    unwrapped_textconditioner = accelerator.unwrap_model(t5conditioner)\n",
    "                    unwrapped_soundconditioner = accelerator.unwrap_model(soundconditioner)\n",
    "                    \n",
    "                    tokenized = {}\n",
    "                    tokenized[\"description\"] =  unwrapped_textconditioner.tokenize(descriptions)\n",
    "                    tokenized[\"sound\"] = wav\n",
    "                    \n",
    "                    # conditioning\n",
    "                    output = {}\n",
    "                    for attribute, inputs in tokenized.items():\n",
    "                        if attribute == \"description\":   \n",
    "                            condition, mask = unwrapped_textconditioner(inputs)\n",
    "                        elif attribute == \"sound\":\n",
    "                            condition, mask = unwrapped_soundconditioner(inputs)\n",
    "                        output[attribute] = (condition, mask)\n",
    "\n",
    "                    unwrapped_vae = accelerator.unwrap_model(compression_model)\n",
    "                    audio_tokens = process_audio_tokenizer(wav, unwrapped_vae)\n",
    "                    audio_tokens, padding_mask = post_process_audio_tokenizer(audio_tokens, lengths, unwrapped_vae, lm, cfg) \n",
    "\n",
    "                loss = model(audio_tokens, padding_mask, attributes=None, condition_tensors=output)\n",
    "                ppl =  torch.exp(loss)\n",
    "                total_loss += loss.detach().float()\n",
    "                accelerator.backward(loss)     \n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if accelerator.sync_gradients:\n",
    "                    progress_bar.update(1)\n",
    "                    completed_steps += 1\n",
    "        \n",
    "        model.eval()\n",
    "        for batch_idx, (wav, descriptions, lengths) in enumerate(eval_dataloader):\n",
    "            with accelerator.accumulate(model):\n",
    "                with torch.no_grad():\n",
    "                    unwrapped_textconditioner = accelerator.unwrap_model(t5conditioner)\n",
    "                    unwrapped_soundconditioner = accelerator.unwrap_model(soundconditioner)\n",
    "                    \n",
    "                    tokenized = {}\n",
    "                    tokenized[\"description\"] =  unwrapped_textconditioner.tokenize(descriptions)\n",
    "                    tokenized[\"sound\"] = wav\n",
    "                    \n",
    "                    # conditioning\n",
    "                    output = {}\n",
    "                    for attribute, inputs in tokenized.items():\n",
    "                        if attribute == \"description\":   \n",
    "                            condition, mask = unwrapped_textconditioner(inputs)\n",
    "                        elif attribute == \"sound\":\n",
    "                            condition, mask = unwrapped_soundconditioner(inputs)\n",
    "                        output[attribute] = (condition, mask)\n",
    "\n",
    "                    unwrapped_vae = accelerator.unwrap_model(compression_model)\n",
    "                    audio_tokens = process_audio_tokenizer(wav, unwrapped_vae)\n",
    "                    audio_tokens, padding_mask = post_process_audio_tokenizer(audio_tokens, lengths, unwrapped_vae, lm, cfg) \n",
    "\n",
    "                    loss = model(audio_tokens, padding_mask, attributes=None, condition_tensors=output)\n",
    "                    total_val_loss += loss  \n",
    "    \n",
    "        if accelerator.is_main_process:         \n",
    "            result = {}\n",
    "            result[\"epoch\"] = save_epoch + 1,\n",
    "            result[\"step\"] = completed_steps\n",
    "            result[\"train_loss\"] = round(total_loss.item()/cfg.save_steps, 4)\n",
    "            result[\"valid_loss\"] = round(total_val_loss.item()/len(eval_dataloader), 4)\n",
    "            \n",
    "            wandb.log(result)\n",
    "            result_string = \"Epoch: {}, Loss Train: {}, Valid: {}\\n\".format(save_epoch + 1, result[\"train_loss\"], result[\"valid_loss\"])    \n",
    "            accelerator.print(result_string) \n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_vae = accelerator.unwrap_model(compression_model)\n",
    "            unwrapped_soundconditioner = accelerator.unwrap_model(soundconditioner)\n",
    "            best_loss = save_checkpoint(cfg, unwrapped_model, result, best_loss, save_epoch)\n",
    "            for test_step, (wav, descriptions) in enumerate(test_dataloader):\n",
    "                audio_conditions = unwrapped_soundconditioner(wav)\n",
    "                gen_token, gen_audio = unwrapped_model.inference(descriptions, audio_conditions, unwrapped_vae)\n",
    "                audio_filename = f\"epoch_{save_epoch}_{test_step}.wav\"\n",
    "                unwrapped_model.save_audio(gen_audio, audio_filename, cfg)\n",
    "            save_epoch += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a0eeb7c-6caf-422b-8e86-54f2fe4b5c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moptimizerai\u001b[0m (\u001b[33moptimizer_ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/audiogen-finetune/wandb/run-20240116_232313-pr6l2bnu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1/runs/pr6l2bnu' target=\"_blank\">whole-cherry-250</a></strong> to <a href='https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1' target=\"_blank\">https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1/runs/pr6l2bnu' target=\"_blank\">https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1/runs/pr6l2bnu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUSER:  {'cross_attention_pos_emb': False, 'cross_attention_pos_emb_scale': 1, 'sum': [], 'prepend': [], 'cross': ['description', 'sound'], 'input_interpolate': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "  0%|          | 0/50000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------EPOCH0-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 50/50000 [00:31<8:30:43,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss Train: 6.198, Valid: 2.3788\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/audiotools/core/audio_signal.py:601: UserWarning: Audio amplitude > 1 clipped when saving\n",
      "  warnings.warn(\"Audio amplitude > 1 clipped when saving\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------EPOCH1-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 100/50000 [03:14<8:26:20,  1.64it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss Train: 5.5799, Valid: 2.3259\n",
      "\n",
      "-------------------EPOCH2-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 150/50000 [05:48<8:23:57,  1.65it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss Train: 5.5875, Valid: 2.2814\n",
      "\n",
      "-------------------EPOCH3-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 200/50000 [08:18<8:20:27,  1.66it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss Train: 5.3535, Valid: 2.2375\n",
      "\n",
      "-------------------EPOCH4-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 250/50000 [11:00<8:34:01,  1.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss Train: 5.2808, Valid: 2.1868\n",
      "\n",
      "-------------------EPOCH5-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 300/50000 [13:33<8:24:00,  1.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Loss Train: 5.288, Valid: 2.1317\n",
      "\n",
      "-------------------EPOCH6-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 350/50000 [16:00<8:23:46,  1.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Loss Train: 5.1227, Valid: 2.0706\n",
      "\n",
      "-------------------EPOCH7-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 400/50000 [18:38<8:19:38,  1.65it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Loss Train: 4.9106, Valid: 2.0166\n",
      "\n",
      "-------------------EPOCH8-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 450/50000 [21:16<8:23:38,  1.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Loss Train: 4.7773, Valid: 1.9531\n",
      "\n",
      "-------------------EPOCH9-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 500/50000 [23:50<8:28:02,  1.62it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss Train: 4.7438, Valid: 1.8975\n",
      "\n",
      "-------------------EPOCH10-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 550/50000 [26:27<8:45:40,  1.57it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Loss Train: 4.4945, Valid: 1.8513\n",
      "\n",
      "-------------------EPOCH11-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 600/50000 [29:06<8:35:41,  1.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Loss Train: 4.3462, Valid: 1.8046\n",
      "\n",
      "-------------------EPOCH12-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 650/50000 [31:46<8:31:28,  1.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Loss Train: 4.1591, Valid: 1.7687\n",
      "\n",
      "-------------------EPOCH13-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 700/50000 [34:26<8:36:02,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Loss Train: 4.1328, Valid: 1.7437\n",
      "\n",
      "-------------------EPOCH14-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 750/50000 [37:06<8:40:56,  1.58it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Loss Train: 4.2902, Valid: 1.7124\n",
      "\n",
      "-------------------EPOCH15-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 800/50000 [39:43<8:35:37,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Loss Train: 3.9247, Valid: 1.6881\n",
      "\n",
      "-------------------EPOCH16-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 850/50000 [42:17<8:40:49,  1.57it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Loss Train: 4.0537, Valid: 1.6716\n",
      "\n",
      "-------------------EPOCH17-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 900/50000 [44:48<8:17:38,  1.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Loss Train: 3.8975, Valid: 1.6594\n",
      "\n",
      "-------------------EPOCH18-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 950/50000 [47:20<8:12:24,  1.66it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Loss Train: 3.9747, Valid: 1.6542\n",
      "\n",
      "-------------------EPOCH19-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1000/50000 [49:43<8:15:19,  1.65it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Loss Train: 3.9652, Valid: 1.6298\n",
      "\n",
      "-------------------EPOCH20-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1050/50000 [52:09<8:28:57,  1.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Loss Train: 3.8466, Valid: 1.6174\n",
      "\n",
      "-------------------EPOCH21-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1100/50000 [54:41<8:12:59,  1.65it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Loss Train: 3.9264, Valid: 1.6105\n",
      "\n",
      "-------------------EPOCH22-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1150/50000 [57:11<8:22:15,  1.62it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Loss Train: 4.0448, Valid: 1.6017\n",
      "\n",
      "-------------------EPOCH23-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1200/50000 [59:38<8:16:36,  1.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Loss Train: 3.8256, Valid: 1.5957\n",
      "\n",
      "-------------------EPOCH24-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 1250/50000 [1:02:19<8:34:38,  1.58it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Loss Train: 3.7045, Valid: 1.5889\n",
      "\n",
      "-------------------EPOCH25-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1300/50000 [1:04:58<8:29:31,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Loss Train: 3.5998, Valid: 1.5842\n",
      "\n",
      "-------------------EPOCH26-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1350/50000 [1:07:35<8:31:20,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Loss Train: 3.7249, Valid: 1.583\n",
      "\n",
      "-------------------EPOCH27-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1400/50000 [1:10:14<8:28:07,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Loss Train: 4.104, Valid: 1.5795\n",
      "\n",
      "-------------------EPOCH28-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1450/50000 [1:12:40<8:28:01,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Loss Train: 3.7691, Valid: 1.572\n",
      "\n",
      "-------------------EPOCH29-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1500/50000 [1:15:10<8:20:39,  1.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Loss Train: 3.3003, Valid: 1.5677\n",
      "\n",
      "-------------------EPOCH30-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1550/50000 [1:17:49<8:23:21,  1.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31, Loss Train: 3.8439, Valid: 1.5669\n",
      "\n",
      "-------------------EPOCH31-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1600/50000 [1:20:23<8:28:44,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32, Loss Train: 3.6988, Valid: 1.5645\n",
      "\n",
      "-------------------EPOCH32-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1650/50000 [1:22:52<8:23:17,  1.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33, Loss Train: 3.6993, Valid: 1.5609\n",
      "\n",
      "-------------------EPOCH33-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1700/50000 [1:25:20<8:22:56,  1.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34, Loss Train: 3.8863, Valid: 1.5582\n",
      "\n",
      "-------------------EPOCH34-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 1750/50000 [1:27:48<8:09:10,  1.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35, Loss Train: 3.8211, Valid: 1.5567\n",
      "\n",
      "-------------------EPOCH35-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 1800/50000 [1:30:15<8:13:01,  1.63it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36, Loss Train: 3.8264, Valid: 1.5563\n",
      "\n",
      "-------------------EPOCH36-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 1850/50000 [1:32:42<8:08:20,  1.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37, Loss Train: 3.6404, Valid: 1.5512\n",
      "\n",
      "-------------------EPOCH37-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1900/50000 [1:35:08<8:08:17,  1.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38, Loss Train: 3.7977, Valid: 1.5466\n",
      "\n",
      "-------------------EPOCH38-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1950/50000 [1:37:32<8:06:31,  1.65it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39, Loss Train: 3.5955, Valid: 1.5459\n",
      "\n",
      "-------------------EPOCH39-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2000/50000 [1:39:58<8:04:05,  1.65it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40, Loss Train: 3.6488, Valid: 1.543\n",
      "\n",
      "-------------------EPOCH40-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2050/50000 [1:42:23<8:06:30,  1.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 41, Loss Train: 3.7306, Valid: 1.5413\n",
      "\n",
      "-------------------EPOCH41-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2100/50000 [1:44:50<8:20:58,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 42, Loss Train: 3.7485, Valid: 1.5383\n",
      "\n",
      "-------------------EPOCH42-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2150/50000 [1:47:18<8:21:31,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43, Loss Train: 3.4957, Valid: 1.5366\n",
      "\n",
      "-------------------EPOCH43-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2200/50000 [1:49:44<8:05:42,  1.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 44, Loss Train: 3.7325, Valid: 1.5355\n",
      "\n",
      "-------------------EPOCH44-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2250/50000 [1:52:11<8:07:40,  1.63it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45, Loss Train: 3.7423, Valid: 1.5325\n",
      "\n",
      "-------------------EPOCH45-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2300/50000 [1:54:38<7:59:47,  1.66it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 46, Loss Train: 3.5762, Valid: 1.5342\n",
      "\n",
      "-------------------EPOCH46-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2350/50000 [1:57:05<7:59:31,  1.66it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47, Loss Train: 3.6612, Valid: 1.5291\n",
      "\n",
      "-------------------EPOCH47-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2400/50000 [1:59:30<8:01:36,  1.65it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 48, Loss Train: 3.5878, Valid: 1.5289\n",
      "\n",
      "-------------------EPOCH48-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2450/50000 [2:01:56<8:00:30,  1.65it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49, Loss Train: 3.8204, Valid: 1.5253\n",
      "\n",
      "-------------------EPOCH49-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2500/50000 [2:04:25<7:58:30,  1.65it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50, Loss Train: 3.544, Valid: 1.5238\n",
      "\n",
      "-------------------EPOCH50-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2550/50000 [2:06:54<8:06:08,  1.63it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 51, Loss Train: 3.6089, Valid: 1.5207\n",
      "\n",
      "-------------------EPOCH51-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2600/50000 [2:09:23<8:17:09,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52, Loss Train: 3.7189, Valid: 1.5174\n",
      "\n",
      "-------------------EPOCH52-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2650/50000 [2:11:52<8:01:56,  1.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53, Loss Train: 3.5306, Valid: 1.5155\n",
      "\n",
      "-------------------EPOCH53-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2700/50000 [2:14:22<8:14:21,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54, Loss Train: 3.5496, Valid: 1.5149\n",
      "\n",
      "-------------------EPOCH54-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 2750/50000 [2:16:47<8:09:29,  1.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55, Loss Train: 3.5042, Valid: 1.5148\n",
      "\n",
      "-------------------EPOCH55-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 2800/50000 [2:19:12<8:00:28,  1.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56, Loss Train: 3.5516, Valid: 1.5118\n",
      "\n",
      "-------------------EPOCH56-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 2850/50000 [2:21:38<8:12:53,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 57, Loss Train: 3.6427, Valid: 1.5113\n",
      "\n",
      "-------------------EPOCH57-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 2900/50000 [2:24:07<8:11:47,  1.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 58, Loss Train: 3.4072, Valid: 1.5098\n",
      "\n",
      "-------------------EPOCH58-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 2950/50000 [2:26:36<7:58:16,  1.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59, Loss Train: 3.4861, Valid: 1.508\n",
      "\n",
      "-------------------EPOCH59-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3000/50000 [2:29:05<7:57:19,  1.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 60, Loss Train: 3.671, Valid: 1.5063\n",
      "\n",
      "-------------------EPOCH60-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3050/50000 [2:31:31<7:51:55,  1.66it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61, Loss Train: 3.4194, Valid: 1.5016\n",
      "\n",
      "-------------------EPOCH61-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3100/50000 [2:34:00<7:51:29,  1.66it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62, Loss Train: 3.6481, Valid: 1.5017\n",
      "\n",
      "-------------------EPOCH62-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 3150/50000 [2:36:28<7:52:26,  1.65it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63, Loss Train: 3.6663, Valid: 1.4978\n",
      "\n",
      "-------------------EPOCH63-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 3200/50000 [2:38:58<8:04:12,  1.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 64, Loss Train: 3.695, Valid: 1.4976\n",
      "\n",
      "-------------------EPOCH64-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 3250/50000 [2:41:25<7:54:59,  1.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65, Loss Train: 3.2115, Valid: 1.4968\n",
      "\n",
      "-------------------EPOCH65-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3300/50000 [2:43:59<7:52:45,  1.65it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66, Loss Train: 3.5234, Valid: 1.4943\n",
      "\n",
      "-------------------EPOCH66-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3350/50000 [2:46:29<7:54:09,  1.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 67, Loss Train: 3.3293, Valid: 1.4965\n",
      "\n",
      "-------------------EPOCH67-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3400/50000 [2:48:56<8:14:10,  1.57it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68, Loss Train: 3.5285, Valid: 1.4938\n",
      "\n",
      "-------------------EPOCH68-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3450/50000 [2:51:31<7:47:26,  1.66it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 69, Loss Train: 3.6151, Valid: 1.4903\n",
      "\n",
      "-------------------EPOCH69-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3500/50000 [2:53:59<7:51:19,  1.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 70, Loss Train: 3.5177, Valid: 1.4886\n",
      "\n",
      "-------------------EPOCH70-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3550/50000 [2:56:34<8:05:58,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71, Loss Train: 3.5864, Valid: 1.4937\n",
      "\n",
      "-------------------EPOCH71-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3600/50000 [2:59:09<8:02:14,  1.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72, Loss Train: 3.4531, Valid: 1.4869\n",
      "\n",
      "-------------------EPOCH72-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3650/50000 [3:01:43<8:04:51,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73, Loss Train: 3.2977, Valid: 1.488\n",
      "\n",
      "-------------------EPOCH73-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3700/50000 [3:04:18<8:00:15,  1.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 74, Loss Train: 3.5963, Valid: 1.4828\n",
      "\n",
      "-------------------EPOCH74-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3750/50000 [3:06:49<8:02:45,  1.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75, Loss Train: 3.5288, Valid: 1.4816\n",
      "\n",
      "-------------------EPOCH75-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3800/50000 [3:09:22<8:05:06,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76, Loss Train: 3.4222, Valid: 1.4816\n",
      "\n",
      "-------------------EPOCH76-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3850/50000 [3:12:01<8:11:00,  1.57it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77, Loss Train: 3.4431, Valid: 1.4817\n",
      "\n",
      "-------------------EPOCH77-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3900/50000 [3:14:40<8:02:53,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 78, Loss Train: 3.5238, Valid: 1.4806\n",
      "\n",
      "-------------------EPOCH78-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3950/50000 [3:17:21<8:02:13,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 79, Loss Train: 3.5032, Valid: 1.477\n",
      "\n",
      "-------------------EPOCH79-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4000/50000 [3:19:59<8:03:40,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80, Loss Train: 3.421, Valid: 1.4766\n",
      "\n",
      "-------------------EPOCH80-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4050/50000 [3:22:35<7:46:29,  1.64it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81, Loss Train: 3.4176, Valid: 1.4789\n",
      "\n",
      "-------------------EPOCH81-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4100/50000 [3:25:10<7:58:13,  1.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 82, Loss Train: 3.2793, Valid: 1.4763\n",
      "\n",
      "-------------------EPOCH82-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4150/50000 [3:27:45<7:55:03,  1.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 83, Loss Train: 3.4833, Valid: 1.4791\n",
      "\n",
      "-------------------EPOCH83-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4200/50000 [3:30:14<8:02:15,  1.58it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 84, Loss Train: 3.5871, Valid: 1.4773\n",
      "\n",
      "-------------------EPOCH84-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4250/50000 [3:32:41<7:49:51,  1.62it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 85, Loss Train: 3.3707, Valid: 1.4736\n",
      "\n",
      "-------------------EPOCH85-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 4300/50000 [3:35:10<7:54:33,  1.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86, Loss Train: 3.4753, Valid: 1.4726\n",
      "\n",
      "-------------------EPOCH86-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 4350/50000 [3:37:41<7:57:27,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87, Loss Train: 3.1493, Valid: 1.4723\n",
      "\n",
      "-------------------EPOCH87-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 4400/50000 [3:40:17<7:41:54,  1.65it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 88, Loss Train: 3.5135, Valid: 1.4702\n",
      "\n",
      "-------------------EPOCH88-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 4450/50000 [3:42:51<7:59:38,  1.58it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89, Loss Train: 3.3592, Valid: 1.4693\n",
      "\n",
      "-------------------EPOCH89-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 4500/50000 [3:45:23<8:00:01,  1.58it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90, Loss Train: 3.5216, Valid: 1.4697\n",
      "\n",
      "-------------------EPOCH90-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 4550/50000 [3:47:57<8:02:18,  1.57it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 91, Loss Train: 3.4419, Valid: 1.4658\n",
      "\n",
      "-------------------EPOCH91-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 4600/50000 [3:50:33<7:51:01,  1.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92, Loss Train: 3.3656, Valid: 1.464\n",
      "\n",
      "-------------------EPOCH92-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 4650/50000 [3:53:06<7:51:57,  1.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93, Loss Train: 3.4589, Valid: 1.4604\n",
      "\n",
      "-------------------EPOCH93-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 4700/50000 [3:55:36<7:52:42,  1.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 94, Loss Train: 3.357, Valid: 1.4575\n",
      "\n",
      "-------------------EPOCH94-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 4750/50000 [3:58:09<7:57:55,  1.58it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 95, Loss Train: 3.4654, Valid: 1.4595\n",
      "\n",
      "-------------------EPOCH95-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 4800/50000 [4:00:36<7:53:12,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 96, Loss Train: 3.3507, Valid: 1.4569\n",
      "\n",
      "-------------------EPOCH96-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 4850/50000 [4:03:07<7:52:03,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97, Loss Train: 3.4681, Valid: 1.4583\n",
      "\n",
      "-------------------EPOCH97-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 4900/50000 [4:05:39<7:59:26,  1.57it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 98, Loss Train: 3.1649, Valid: 1.4603\n",
      "\n",
      "-------------------EPOCH98-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 4950/50000 [4:08:05<7:41:52,  1.63it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99, Loss Train: 3.3946, Valid: 1.4606\n",
      "\n",
      "-------------------EPOCH99-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5000/50000 [4:10:39<7:56:33,  1.57it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss Train: 3.4465, Valid: 1.4566\n",
      "\n",
      "-------------------EPOCH100-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5050/50000 [4:13:13<7:52:27,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 101, Loss Train: 3.3593, Valid: 1.4553\n",
      "\n",
      "-------------------EPOCH101-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5100/50000 [4:15:46<7:48:19,  1.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 102, Loss Train: 3.5119, Valid: 1.4519\n",
      "\n",
      "-------------------EPOCH102-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5150/50000 [4:18:17<7:41:28,  1.62it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 103, Loss Train: 3.3127, Valid: 1.4526\n",
      "\n",
      "-------------------EPOCH103-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5200/50000 [4:20:48<7:50:04,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 104, Loss Train: 3.3957, Valid: 1.45\n",
      "\n",
      "-------------------EPOCH104-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5250/50000 [4:23:21<7:49:22,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 105, Loss Train: 3.5011, Valid: 1.4485\n",
      "\n",
      "-------------------EPOCH105-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 5300/50000 [4:25:52<7:50:50,  1.58it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 106, Loss Train: 3.4975, Valid: 1.4483\n",
      "\n",
      "-------------------EPOCH106-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 5350/50000 [4:28:25<7:43:58,  1.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 107, Loss Train: 3.3989, Valid: 1.4498\n",
      "\n",
      "-------------------EPOCH107-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 5400/50000 [4:30:57<7:44:27,  1.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 108, Loss Train: 3.5222, Valid: 1.4461\n",
      "\n",
      "-------------------EPOCH108-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 5450/50000 [4:33:33<7:47:40,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 109, Loss Train: 3.281, Valid: 1.4455\n",
      "\n",
      "-------------------EPOCH109-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 5500/50000 [4:36:06<7:43:26,  1.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 110, Loss Train: 3.4003, Valid: 1.4465\n",
      "\n",
      "-------------------EPOCH110-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 5550/50000 [4:38:39<7:41:01,  1.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 111, Loss Train: 3.3039, Valid: 1.4464\n",
      "\n",
      "-------------------EPOCH111-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 5600/50000 [4:41:13<7:39:32,  1.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 112, Loss Train: 3.2873, Valid: 1.4435\n",
      "\n",
      "-------------------EPOCH112-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 5650/50000 [4:43:47<7:40:14,  1.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 113, Loss Train: 3.1575, Valid: 1.4428\n",
      "\n",
      "-------------------EPOCH113-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 5700/50000 [4:46:21<7:40:33,  1.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 114, Loss Train: 3.405, Valid: 1.44\n",
      "\n",
      "-------------------EPOCH114-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 5750/50000 [4:48:53<7:41:29,  1.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 115, Loss Train: 3.2915, Valid: 1.4395\n",
      "\n",
      "-------------------EPOCH115-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 5800/50000 [4:51:28<7:41:46,  1.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 116, Loss Train: 3.3832, Valid: 1.4419\n",
      "\n",
      "-------------------EPOCH116-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 5850/50000 [4:54:01<7:37:14,  1.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 117, Loss Train: 3.3942, Valid: 1.4398\n",
      "\n",
      "-------------------EPOCH117-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 5900/50000 [4:56:35<7:49:06,  1.57it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 118, Loss Train: 3.1458, Valid: 1.4446\n",
      "\n",
      "-------------------EPOCH118-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 5950/50000 [4:59:18<7:34:28,  1.62it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 119, Loss Train: 3.4171, Valid: 1.4497\n",
      "\n",
      "-------------------EPOCH119-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6000/50000 [5:01:49<7:38:19,  1.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 120, Loss Train: 3.3776, Valid: 1.4496\n",
      "\n",
      "-------------------EPOCH120-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6050/50000 [5:04:22<7:38:40,  1.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 121, Loss Train: 3.3732, Valid: 1.4395\n",
      "\n",
      "-------------------EPOCH121-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6100/50000 [5:06:55<7:35:01,  1.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 122, Loss Train: 3.1889, Valid: 1.4374\n",
      "\n",
      "-------------------EPOCH122-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6150/50000 [5:09:28<7:38:13,  1.59it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 123, Loss Train: 3.313, Valid: 1.4373\n",
      "\n",
      "-------------------EPOCH123-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6200/50000 [5:12:01<7:34:19,  1.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 124, Loss Train: 3.4514, Valid: 1.4421\n",
      "\n",
      "-------------------EPOCH124-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 6250/50000 [5:14:33<7:37:07,  1.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 125, Loss Train: 3.4727, Valid: 1.441\n",
      "\n",
      "-------------------EPOCH125-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 6300/50000 [5:17:06<7:35:04,  1.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 126, Loss Train: 3.3577, Valid: 1.4415\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:424] . unexpected pos 6692818368 vs 6692818264",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:619\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 619\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:853\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    852\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 853\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/579: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m notebook_launcher\n\u001b[1;32m      2\u001b[0m args \u001b[38;5;241m=\u001b[39m ()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mnotebook_launcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/launchers.py:221\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLaunching training on CPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 221\u001b[0m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 146\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m unwrapped_vae \u001b[38;5;241m=\u001b[39m accelerator\u001b[38;5;241m.\u001b[39munwrap_model(compression_model)\n\u001b[1;32m    145\u001b[0m unwrapped_soundconditioner \u001b[38;5;241m=\u001b[39m accelerator\u001b[38;5;241m.\u001b[39munwrap_model(soundconditioner)\n\u001b[0;32m--> 146\u001b[0m best_loss \u001b[38;5;241m=\u001b[39m \u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munwrapped_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_step, (wav, descriptions) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_dataloader):\n\u001b[1;32m    148\u001b[0m     audio_conditions \u001b[38;5;241m=\u001b[39m unwrapped_soundconditioner(wav)\n",
      "Cell \u001b[0;32mIn[5], line 62\u001b[0m, in \u001b[0;36msave_checkpoint\u001b[0;34m(cfg, model, result, best_loss, epoch)\u001b[0m\n\u001b[1;32m     59\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cfg\u001b[38;5;241m.\u001b[39moutput_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     61\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cfg\u001b[38;5;241m.\u001b[39moutput_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m---> 62\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_loss\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:618\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    615\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    619\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    620\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:466\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    468\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:424] . unexpected pos 6692818368 vs 6692818264"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "args = ()\n",
    "notebook_launcher(main, args, num_processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "718c1a0d-972d-4cd0-b63d-43172684aeda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b105a6b72aa34b2882f9a4f9ff20d2bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "compression_state_dict.bin:   0%|          | 0.00/236M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4124551d9a84e6fae6d08a312ca59ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "state_dict.bin:   0%|          | 0.00/3.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUSER:  {'cross_attention_pos_emb': False, 'cross_attention_pos_emb_scale': 1, 'sum': [], 'prepend': [], 'cross': ['description', 'sound'], 'input_interpolate': []}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff69180137d04cf7bc90b50b7112b3d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023a5a88fc164ec18659574f98440a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba4a38c95aa4296b35e0eef03d3bc87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40d565eb98d4a5698d7a2dedb91ee70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfg = Config()\n",
    "accelerator = Accelerator(gradient_accumulation_steps=cfg.gradient_accumulation_steps)\n",
    "device = accelerator.device\n",
    "cfg.update(device=accelerator.device)\n",
    "\n",
    "base_path = \"./csv_files/\"\n",
    "train_data_path = f\"{base_path}/eval_epidemic_dataset.csv\"\n",
    "eval_data_path = f\"{base_path}/eval_epidemic_dataset.csv\"\n",
    "cfg.update(train_data_path=train_data_path, eval_data_path=eval_data_path)\n",
    "\n",
    "# 'sound'를 'cross' 키의 리스트에 추가\n",
    "cfg.fuser['cross'].append('sound')\n",
    "\n",
    "compression_model, lm = build_model(cfg)\n",
    "model = AudioProcessing(cfg, lm)\n",
    "\n",
    "audio_dataset = AudioDataset(cfg, train=True) \n",
    "eval_dataset = AudioDataset(cfg, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4ddb9e2a-a7f5-4030-8ac1-c9e041d7df36",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5conditioner = copy.deepcopy(lm.condition_provider.conditioners.description)\n",
    "soundconditioner = SoundConditionerEncodec(compression_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8a02f8de-9b30-4297-9aaf-190e4c43748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dataloader = DataLoader(audio_dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "optimizer_parameters = [param for param in model.lm.parameters() if param.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    optimizer_parameters, lr=cfg.learning_rate,\n",
    "    betas=(cfg.adam_beta1, cfg.adam_beta2),\n",
    "    weight_decay=cfg.adam_weight_decay,\n",
    "    eps=cfg.adam_epsilon,\n",
    ")\n",
    "\n",
    "num_update_steps_per_epoch = math.ceil(len(audio_dataloader) / cfg.gradient_accumulation_steps)\n",
    "if cfg.max_train_steps is None:\n",
    "  cfg.max_train_steps = cfg.num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "      name=cfg.lr_scheduler_type,\n",
    "      optimizer=optimizer,\n",
    "      num_warmup_steps=cfg.num_warmup_steps * cfg.gradient_accumulation_steps,\n",
    "      num_training_steps=cfg.max_train_steps * cfg.gradient_accumulation_steps,\n",
    "  )\n",
    "\n",
    "\n",
    "audio_dataloader, eval_dataloader, model, compression_model, t5conditioner, soundconditioner, optimizer, lr_scheduler = accelerator.prepare(\n",
    "audio_dataloader, eval_dataloader, model, compression_model, t5conditioner, soundconditioner, optimizer, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "42d8cd76-08dc-4faf-b779-5bf740b3852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav, description, lengths = next(iter(audio_dataloader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    unwrapped_textconditioner = accelerator.unwrap_model(t5conditioner)\n",
    "    unwrapped_soundconditioner = accelerator.unwrap_model(soundconditioner)\n",
    "    \n",
    "    tokenized = {}\n",
    "    tokenized[\"description\"] =  unwrapped_textconditioner.tokenize(description)\n",
    "    tokenized[\"sound\"] = wav\n",
    "    \n",
    "    # conditioning\n",
    "    output = {}\n",
    "    for attribute, inputs in tokenized.items():\n",
    "        if attribute == \"description\":   \n",
    "            condition, mask = unwrapped_textconditioner(inputs)\n",
    "        elif attribute == \"sound\":\n",
    "            condition, mask = unwrapped_soundconditioner(inputs)\n",
    "        output[attribute] = (condition, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fad4a54e-7a87-4772-b3d8-12f99e2788a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 150, 128])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition.permute(0,2,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af55730d-482e-4222-adaf-b1a076c3bcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    unwrapped_vae = accelerator.unwrap_model(compression_model)\n",
    "    audio_tokens = process_audio_tokenizer(wav, unwrapped_vae)\n",
    "    audio_tokens, padding_mask = post_process_audio_tokenizer(audio_tokens, lengths, unwrapped_vae, lm, cfg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7c63580-dbc0-45b2-a235-19e5ceb01576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description torch.Size([1, 7, 1536])\n",
      "sound torch.Size([1, 180, 1536])\n",
      "torch.Size([1, 7, 1536])\n",
      "torch.Size([1, 180, 1536])\n"
     ]
    }
   ],
   "source": [
    "loss = model(audio_tokens, padding_mask, attributes=None, condition_tensors=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8147bed3-f1b0-41e4-9efe-462f3981edb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0381e8c-92a6-4f49-abe8-56a444b041ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AudioProcessing(\n",
       "  (lm): LMModel(\n",
       "    (cfg_dropout): ClassifierFreeGuidanceDropout(p=0.1)\n",
       "    (att_dropout): AttributeDropout({})\n",
       "    (condition_provider): ConditioningProvider(\n",
       "      (conditioners): ModuleDict(\n",
       "        (description): T5Conditioner(\n",
       "          (output_proj): Linear(in_features=1024, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fuser): ConditionFuser()\n",
       "    (emb): ModuleList(\n",
       "      (0-3): 4 x ScaledEmbedding(2049, 1536)\n",
       "    )\n",
       "    (transformer): StreamingTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-47): 48 x StreamingTransformerLayer(\n",
       "          (self_attn): StreamingMultiheadAttention(\n",
       "            (out_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1536, out_features=6144, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (linear2): Linear(in_features=6144, out_features=1536, bias=False)\n",
       "          (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.0, inplace=False)\n",
       "          (dropout2): Dropout(p=0.0, inplace=False)\n",
       "          (layer_scale_1): Identity()\n",
       "          (layer_scale_2): Identity()\n",
       "          (cross_attention): StreamingMultiheadAttention(\n",
       "            (out_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
       "          )\n",
       "          (dropout_cross): Dropout(p=0.0, inplace=False)\n",
       "          (norm_cross): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          (layer_scale_cross): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (out_norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "    (linears): ModuleList(\n",
       "      (0-3): 4 x Linear(in_features=1536, out_features=2048, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (audio_embeds_linear): Linear(in_features=768, out_features=1536, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6325d2f-8c6c-4964-bd41-a97db182009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TestDataset(cfg)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccb90bf2-2b1d-4100-a5ae-1fc455476217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<Random>', '<Random>')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "665c0beb-5d6b-4ed5-afde-475b4588affb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ConditioningAttributes(text={'description': '<Random>'}, wav={}, joint_embed={}), ConditioningAttributes(text={'description': '<Random>'}, wav={}, joint_embed={}), ConditioningAttributes(text={'description': None}, wav={}, joint_embed={}), ConditioningAttributes(text={'description': None}, wav={}, joint_embed={})]\n",
      "cfg_conditions {'description': (tensor([[[-0.1169, -0.0594, -0.0393,  ...,  0.0848, -0.1661,  0.0396],\n",
      "         [-0.0717,  0.0090, -0.0641,  ..., -0.1875,  0.1627, -0.0029],\n",
      "         [-0.0853, -0.0236, -0.1074,  ..., -0.0780,  0.4068,  0.0394],\n",
      "         ...,\n",
      "         [-0.1232,  0.0309,  0.0335,  ...,  0.0234, -0.0042,  0.1150],\n",
      "         [ 0.3359, -0.1430, -0.1474,  ..., -0.2348, -0.2121,  0.1714],\n",
      "         [ 0.0261, -0.0256, -0.0012,  ...,  0.0121, -0.0851,  0.0096]],\n",
      "\n",
      "        [[-0.1169, -0.0594, -0.0393,  ...,  0.0848, -0.1661,  0.0396],\n",
      "         [-0.0717,  0.0090, -0.0641,  ..., -0.1875,  0.1627, -0.0029],\n",
      "         [-0.0853, -0.0236, -0.1074,  ..., -0.0780,  0.4068,  0.0394],\n",
      "         ...,\n",
      "         [-0.1232,  0.0309,  0.0335,  ...,  0.0234, -0.0042,  0.1150],\n",
      "         [ 0.3359, -0.1430, -0.1474,  ..., -0.2348, -0.2121,  0.1714],\n",
      "         [ 0.0261, -0.0256, -0.0012,  ...,  0.0121, -0.0851,  0.0096]],\n",
      "\n",
      "        [[ 0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
      "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000,  0.0000]]],\n",
      "       device='cuda:0'), tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0]], device='cuda:0'))}\n",
      "4\n",
      "torch.Size([4, 7, 1536])\n",
      "torch.Size([8, 180, 1536])\n",
      "cfg_coef,  None\n",
      "description torch.Size([4, 7, 1536])\n",
      "sound torch.Size([8, 180, 1536])\n",
      "torch.Size([4, 7, 1536])\n",
      "torch.Size([8, 180, 1536])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 4 but got size 8 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_step, (wav, descriptions) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_dataloader):\n\u001b[1;32m      6\u001b[0m     audio_conditions \u001b[38;5;241m=\u001b[39m unwrapped_soundconditioner(wav)\n\u001b[0;32m----> 7\u001b[0m     gen_token, gen_audio \u001b[38;5;241m=\u001b[39m \u001b[43munwrapped_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdescriptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_conditions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munwrapped_vae\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/audiogen-finetune/audiomodel_inpainting.py:77\u001b[0m, in \u001b[0;36mAudioProcessing.inference\u001b[0;34m(self, descriptions, audio_conditions, compression_model)\u001b[0m\n\u001b[1;32m     75\u001b[0m     audio_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio_embeds_linear(audio_embeds)\n\u001b[1;32m     76\u001b[0m     audio_conditions \u001b[38;5;241m=\u001b[39m (audio_embeds, mask)\n\u001b[0;32m---> 77\u001b[0m     gen_tokens, gen_audio \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_conditions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_conditions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgen_duration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gen_tokens, gen_audio\n",
      "File \u001b[0;32m/workspace/audiogen-finetune/audiomodel_inpainting.py:62\u001b[0m, in \u001b[0;36mAudioProcessing.audio_generate\u001b[0;34m(self, condition_tensors, audio_conditions, gen_duration, compression_model)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     61\u001b[0m     total_gen_len \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(gen_duration \u001b[38;5;241m*\u001b[39m compression_model\u001b[38;5;241m.\u001b[39mframe_rate)\n\u001b[0;32m---> 62\u001b[0m     gen_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_audio_inpainting\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondition_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_conditions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_conditions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_gen_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_gen_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     gen_audio \u001b[38;5;241m=\u001b[39m compression_model\u001b[38;5;241m.\u001b[39mdecode(gen_tokens, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gen_tokens, gen_audio\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/audiogen-finetune/audiocraft/models/lm_audio_inpainting.py:508\u001b[0m, in \u001b[0;36mLMModel.generate_audio_inpainting\u001b[0;34m(self, prompt, conditions, audio_conditions, num_samples, max_gen_len, use_sampling, temp, top_k, top_p, cfg_coef, two_step_cfg, remove_prompts, check, callback)\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (curr_sequence \u001b[38;5;241m==\u001b[39m unknown_token)\u001b[38;5;241m.\u001b[39many()\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# sample next token from the model, next token shape is [B, K, 1]\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m next_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample_next_token\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcurr_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg_conditions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munconditional_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_sampling\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcfg_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg_coef\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtwo_step_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtwo_step_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m# ensure the tokens that should be masked are properly set to special_token_id\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;66;03m# as the model never output special_token_id\u001b[39;00m\n\u001b[1;32m    513\u001b[0m valid_mask \u001b[38;5;241m=\u001b[39m mask[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, offset:offset\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mexpand(B, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/workspace/audiogen-finetune/audiocraft/models/lm_audio_inpainting.py:356\u001b[0m, in \u001b[0;36mLMModel._sample_next_token\u001b[0;34m(self, sequence, cfg_conditions, unconditional_state, use_sampling, temp, top_k, top_p, cfg_coef, two_step_cfg)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m condition_tensors:\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;66;03m# Preparing for CFG, predicting both conditional and unconditional logits.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m     sequence \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([sequence, sequence], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 356\u001b[0m all_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconditions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondition_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcondition_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m condition_tensors:\n\u001b[1;32m    360\u001b[0m     cond_logits, uncond_logits \u001b[38;5;241m=\u001b[39m all_logits\u001b[38;5;241m.\u001b[39msplit(B, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# [B, K, T, card]\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/audiogen-finetune/audiocraft/models/lm_audio_inpainting.py:251\u001b[0m, in \u001b[0;36mLMModel.forward\u001b[0;34m(self, sequence, conditions, condition_tensors)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conditions, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt pass both conditions and condition_tensors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 251\u001b[0m input_, cross_attention_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfuser\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondition_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(input_, cross_attention_src\u001b[38;5;241m=\u001b[39mcross_attention_input)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_norm:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/audiogen-finetune/audiocraft/modules/conditioners.py:1402\u001b[0m, in \u001b[0;36mConditionFuser.forward\u001b[0;34m(self, input, conditions)\u001b[0m\n\u001b[1;32m   1400\u001b[0m     \u001b[38;5;28mprint\u001b[39m(cross_attention_output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m   1401\u001b[0m     \u001b[38;5;28mprint\u001b[39m(cond\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m-> 1402\u001b[0m     cross_attention_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcross_attention_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1404\u001b[0m     cross_attention_output \u001b[38;5;241m=\u001b[39m cond\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 4 but got size 8 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "unwrapped_model = accelerator.unwrap_model(model)\n",
    "unwrapped_vae = accelerator.unwrap_model(compression_model)\n",
    "unwrapped_soundconditioner = accelerator.unwrap_model(soundconditioner)\n",
    "#best_loss = save_checkpoint(cfg, unwrapped_model, result, best_loss, save_epoch)\n",
    "for test_step, (wav, descriptions) in enumerate(test_dataloader):\n",
    "    audio_conditions = unwrapped_soundconditioner(wav)\n",
    "    gen_token, gen_audio = unwrapped_model.inference(descriptions, audio_conditions, unwrapped_vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0821a85-c54a-4665-a157-6e0d9154a2b6",
   "metadata": {},
   "source": [
    "### run train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "babea65d-07e9-465f-bc14-316616511a90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510c8896-d209-4925-8acd-588e45fe0578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "\n",
      "  0%|          | 0/14893000 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------EPOCH0-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 1/14893000 [00:07<30143:36:38,  7.29s/it]\u001b[A\n",
      "  0%|          | 2/14893000 [00:11<21975:37:12,  5.31s/it]\u001b[A\n",
      "  0%|          | 3/14893000 [00:15<19416:09:57,  4.69s/it]\u001b[A\n",
      "  0%|          | 4/14893000 [00:19<18229:25:53,  4.41s/it]\u001b[A\n",
      "  0%|          | 5/14893000 [00:22<17288:14:16,  4.18s/it]\u001b[A\n",
      "  0%|          | 6/14893000 [00:26<16950:58:43,  4.10s/it]\u001b[A\n",
      "  0%|          | 7/14893000 [00:30<16643:03:51,  4.02s/it]\u001b[A\n",
      "  0%|          | 8/14893000 [00:34<16558:05:44,  4.00s/it]\u001b[A\n",
      "  0%|          | 9/14893000 [00:38<16448:22:03,  3.98s/it]\u001b[A\n",
      "  0%|          | 10/14893000 [00:42<16383:33:38,  3.96s/it]\u001b[A\n",
      "  0%|          | 11/14893000 [00:46<16338:35:22,  3.95s/it]\u001b[A\n",
      "  0%|          | 12/14893000 [00:50<16323:30:47,  3.95s/it]\u001b[A\n",
      "  0%|          | 13/14893000 [00:54<16315:28:48,  3.94s/it]\u001b[A\n",
      "  0%|          | 14/14893000 [00:58<16269:39:58,  3.93s/it]\u001b[A\n",
      "  0%|          | 15/14893000 [01:02<16331:42:04,  3.95s/it]\u001b[A\n",
      "  0%|          | 16/14893000 [01:06<16288:19:17,  3.94s/it]\u001b[A\n",
      "  0%|          | 17/14893000 [01:10<16249:26:09,  3.93s/it]\u001b[A\n",
      "  0%|          | 18/14893000 [01:13<16226:02:45,  3.92s/it]\u001b[A\n",
      "  0%|          | 19/14893000 [01:17<16253:03:38,  3.93s/it]\u001b[A\n",
      "  0%|          | 20/14893000 [01:21<16254:39:48,  3.93s/it]\u001b[A\n",
      "  0%|          | 21/14893000 [01:25<16230:05:21,  3.92s/it]\u001b[A\n",
      "  0%|          | 22/14893000 [01:29<16336:35:32,  3.95s/it]\u001b[A\n",
      "  0%|          | 23/14893000 [01:33<16329:14:07,  3.95s/it]\u001b[A\n",
      "  0%|          | 24/14893000 [01:37<16309:04:07,  3.94s/it]\u001b[A\n",
      "  0%|          | 25/14893000 [01:41<16264:37:31,  3.93s/it]\u001b[A\n",
      "  0%|          | 26/14893000 [01:45<16240:27:14,  3.93s/it]\u001b[A\n",
      "  0%|          | 27/14893000 [01:49<16219:03:19,  3.92s/it]\u001b[A\n",
      "  0%|          | 28/14893000 [01:53<16263:04:39,  3.93s/it]\u001b[A\n",
      "  0%|          | 29/14893000 [01:57<16297:08:12,  3.94s/it]\u001b[A\n",
      "  0%|          | 30/14893000 [02:01<16314:19:19,  3.94s/it]\u001b[A\n",
      "  0%|          | 31/14893000 [02:05<16359:26:04,  3.95s/it]\u001b[A\n",
      "  0%|          | 32/14893000 [02:09<16321:54:51,  3.95s/it]\u001b[A\n",
      "  0%|          | 33/14893000 [02:13<16301:58:57,  3.94s/it]\u001b[A\n",
      "  0%|          | 34/14893000 [02:16<16285:38:08,  3.94s/it]\u001b[A\n",
      "  0%|          | 35/14893000 [02:20<16263:01:22,  3.93s/it]\u001b[A\n",
      "  0%|          | 36/14893000 [02:24<16287:38:26,  3.94s/it]\u001b[A\n",
      "  0%|          | 37/14893000 [02:28<16306:30:00,  3.94s/it]\u001b[A\n",
      "  0%|          | 38/14893000 [02:32<16621:04:57,  4.02s/it]\u001b[A\n",
      "  0%|          | 39/14893000 [02:36<16543:11:27,  4.00s/it]\u001b[A\n",
      "  0%|          | 40/14893000 [02:40<16247:53:48,  3.93s/it]\u001b[A\n",
      "  0%|          | 41/14893000 [02:44<16265:46:49,  3.93s/it]\u001b[A\n",
      "  0%|          | 42/14893000 [02:48<16082:23:54,  3.89s/it]\u001b[A\n",
      "  0%|          | 43/14893000 [02:52<16119:22:45,  3.90s/it]\u001b[A\n",
      "  0%|          | 44/14893000 [02:56<16149:20:52,  3.90s/it]\u001b[A\n",
      "  0%|          | 45/14893000 [03:00<16176:43:58,  3.91s/it]\u001b[A\n",
      "  0%|          | 46/14893000 [03:04<16232:47:49,  3.92s/it]\u001b[A\n",
      "  0%|          | 47/14893000 [03:08<16255:14:46,  3.93s/it]\u001b[A\n",
      "  0%|          | 48/14893000 [03:11<16047:26:45,  3.88s/it]\u001b[A\n",
      "  0%|          | 49/14893000 [03:15<16083:50:06,  3.89s/it]\u001b[A\n",
      "  0%|          | 50/14893000 [03:19<16129:49:49,  3.90s/it]\u001b[A\n",
      "  0%|          | 51/14893000 [03:23<16155:47:37,  3.91s/it]\u001b[A\n",
      "  0%|          | 52/14893000 [03:27<16155:44:51,  3.91s/it]\u001b[A\n",
      "  0%|          | 53/14893000 [03:31<16340:20:09,  3.95s/it]\u001b[A\n",
      "  0%|          | 54/14893000 [03:35<16310:44:12,  3.94s/it]\u001b[A\n",
      "  0%|          | 55/14893000 [03:39<16285:18:12,  3.94s/it]\u001b[A\n",
      "  0%|          | 56/14893000 [03:43<16252:51:28,  3.93s/it]\u001b[A\n",
      "  0%|          | 57/14893000 [03:47<16225:46:36,  3.92s/it]\u001b[A\n",
      "  0%|          | 58/14893000 [03:51<16218:10:18,  3.92s/it]\u001b[A\n",
      "  0%|          | 59/14893000 [03:55<16210:45:23,  3.92s/it]\u001b[A\n",
      "  0%|          | 60/14893000 [03:58<16228:56:49,  3.92s/it]\u001b[A\n",
      "  0%|          | 61/14893000 [04:02<16228:24:33,  3.92s/it]\u001b[A\n",
      "  0%|          | 62/14893000 [04:06<16289:22:24,  3.94s/it]\u001b[A\n",
      "  0%|          | 63/14893000 [04:10<16264:01:30,  3.93s/it]\u001b[A\n",
      "  0%|          | 64/14893000 [04:14<16242:27:27,  3.93s/it]\u001b[A\n",
      "  0%|          | 65/14893000 [04:18<16234:30:34,  3.92s/it]\u001b[A\n",
      "  0%|          | 66/14893000 [04:22<16225:10:29,  3.92s/it]\u001b[A\n",
      "  0%|          | 67/14893000 [04:26<16237:13:56,  3.92s/it]\u001b[A\n",
      "  0%|          | 68/14893000 [04:30<16379:37:01,  3.96s/it]\u001b[A\n",
      "  0%|          | 69/14893000 [04:34<16320:18:47,  3.95s/it]\u001b[A\n",
      "  0%|          | 70/14893000 [04:38<16260:10:26,  3.93s/it]\u001b[A\n",
      "  0%|          | 71/14893000 [04:42<16261:39:37,  3.93s/it]\u001b[A\n",
      "  0%|          | 72/14893000 [04:46<16248:40:59,  3.93s/it]\u001b[A\n",
      "  0%|          | 73/14893000 [04:49<16050:52:10,  3.88s/it]\u001b[A\n",
      "  0%|          | 74/14893000 [04:53<16118:36:34,  3.90s/it]\u001b[A\n",
      "  0%|          | 75/14893000 [04:57<16142:11:14,  3.90s/it]\u001b[A\n",
      "  0%|          | 76/14893000 [05:01<16155:43:15,  3.91s/it]\u001b[A\n",
      "  0%|          | 77/14893000 [05:05<16175:19:57,  3.91s/it]\u001b[A\n",
      "  0%|          | 78/14893000 [05:09<16185:12:54,  3.91s/it]\u001b[A\n",
      "  0%|          | 79/14893000 [05:13<16201:32:56,  3.92s/it]\u001b[A\n",
      "  0%|          | 80/14893000 [05:17<16259:06:52,  3.93s/it]\u001b[A\n",
      "  0%|          | 81/14893000 [05:21<16408:09:01,  3.97s/it]\u001b[A\n",
      "  0%|          | 82/14893000 [05:25<16358:11:09,  3.95s/it]\u001b[A\n",
      "  0%|          | 83/14893000 [05:29<16362:10:26,  3.96s/it]\u001b[A\n",
      "  0%|          | 84/14893000 [05:33<16406:14:53,  3.97s/it]\u001b[A\n",
      "  0%|          | 85/14893000 [05:37<16518:07:18,  3.99s/it]\u001b[A\n",
      "  0%|          | 86/14893000 [05:41<16601:07:11,  4.01s/it]\u001b[A\n",
      "  0%|          | 87/14893000 [05:45<16635:44:29,  4.02s/it]\u001b[A\n",
      "  0%|          | 88/14893000 [05:49<16353:23:14,  3.95s/it]\u001b[A\n",
      "  0%|          | 89/14893000 [05:53<16323:08:33,  3.95s/it]\u001b[A\n",
      "  0%|          | 90/14893000 [05:57<16300:45:25,  3.94s/it]\u001b[A\n",
      "  0%|          | 91/14893000 [06:01<16253:58:33,  3.93s/it]\u001b[A\n",
      "  0%|          | 92/14893000 [06:05<16262:08:58,  3.93s/it]\u001b[A\n",
      "  0%|          | 93/14893000 [06:08<16036:16:28,  3.88s/it]\u001b[A\n",
      "  0%|          | 94/14893000 [06:12<16092:53:27,  3.89s/it]\u001b[A\n",
      "  0%|          | 95/14893000 [06:16<16250:34:35,  3.93s/it]\u001b[A\n",
      "  0%|          | 96/14893000 [06:20<16232:05:13,  3.92s/it]\u001b[A\n",
      "  0%|          | 97/14893000 [06:24<16256:58:49,  3.93s/it]\u001b[A\n",
      "  0%|          | 98/14893000 [06:28<16277:51:03,  3.93s/it]\u001b[A\n",
      "  0%|          | 99/14893000 [06:32<16465:03:32,  3.98s/it]\u001b[A\n",
      "  0%|          | 100/14893000 [06:36<16441:35:19,  3.97s/it]\u001b[A\n",
      "  0%|          | 101/14893000 [06:40<16402:03:15,  3.96s/it]\u001b[A\n",
      "  0%|          | 102/14893000 [06:44<16406:35:27,  3.97s/it]\u001b[A\n",
      "  0%|          | 103/14893000 [06:48<16348:16:33,  3.95s/it]\u001b[A\n",
      "  0%|          | 104/14893000 [06:52<16315:05:33,  3.94s/it]\u001b[A\n",
      "  0%|          | 105/14893000 [06:56<16464:46:18,  3.98s/it]\u001b[A\n",
      "  0%|          | 106/14893000 [07:00<16398:49:07,  3.96s/it]\u001b[A\n",
      "  0%|          | 107/14893000 [07:04<16362:59:35,  3.96s/it]\u001b[A\n",
      "  0%|          | 108/14893000 [07:08<16316:37:53,  3.94s/it]\u001b[A\n",
      "  0%|          | 109/14893000 [07:11<16085:24:11,  3.89s/it]\u001b[A\n",
      "  0%|          | 110/14893000 [07:15<15913:54:12,  3.85s/it]\u001b[A"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c6fdf-68ae-41e5-bf0d-da959e263ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb1a693-7227-49c8-af2c-8b99e1bf3bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcea2610-72c9-44b1-863f-4f6e2cb2f818",
   "metadata": {},
   "source": [
    "### captioning unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0fb4c2-58e5-445e-852f-007cfeebeba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    cfg = Config()\n",
    "    \n",
    "    accelerator = Accelerator(gradient_accumulation_steps=cfg.gradient_accumulation_steps)\n",
    "    make_dir(cfg.output_dir)\n",
    "    make_dir(cfg.generated_dir)\n",
    "    wandb_init(cfg)\n",
    "    \n",
    "    #compression_model, lm = build_model(cfg)\n",
    "    model = AudioProcessing(cfg)\n",
    "    \n",
    "    audio_dataset = AudioDataset(cfg, train=True) \n",
    "    eval_dataset = AudioDataset(cfg, train=False)\n",
    "    test_dataset = TestDataset(cfg)\n",
    "\n",
    "    audio_dataloader = DataLoader(audio_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=12)\n",
    "    eval_dataloader = DataLoader(eval_dataset, batch_size=cfg.eval_batch_size, shuffle=False, num_workers=4)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "    optimizer_parameters = [param for param in model.lm.parameters() if param.requires_grad]\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        optimizer_parameters, lr=cfg.learning_rate,\n",
    "        betas=(cfg.adam_beta1, cfg.adam_beta2),\n",
    "        weight_decay=cfg.adam_weight_decay,\n",
    "        eps=cfg.adam_epsilon,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    num_update_steps_per_epoch = math.ceil(len(audio_dataloader) / cfg.gradient_accumulation_steps)\n",
    "    if cfg.max_train_steps is None:\n",
    "      cfg.max_train_steps = cfg.num_train_epochs * num_update_steps_per_epoch\n",
    "    \n",
    "    lr_scheduler = get_scheduler(\n",
    "          name=cfg.lr_scheduler_type,\n",
    "          optimizer=optimizer,\n",
    "          num_warmup_steps=cfg.num_warmup_steps * cfg.gradient_accumulation_steps,\n",
    "          num_training_steps=cfg.max_train_steps * cfg.gradient_accumulation_steps,\n",
    "      )\n",
    "\n",
    "\n",
    "    audio_dataloader, eval_dataloader, model, optimizer, lr_scheduler = accelerator.prepare(\n",
    "        audio_dataloader, eval_dataloader, model, optimizer, lr_scheduler\n",
    "    )\n",
    "\n",
    "    starting_epoch, completed_steps, best_loss = 0, 0, np.inf\n",
    "    progress_bar = tqdm(range(cfg.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "    \n",
    "    for epoch in range(starting_epoch, cfg.num_train_epochs):\n",
    "        print(f\"-------------------EPOCH{epoch}-------------------------\" )\n",
    "        total_loss, total_val_loss = 0, 0\n",
    "        model.train()\n",
    "        for batch_idx, (wav, descriptions, lengths) in enumerate(audio_dataloader):\n",
    "          with accelerator.accumulate(model):\n",
    "              loss = model(wav, descriptions, lengths)\n",
    "              ppl =  torch.exp(loss)\n",
    "              total_loss += loss.detach().float()\n",
    "              accelerator.backward(loss)     \n",
    "              optimizer.step()\n",
    "              lr_scheduler.step()\n",
    "              optimizer.zero_grad()\n",
    "              \n",
    "          if accelerator.sync_gradients:\n",
    "              progress_bar.update(1)\n",
    "              completed_steps += 1\n",
    "            \n",
    "        model.eval()\n",
    "        for batch_idx, (wav, descriptions, lengths) in enumerate(eval_dataloader):\n",
    "              loss = model(wav, descriptions, lengths)\n",
    "              total_val_loss += loss  \n",
    "    \n",
    "        if accelerator.is_main_process:         \n",
    "            result = {}\n",
    "            result[\"epoch\"] = epoch + 1,\n",
    "            result[\"step\"] = completed_steps\n",
    "            result[\"train_loss\"] = round(total_loss.item()/len(audio_dataloader), 4)\n",
    "            result[\"valid_loss\"] = round(total_val_loss.item()/len(eval_dataloader), 4)\n",
    "            \n",
    "            wandb.log(result)\n",
    "            result_string = \"Epoch: {}, Loss Train: {}, Valid: {}\\n\".format(epoch + 1, result[\"train_loss\"], result[\"valid_loss\"])    \n",
    "            accelerator.print(result_string) \n",
    "            best_loss = save_checkpoint(cfg, model, result, best_loss, epoch)\n",
    "            \n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            for test_step, batch in enumerate(test_dataloader):\n",
    "                gen_audio = unwrapped_model.inference(batch)\n",
    "                audio_filename = f\"epoch_{epoch}_{test_step}.wav\"\n",
    "                unwrapped_model.save_audio(gen_audio, audio_filename, cfg)\n",
    "             \n",
    "    #wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b5a51e4-87db-475e-be27-5550ac28e8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:wfdvdk2e) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.014 MB of 0.014 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>▁█</td></tr><tr><td>train_loss</td><td>█▁</td></tr><tr><td>valid_loss</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>92</td></tr><tr><td>train_loss</td><td>2.151</td></tr><tr><td>valid_loss</td><td>2.4137</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">driven-dragon-52</strong> at: <a href='https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1/runs/wfdvdk2e' target=\"_blank\">https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1/runs/wfdvdk2e</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231214_081454-wfdvdk2e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:wfdvdk2e). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c8a20207c844f481968d9b27df390a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112272594538, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20231214_081933-pkdbo0ih</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1/runs/pkdbo0ih' target=\"_blank\">rural-firebrand-53</a></strong> to <a href='https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1' target=\"_blank\">https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1/runs/pkdbo0ih' target=\"_blank\">https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1/runs/pkdbo0ih</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "\n",
      "  0%|          | 0/46000 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------EPOCH0-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 1/46000 [00:03<40:52:21,  3.20s/it]\u001b[A\n",
      "  0%|          | 2/46000 [00:03<20:06:14,  1.57s/it]\u001b[A\n",
      "  0%|          | 3/46000 [00:04<13:44:09,  1.08s/it]\u001b[A\n",
      "  0%|          | 4/46000 [00:04<10:43:04,  1.19it/s]\u001b[A\n",
      "  0%|          | 5/46000 [00:05<9:00:29,  1.42it/s] \u001b[A\n",
      "  0%|          | 6/46000 [00:05<8:01:04,  1.59it/s]\u001b[A\n",
      "  0%|          | 7/46000 [00:06<7:22:41,  1.73it/s]\u001b[A\n",
      "  0%|          | 8/46000 [00:06<6:57:42,  1.84it/s]\u001b[A\n",
      "  0%|          | 9/46000 [00:06<6:41:39,  1.91it/s]\u001b[A\n",
      "  0%|          | 10/46000 [00:07<6:31:49,  1.96it/s]\u001b[A\n",
      "  0%|          | 11/46000 [00:07<6:23:56,  2.00it/s]\u001b[A\n",
      "  0%|          | 12/46000 [00:08<6:19:03,  2.02it/s]\u001b[A\n",
      "  0%|          | 13/46000 [00:08<6:16:05,  2.04it/s]\u001b[A\n",
      "  0%|          | 14/46000 [00:09<6:13:20,  2.05it/s]\u001b[A\n",
      "  0%|          | 15/46000 [00:09<6:12:47,  2.06it/s]\u001b[A\n",
      "  0%|          | 16/46000 [00:10<6:09:37,  2.07it/s]\u001b[A\n",
      "  0%|          | 17/46000 [00:10<6:10:40,  2.07it/s]\u001b[A\n",
      "  0%|          | 18/46000 [00:11<6:09:34,  2.07it/s]\u001b[A\n",
      "  0%|          | 19/46000 [00:11<6:08:36,  2.08it/s]\u001b[A\n",
      "  0%|          | 20/46000 [00:12<6:07:15,  2.09it/s]\u001b[A\n",
      "  0%|          | 21/46000 [00:12<6:07:10,  2.09it/s]\u001b[A\n",
      "  0%|          | 22/46000 [00:13<6:05:50,  2.09it/s]\u001b[A\n",
      "  0%|          | 23/46000 [00:13<6:06:08,  2.09it/s]\u001b[A\n",
      "  0%|          | 24/46000 [00:14<6:07:11,  2.09it/s]\u001b[A\n",
      "  0%|          | 25/46000 [00:14<6:06:24,  2.09it/s]\u001b[A\n",
      "  0%|          | 26/46000 [00:15<6:06:42,  2.09it/s]\u001b[A\n",
      "  0%|          | 27/46000 [00:15<6:05:53,  2.09it/s]\u001b[A\n",
      "  0%|          | 28/46000 [00:16<6:06:08,  2.09it/s]\u001b[A\n",
      "  0%|          | 29/46000 [00:16<6:07:14,  2.09it/s]\u001b[A\n",
      "  0%|          | 30/46000 [00:17<6:07:23,  2.09it/s]\u001b[A\n",
      "  0%|          | 31/46000 [00:17<6:06:47,  2.09it/s]\u001b[A\n",
      "  0%|          | 32/46000 [00:17<6:07:00,  2.09it/s]\u001b[A\n",
      "  0%|          | 33/46000 [00:18<6:06:08,  2.09it/s]\u001b[A\n",
      "  0%|          | 34/46000 [00:18<6:06:19,  2.09it/s]\u001b[A\n",
      "  0%|          | 35/46000 [00:19<6:05:32,  2.10it/s]\u001b[A\n",
      "  0%|          | 36/46000 [00:19<6:06:46,  2.09it/s]\u001b[A\n",
      "  0%|          | 37/46000 [00:20<6:06:11,  2.09it/s]\u001b[A\n",
      "  0%|          | 38/46000 [00:20<6:07:21,  2.09it/s]\u001b[A\n",
      "  0%|          | 39/46000 [00:21<6:06:47,  2.09it/s]\u001b[A\n",
      "  0%|          | 40/46000 [00:21<6:04:57,  2.10it/s]\u001b[A\n",
      "  0%|          | 41/46000 [00:22<6:05:16,  2.10it/s]\u001b[A\n",
      "  0%|          | 42/46000 [00:22<6:05:21,  2.10it/s]\u001b[A\n",
      "  0%|          | 43/46000 [00:23<6:05:21,  2.10it/s]\u001b[A\n",
      "  0%|          | 44/46000 [00:23<6:04:09,  2.10it/s]\u001b[A\n",
      "  0%|          | 45/46000 [00:24<6:06:02,  2.09it/s]\u001b[A\n",
      "  0%|          | 46/46000 [00:24<6:10:26,  2.07it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss Train: 2.457, Valid: 2.4067\n",
      "\n",
      "-------------------EPOCH1-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 47/46000 [01:00<140:45:57, 11.03s/it]\u001b[A\n",
      "  0%|          | 48/46000 [01:00<100:20:00,  7.86s/it]\u001b[A\n",
      "  0%|          | 49/46000 [01:01<72:03:39,  5.65s/it] \u001b[A\n",
      "  0%|          | 50/46000 [01:01<52:17:27,  4.10s/it]\u001b[A\n",
      "  0%|          | 51/46000 [01:02<38:27:35,  3.01s/it]\u001b[A\n",
      "  0%|          | 52/46000 [01:02<28:46:16,  2.25s/it]\u001b[A\n",
      "  0%|          | 53/46000 [01:03<21:59:09,  1.72s/it]\u001b[A\n",
      "  0%|          | 54/46000 [01:03<17:13:31,  1.35s/it]\u001b[A\n",
      "  0%|          | 55/46000 [01:04<13:53:30,  1.09s/it]\u001b[A\n",
      "  0%|          | 56/46000 [01:04<11:34:12,  1.10it/s]\u001b[A\n",
      "  0%|          | 57/46000 [01:05<9:56:37,  1.28it/s] \u001b[A\n",
      "  0%|          | 58/46000 [01:05<8:49:51,  1.45it/s]\u001b[A\n",
      "  0%|          | 59/46000 [01:06<8:01:54,  1.59it/s]\u001b[A\n",
      "  0%|          | 60/46000 [01:06<7:27:37,  1.71it/s]\u001b[A\n",
      "  0%|          | 61/46000 [01:07<7:03:48,  1.81it/s]\u001b[A\n",
      "  0%|          | 62/46000 [01:07<6:47:33,  1.88it/s]\u001b[A\n",
      "  0%|          | 63/46000 [01:08<6:35:16,  1.94it/s]\u001b[A\n",
      "  0%|          | 64/46000 [01:08<6:28:24,  1.97it/s]\u001b[A\n",
      "  0%|          | 65/46000 [01:08<6:22:05,  2.00it/s]\u001b[A\n",
      "  0%|          | 66/46000 [01:09<6:17:55,  2.03it/s]\u001b[A\n",
      "  0%|          | 67/46000 [01:09<6:13:49,  2.05it/s]\u001b[A\n",
      "  0%|          | 68/46000 [01:10<6:14:20,  2.04it/s]\u001b[A\n",
      "  0%|          | 69/46000 [01:10<6:11:11,  2.06it/s]\u001b[A\n",
      "  0%|          | 70/46000 [01:11<6:10:57,  2.06it/s]\u001b[A\n",
      "  0%|          | 71/46000 [01:11<6:09:44,  2.07it/s]\u001b[A\n",
      "  0%|          | 72/46000 [01:12<6:10:03,  2.07it/s]\u001b[A\n",
      "  0%|          | 73/46000 [01:12<6:08:00,  2.08it/s]\u001b[A\n",
      "  0%|          | 74/46000 [01:13<6:06:44,  2.09it/s]\u001b[A\n",
      "  0%|          | 75/46000 [01:13<6:07:42,  2.08it/s]\u001b[A\n",
      "  0%|          | 76/46000 [01:14<6:08:14,  2.08it/s]\u001b[A\n",
      "  0%|          | 77/46000 [01:14<6:08:25,  2.08it/s]\u001b[A\n",
      "  0%|          | 78/46000 [01:15<6:07:38,  2.08it/s]\u001b[A\n",
      "  0%|          | 79/46000 [01:15<6:08:26,  2.08it/s]\u001b[A\n",
      "  0%|          | 80/46000 [01:16<6:08:47,  2.08it/s]\u001b[A\n",
      "  0%|          | 81/46000 [01:16<6:08:38,  2.08it/s]\u001b[A\n",
      "  0%|          | 82/46000 [01:17<6:08:01,  2.08it/s]\u001b[A\n",
      "  0%|          | 83/46000 [01:17<6:06:59,  2.09it/s]\u001b[A\n",
      "  0%|          | 84/46000 [01:18<6:07:29,  2.08it/s]\u001b[A\n",
      "  0%|          | 85/46000 [01:18<6:08:59,  2.07it/s]\u001b[A\n",
      "  0%|          | 86/46000 [01:19<6:08:12,  2.08it/s]\u001b[A\n",
      "  0%|          | 87/46000 [01:19<6:08:41,  2.08it/s]\u001b[A\n",
      "  0%|          | 88/46000 [01:20<6:06:58,  2.09it/s]\u001b[A\n",
      "  0%|          | 89/46000 [01:20<6:07:56,  2.08it/s]\u001b[A\n",
      "  0%|          | 90/46000 [01:21<6:08:33,  2.08it/s]\u001b[A\n",
      "  0%|          | 91/46000 [01:21<6:09:23,  2.07it/s]\u001b[A\n",
      "  0%|          | 92/46000 [01:21<6:00:00,  2.13it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss Train: 2.151, Valid: 2.4137\n",
      "\n",
      "-------------------EPOCH2-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 93/46000 [01:57<139:14:47, 10.92s/it]\u001b[A\n",
      "  0%|          | 94/46000 [01:57<99:18:36,  7.79s/it] \u001b[A\n",
      "  0%|          | 95/46000 [01:58<71:21:33,  5.60s/it]\u001b[A\n",
      "  0%|          | 96/46000 [01:58<51:45:49,  4.06s/it]\u001b[A\n",
      "  0%|          | 97/46000 [01:59<38:04:44,  2.99s/it]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#main()\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m notebook_launcher\n\u001b[0;32m----> 3\u001b[0m \u001b[43mnotebook_launcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/launchers.py:207\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLaunching training on CPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 207\u001b[0m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 112\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (wav, descriptions, lengths) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(audio_dataloader):\n\u001b[1;32m    111\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m accelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m--> 112\u001b[0m       loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescriptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m       ppl \u001b[38;5;241m=\u001b[39m  torch\u001b[38;5;241m.\u001b[39mexp(loss)\n\u001b[1;32m    114\u001b[0m       total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/audiomodel.py:32\u001b[0m, in \u001b[0;36mAudioProcessing.forward\u001b[0;34m(self, wav, descriptions, lengths)\u001b[0m\n\u001b[1;32m     26\u001b[0m audio_tokens, padding_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_process_audio_tokenizer(audio_tokens, audio_lengths\u001b[38;5;241m=\u001b[39mlengths)\n\u001b[1;32m     28\u001b[0m attributes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     29\u001b[0m     ConditioningAttributes(text\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m: description})\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m description \u001b[38;5;129;01min\u001b[39;00m descriptions]\n\u001b[0;32m---> 32\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondition_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     33\u001b[0m logits \u001b[38;5;241m=\u001b[39m model_output\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     35\u001b[0m mask \u001b[38;5;241m=\u001b[39m padding_mask \u001b[38;5;241m&\u001b[39m model_output\u001b[38;5;241m.\u001b[39mmask\n",
      "File \u001b[0;32m/workspace/audiocraft/models/lm.py:297\u001b[0m, in \u001b[0;36mLMModel.compute_predictions\u001b[0;34m(self, codes, conditions, condition_tensors)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# apply model on pattern sequence\u001b[39;00m\n\u001b[1;32m    296\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fsdp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fsdp\n\u001b[0;32m--> 297\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_codes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondition_tensors\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, K, S, card]\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# map back the logits on pattern sequence to logits on original codes: [B, K, S, card] -> [B, K, T, card]\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# and provide the corresponding mask over invalid positions of tokens\u001b[39;00m\n\u001b[1;32m    300\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# [B, card, K, S]\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/audiocraft/models/lm.py:253\u001b[0m, in \u001b[0;36mLMModel.forward\u001b[0;34m(self, sequence, conditions, condition_tensors)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conditions, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt pass both conditions and condition_tensors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m input_, cross_attention_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuser(input_, condition_tensors)\n\u001b[0;32m--> 253\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcross_attention_src\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_norm:\n\u001b[1;32m    255\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_norm(out)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/audiocraft/modules/transformer.py:698\u001b[0m, in \u001b[0;36mStreamingTransformer.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    695\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_scale \u001b[38;5;241m*\u001b[39m pos_emb\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 698\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_streaming:\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moffsets\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m offsets \u001b[38;5;241m+\u001b[39m T\n",
      "File \u001b[0;32m/workspace/audiocraft/modules/transformer.py:655\u001b[0m, in \u001b[0;36mStreamingTransformer._apply_layer\u001b[0;34m(self, layer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointing\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_checkpoint(layer, \u001b[38;5;241m*\u001b[39margs, use_reentrant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/audiocraft/modules/transformer.py:550\u001b[0m, in \u001b[0;36mStreamingTransformerLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, cross_attention_src)\u001b[0m\n\u001b[1;32m    547\u001b[0m x \u001b[38;5;241m=\u001b[39m src\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_first:\n\u001b[1;32m    549\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_scale_1(\n\u001b[0;32m--> 550\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cross_attention_src \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    552\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_scale_cross(\n\u001b[1;32m    553\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cross_attention_block(\n\u001b[1;32m    554\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_cross(x), cross_attention_src))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:715\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[1;32m    714\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 715\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/audiocraft/modules/transformer.py:372\u001b[0m, in \u001b[0;36mStreamingMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m    370\u001b[0m         bound_layout \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb t p h d\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m     packed \u001b[38;5;241m=\u001b[39m rearrange(projected, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb t (p h d) -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbound_layout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, h\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads)\n\u001b[0;32m--> 372\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    374\u001b[0m     embed_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xformers/ops/unbind.py:117\u001b[0m, in \u001b[0;36munbind\u001b[0;34m(x, dim)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munbind\u001b[39m(x: torch\u001b[38;5;241m.\u001b[39mTensor, dim: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    Does exactly the same as :attr:`torch.unbind` for the forward.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    In backward, avoids a :attr:`torch.cat` if the gradients\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m    are already multiple views of the same storage\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_Unbind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:536\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_are_functorch_transforms_active\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    537\u001b[0m         \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m         args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m    539\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#main()\n",
    "from accelerate import notebook_launcher\n",
    "notebook_launcher(main, num_processes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f65cfa-85aa-4bc7-b7a7-770a50dd7df7",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "325e8d99-5bcc-4503-b82c-00fb9c1f1204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "import tqdm\n",
    "import json\n",
    "import typing as tp\n",
    "import pandas as pd\n",
    "import glob2\n",
    "import math\n",
    "import omegaconf\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.sample_rate = 16000\n",
    "        self.is_training = True\n",
    "        self.duration = 1\n",
    "        self.total_updates = 10000\n",
    "        self.eval_steps = 4\n",
    "        self.device = 'cuda'  # 'cuda' 또는 'cpu'\n",
    "        self.batch_size = 24\n",
    "        self.eval_batch_size = 4\n",
    "        self.train_data_path = \"/workspace/train_dataset.csv\"\n",
    "        self.eval_data_path = \"/workspace/eval_dataset.csv\"\n",
    "        self.output_dir = \"./output_dir\"\n",
    "        self.checkpointing_steps = \"best\"\n",
    "        self.save_every = 10\n",
    "        self.with_tracking = False\n",
    "        self.text_encoder_name = None  # 나중에 설정\n",
    "        self.snr_gamma = 5.0\n",
    "        self.freeze_text_encoder = True\n",
    "        self.uncondition = False\n",
    "        self.learning_rate = 3e-5\n",
    "        self.adam_beta1 = 0.9\n",
    "        self.adam_beta2 = 0.999\n",
    "        self.adam_weight_decay = 1e-2\n",
    "        self.adam_epsilon = 1e-08\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.num_train_epochs = 1000\n",
    "        self.num_warmup_steps = 0\n",
    "        self.max_train_steps = None\n",
    "        self.lr_scheduler_type = \"linear\"\n",
    "        self.resume_from_checkpoint = None #\"/workspace/output_dir_batch48/last/\" #None\n",
    "        self.wandb_project_name = \"audiogen-finetune-init-test1\"\n",
    "        self.wandb_id = None #\"earnest-pond-52\"\n",
    "        self.resume_epoch = 0 #127\n",
    "        self.dtype = \"float32\"\n",
    "        \n",
    "        self.update_audiocraft_config()\n",
    "        \n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            # 기존 속성에 값 할당하거나 새 속성 생성\n",
    "            if not hasattr(self, key):\n",
    "                print(key)\n",
    "            setattr(self, key, value)\n",
    "            \n",
    "    def update_audiocraft_config(self):\n",
    "        self.solver = None\n",
    "        self.fsdp = None\n",
    "        self.profiler = None\n",
    "        self.deadlock = None\n",
    "        self.dataset = None\n",
    "        self.checkpoint = None\n",
    "        self.generate = None\n",
    "        self.evaluate = None\n",
    "        self.optim = None\n",
    "        self.schedule = None\n",
    "        self.default = None\n",
    "        self.defaults = None\n",
    "        self.autocast = None\n",
    "        self.autocast_dtype = None\n",
    "\n",
    "        self.compression_model_checkpoint = None\n",
    "        self.channels = None\n",
    "        self.logging = None\n",
    "        self.lm_model = None\n",
    "        self.codebooks_pattern = None\n",
    "        self.transformer_lm = None\n",
    "        self.classifier_free_guidance = None\n",
    "        self.attribute_dropout = None\n",
    "        self.fuser = None\n",
    "        self.conditioners = None\n",
    "        self.datasource = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "463fbdc2-a1ab-405d-b417-a9dfe15418fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_paths, device, target_sample_rate=44100, duration=3):\n",
    "        import pandas as pd\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_files_list (list): List of paths to audio files.\n",
    "            target_sample_rate (int): The sample rate to which audio should be resampled.\n",
    "            frame_length (int): The frame length for slicing or padding audio.\n",
    "        \"\"\"\n",
    "        self.audio_paths = audio_paths\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.duration = duration\n",
    "        self.device = device\n",
    "\n",
    "        self.df = pd.read_csv(self.audio_paths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        data = self.df.iloc[idx] #self.audio_files_list[idx]\n",
    "        audio_path = data['sliced_audio_path']\n",
    "        description = data['description']\n",
    "\n",
    "        # Load audio signal file\n",
    "        from audiotools import AudioSignal\n",
    "        wav = AudioSignal(audio_path)\n",
    "        length = wav.signal_length\n",
    "\n",
    "        # Encode audio signal as one long file\n",
    "        wav.to_mono()\n",
    "        wav.resample(self.target_sample_rate)\n",
    "\n",
    "        if wav.duration < self.duration:\n",
    "          pad_len = int(self.duration * self.target_sample_rate) - wav.signal_length\n",
    "          wav.zero_pad(0, pad_len)\n",
    "        elif wav.duration > self.duration:\n",
    "          wav.truncate_samples(self.duration * self.target_sample_rate)\n",
    "\n",
    "\n",
    "        return wav.audio_data.squeeze(1), description, length\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, prompts):\n",
    "\n",
    "        self.prompts = prompts\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.prompts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c36c688-9e32-408e-b60f-01811aa9fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.solvers import base, builders\n",
    "from audiocraft.solvers.compression import CompressionSolver\n",
    "from audiocraft import metrics as eval_metrics\n",
    "from audiocraft import models\n",
    "from audiocraft.data.audio_utils import normalize_audio\n",
    "from audiocraft.modules.conditioners import JointEmbedCondition, SegmentWithAttributes, WavCondition, ConditioningAttributes\n",
    "from audiocraft.utils.utils import get_dataset_from_loader, is_jsonable, warn_once\n",
    "from audiocraft.models.loaders import load_compression_model, load_lm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2516970a-a1ee-4324-81cc-17167ede6e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioProcessing(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()  # 부모 클래스 초기화 호출\n",
    "        self.cfg = cfg\n",
    "        self.compression_model, self.lm  = self.build_model(self.cfg)\n",
    "        self.to_float32()\n",
    "        self.freeze_layers()\n",
    "\n",
    "    def forward(self, wav, descriptions, lengths):\n",
    "        from audiocraft.modules.conditioners import JointEmbedCondition, SegmentWithAttributes, WavCondition, ConditioningAttributes\n",
    "        audio_tokens = self.process_audio_tokenizer(wav.to(self.cfg.device))\n",
    "        audio_tokens, padding_mask = self.post_process_audio_tokenizer(audio_tokens, audio_lengths=lengths)\n",
    "        \n",
    "        attributes = [\n",
    "            ConditioningAttributes(text={'description': description})\n",
    "            for description in descriptions]\n",
    "    \n",
    "        model_output = self.lm.compute_predictions(audio_tokens, conditions=attributes, condition_tensors=None)  # type: ignore\n",
    "        logits = model_output.logits\n",
    "    \n",
    "        mask = padding_mask & model_output.mask\n",
    "        ce, ce_per_codebook = self.compute_cross_entropy(logits, audio_tokens, mask)\n",
    "        \n",
    "        return ce\n",
    "\n",
    "    def build_model(self, cfg):\n",
    "        from audiocraft.models.loaders import load_compression_model, load_lm_model\n",
    "        \"\"\"Instantiate models and optimizer.\"\"\"\n",
    "        \n",
    "        compression_model = load_compression_model('facebook/audiogen-medium', device=cfg.device)\n",
    "        lm = load_lm_model('facebook/audiogen-medium', device=cfg.device)\n",
    "    \n",
    "        return compression_model, lm\n",
    "\n",
    "    def process_audio_tokenizer(self, wav):\n",
    "        with torch.no_grad():\n",
    "            audio_tokens, scale = self.compression_model.encode(wav)\n",
    "        return audio_tokens\n",
    "\n",
    "    def post_process_audio_tokenizer(self, audio_tokens, audio_lengths=None):\n",
    "        padding_mask = torch.ones_like(audio_tokens, dtype=torch.bool, device=audio_tokens.device)\n",
    "\n",
    "        audio_tokens = audio_tokens.clone()\n",
    "        padding_mask = padding_mask.clone()\n",
    "        token_sample_rate = self.compression_model.frame_rate\n",
    "        B, K, T_s = audio_tokens.shape\n",
    "        for i in range(B):\n",
    "            valid_tokens = math.floor(audio_lengths[i] / self.cfg.sample_rate * token_sample_rate)\n",
    "            audio_tokens[i, :, valid_tokens:] = self.lm.special_token_id\n",
    "            padding_mask[i, :, valid_tokens:] = 0\n",
    "\n",
    "        return audio_tokens, padding_mask\n",
    "\n",
    "    def compute_cross_entropy(self, logits: torch.Tensor, targets: torch.Tensor, mask: torch.Tensor) -> tp.Tuple[torch.Tensor, tp.List[torch.Tensor]]:\n",
    "\n",
    "        B, K, T = targets.shape\n",
    "        assert logits.shape[:-1] == targets.shape\n",
    "        assert mask.shape == targets.shape\n",
    "        ce = torch.zeros([], device=targets.device)\n",
    "        ce_per_codebook: tp.List[torch.Tensor] = []\n",
    "        for k in range(K):\n",
    "            logits_k = logits[:, k, ...].contiguous().view(-1, logits.size(-1))  # [B x T, card]\n",
    "            targets_k = targets[:, k, ...].contiguous().view(-1)  # [B x T]\n",
    "            mask_k = mask[:, k, ...].contiguous().view(-1)  # [B x T]\n",
    "            ce_targets = targets_k[mask_k]\n",
    "            ce_logits = logits_k[mask_k]\n",
    "            q_ce = F.cross_entropy(ce_logits, ce_targets)\n",
    "            ce += q_ce\n",
    "            ce_per_codebook.append(q_ce.detach())\n",
    "        # average cross entropy across codebooks\n",
    "        ce = ce / K\n",
    "        return ce, ce_per_codebook\n",
    "\n",
    "    def audio_generate(self, condition_tensors, gen_duration=5):\n",
    "        with torch.no_grad():\n",
    "            total_gen_len = math.ceil(gen_duration * self.compression_model.frame_rate)\n",
    "            gen_tokens = self.lm.generate(\n",
    "                None, condition_tensors, max_gen_len=total_gen_len,\n",
    "                num_samples=1)\n",
    "            gen_audio = self.compression_model.decode(gen_tokens, None)\n",
    "\n",
    "        return gen_tokens, gen_audio\n",
    "\n",
    "    def inference(self, descriptions):\n",
    "        #with torch.no_grad():\n",
    "        from audiocraft.modules.conditioners import JointEmbedCondition, SegmentWithAttributes, WavCondition, ConditioningAttributes\n",
    "        attributes = [\n",
    "        ConditioningAttributes(text={'description': description})\n",
    "        for description in descriptions]\n",
    "        _, gen_audio = self.audio_generate(attributes, gen_duration=self.cfg.duration)\n",
    "        \n",
    "        return gen_audio\n",
    "\n",
    "    def to_float32(self):\n",
    "        # 모든 가중치를 FP32로 변환\n",
    "        for param in self.lm.parameters():\n",
    "            param.data = param.data.to(dtype=torch.float32)\n",
    "\n",
    "    def freeze_layers(self, train_layers=12):\n",
    "        for param in self.lm.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        if train_layers > 0 :\n",
    "            num_layers = len(self.lm.transformer.layers)\n",
    "            \n",
    "            for i in range(num_layers - train_layers, num_layers):\n",
    "                for param in self.lm.transformer.layers[i].parameters():\n",
    "                    param.requires_grad = True\n",
    "                    \n",
    "            for name, param in self.lm.named_parameters():\n",
    "                if 'out_norm' in name or 'linears' in name:\n",
    "                    param.requires_grad = True\n",
    "                \n",
    "            \n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "102033ad-06dd-418f-8a63-5dcb73587ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract discrete codes from EnCodec\n",
    "def process_audio_tokenizer(wav):\n",
    "  with torch.no_grad():\n",
    "      audio_tokens, scale = compression_model.encode(wav)\n",
    "  return audio_tokens\n",
    "\n",
    "def post_process_audio_tokenizer(audio_tokens, audio_lengths=None, cfg=None):\n",
    "  padding_mask = torch.ones_like(audio_tokens, dtype=torch.bool, device=audio_tokens.device)\n",
    "  # replace encodec tokens from padded audio with special_token_id\n",
    "\n",
    "  audio_tokens = audio_tokens.clone()\n",
    "  padding_mask = padding_mask.clone()\n",
    "  token_sample_rate = compression_model.frame_rate\n",
    "  B, K, T_s = audio_tokens.shape\n",
    "  for i in range(B):\n",
    "      # take the last token generated from actual audio frames (non-padded audio)\n",
    "      #math.floor(float(n_frames[i]) / sr[i] * token_sample_rate)\n",
    "      valid_tokens = math.floor(audio_lengths[i] / cfg.sample_rate * token_sample_rate)\n",
    "      audio_tokens[i, :, valid_tokens:] = lm.special_token_id\n",
    "      padding_mask[i, :, valid_tokens:] = 0\n",
    "\n",
    "  return audio_tokens, padding_mask\n",
    "\n",
    "def _compute_cross_entropy(\n",
    "      logits: torch.Tensor, targets: torch.Tensor, mask: torch.Tensor\n",
    "    ) -> tp.Tuple[torch.Tensor, tp.List[torch.Tensor]]:\n",
    "        \"\"\"Compute cross entropy between multi-codebook targets and model's logits.\n",
    "        The cross entropy is computed per codebook to provide codebook-level cross entropy.\n",
    "        Valid timesteps for each of the codebook are pulled from the mask, where invalid\n",
    "        timesteps are set to 0.\n",
    "\n",
    "        Args:\n",
    "            logits (torch.Tensor): Model's logits of shape [B, K, T, card].\n",
    "            targets (torch.Tensor): Target codes, of shape [B, K, T].\n",
    "            mask (torch.Tensor): Mask for valid target codes, of shape [B, K, T].\n",
    "        Returns:\n",
    "            ce (torch.Tensor): Cross entropy averaged over the codebooks\n",
    "            ce_per_codebook (list of torch.Tensor): Cross entropy per codebook (detached).\n",
    "        \"\"\"\n",
    "        B, K, T = targets.shape\n",
    "        assert logits.shape[:-1] == targets.shape\n",
    "        assert mask.shape == targets.shape\n",
    "        ce = torch.zeros([], device=targets.device)\n",
    "        ce_per_codebook: tp.List[torch.Tensor] = []\n",
    "        for k in range(K):\n",
    "            logits_k = logits[:, k, ...].contiguous().view(-1, logits.size(-1))  # [B x T, card]\n",
    "            targets_k = targets[:, k, ...].contiguous().view(-1)  # [B x T]\n",
    "            mask_k = mask[:, k, ...].contiguous().view(-1)  # [B x T]\n",
    "            ce_targets = targets_k[mask_k]\n",
    "            ce_logits = logits_k[mask_k]\n",
    "            q_ce = F.cross_entropy(ce_logits, ce_targets)\n",
    "            ce += q_ce\n",
    "            ce_per_codebook.append(q_ce.detach())\n",
    "        # average cross entropy across codebooks\n",
    "        ce = ce / K\n",
    "        return ce, ce_per_codebook\n",
    "\n",
    "def audio_generate(condition_tensors, gen_duration=5):\n",
    "    with torch.no_grad():\n",
    "      total_gen_len = math.ceil(gen_duration * compression_model.frame_rate)\n",
    "      gen_tokens = lm.generate(\n",
    "          None, condition_tensors, max_gen_len=total_gen_len,\n",
    "          num_samples=1)\n",
    "      gen_audio = compression_model.decode(gen_tokens, None)\n",
    "\n",
    "    return gen_tokens, gen_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b264f61-dfaa-40a1-9510-328b9f9049c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav, text, length = next(iter(audio_dataloader))\n",
    "audio_tokens = process_audio_tokenizer(wav.to(cfg.device))\n",
    "print(\"Wav shape: \", wav.shape)\n",
    "print(\"Token shape: \", audio_tokens.shape)\n",
    "post_process_audio_tokenizer(audio_tokens, audio_lengths=length)\n",
    "\n",
    "import torch\n",
    "\n",
    "# 가정: model이라는 이름의 PyTorch 모델이 이미 정의되어 있음\n",
    "for name, param in lm.named_parameters():\n",
    "    print(f\"Layer {name} has data type {param.dtype}\")\n",
    "    #break  # 모든 레이어를 표시하지 않고 첫 레이어에서 루프 중단\n",
    "\n",
    "def check_requires_grad(model: torch.nn.Module):\n",
    "    for name, module in model.named_children():\n",
    "        for param_name, param in module.named_parameters():\n",
    "            print(f\"{name}.{param_name}: requires_grad = {param.requires_grad}\")\n",
    "\n",
    "# DAC 모델의 인스턴스를 생성한 후에 아래와 같이 사용할 수 있습니다:\n",
    "# dac_instance = DAC(...)\n",
    "check_requires_grad(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5283af14-dd57-406b-8834-58fb21dcc208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2b4a7e-1ca5-4749-84a3-c5543e669d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
