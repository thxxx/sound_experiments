{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d63d05d5-672d-45f6-bf90-0943c3b44b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.1.0+cu121 with CUDA 1201 (you have 2.1.0+cu118)\n",
      "    Python  3.10.13 (you have 3.10.12)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from transformers import get_scheduler\n",
    "\n",
    "from audiocraft.modules.conditioners import JointEmbedCondition, SegmentWithAttributes, WavCondition, ConditioningAttributes\n",
    "from config import Config\n",
    "from audiomodel import AudioProcessing\n",
    "from audiodataset_seperation import SeperationDataset, TestDataset\n",
    "\n",
    "def make_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def wandb_init(cfg):\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=cfg.wandb_project_name,\n",
    "        \n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "            \"learning_rate\": cfg.learning_rate,\n",
    "            \"dataset\": \"epidemic\",\n",
    "            \"epochs\": cfg.num_train_epochs,\n",
    "            \"batch_size\": cfg.batch_size,\n",
    "        }\n",
    "    )\n",
    "    \n",
    "def save_checkpoint(cfg, model, result, best_loss, epoch=0):\n",
    "    save_checkpoint = False\n",
    "    with open(\"{}/summary.jsonl\".format(cfg.output_dir), \"a\") as f:\n",
    "        f.write(json.dumps(result) + \"\\n\\n\")\n",
    "        \n",
    "    if result[\"valid_loss\"] < best_loss:\n",
    "      best_loss = result[\"valid_loss\"]\n",
    "      save_checkpoint = True\n",
    "      \n",
    "    # 모델 상태 저장\n",
    "    if save_checkpoint and cfg.checkpointing_steps == \"best\":\n",
    "        torch.save(model.state_dict(), os.path.join(cfg.output_dir, f\"best.pth\"))\n",
    "\n",
    "    #torch.save(model.state_dict(), os.path.join(cfg.output_dir, \"last.pth\"))\n",
    "    torch.save(model.state_dict(), os.path.join(cfg.output_dir, f\"{epoch}.pth\"))\n",
    "\n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c470cb93-e060-46d3-bfc5-7efaa6140634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(cfg):\n",
    "        from audiocraft.models.loaders import load_compression_model, load_lm_model\n",
    "        \"\"\"Instantiate models and optimizer.\"\"\"     \n",
    "        compression_model = load_compression_model('facebook/audiogen-medium', device=cfg.device)\n",
    "        lm = load_lm_model('facebook/audiogen-medium', device=cfg.device)\n",
    "        return compression_model, lm\n",
    "\n",
    "def process_audio_tokenizer(wav, compression_model):\n",
    "    \"\"\"\n",
    "    Get wav audio and return audio tokens\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        audio_tokens, scale = compression_model.encode(wav)\n",
    "    return audio_tokens\n",
    "\n",
    "def post_process_audio_tokenizer(audio_tokens, audio_lengths=None, compression_model=None, lm=None, cfg=None):\n",
    "    \"\"\"\n",
    "    For Masking\n",
    "    \"\"\"\n",
    "    padding_mask = torch.ones_like(audio_tokens, dtype=torch.bool, device=audio_tokens.device)\n",
    "    audio_tokens = audio_tokens.clone()\n",
    "    padding_mask = padding_mask.clone()\n",
    "    token_sample_rate = compression_model.frame_rate\n",
    "    # B : batch size\n",
    "    # K : codebook num\n",
    "    # T_s : duration * 50(임의 지정, encodec's frame rate)\n",
    "    B, K, T_s = audio_tokens.shape\n",
    "    \n",
    "    for i in range(B):\n",
    "        valid_tokens = math.floor(audio_lengths[i] / cfg.sample_rate * token_sample_rate)\n",
    "        audio_tokens[i, :, valid_tokens:] = lm.special_token_id # 2048이다.\n",
    "        padding_mask[i, :, valid_tokens:] = 0\n",
    "\n",
    "    return audio_tokens, padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39f6dca3-cfeb-464e-ba1b-9d940db48b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moptimizerai\u001b[0m (\u001b[33moptimizer_ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/audiogen-finetune/wandb/run-20240124_051657-my2hn2ny</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/optimizer_ai/seperation_first/runs/my2hn2ny' target=\"_blank\">royal-glitter-13</a></strong> to <a href='https://wandb.ai/optimizer_ai/seperation_first' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/optimizer_ai/seperation_first' target=\"_blank\">https://wandb.ai/optimizer_ai/seperation_first</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/optimizer_ai/seperation_first/runs/my2hn2ny' target=\"_blank\">https://wandb.ai/optimizer_ai/seperation_first/runs/my2hn2ny</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumed from local checkpoint: ./output_dir_finetune/best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2575000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "base_path = \"./csv_files/\"\n",
    "train_data_path = f\"{base_path}/train_dataset_epidemic_sub.csv\"\n",
    "eval_data_path = f\"{base_path}/eval_dataset_epidemic_sub.csv\"\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "cfg.update(train_data_path=train_data_path, eval_data_path=eval_data_path, batch_size=16)\n",
    "\n",
    "accelerator = Accelerator(gradient_accumulation_steps=cfg.gradient_accumulation_steps)\n",
    "cfg.update(device=accelerator.device)\n",
    "make_dir(cfg.output_dir)\n",
    "make_dir(cfg.generated_dir)\n",
    "if accelerator.is_main_process: \n",
    "    wandb_init(cfg)\n",
    "\n",
    "with accelerator.main_process_first():  \n",
    "    compression_model, lm = build_model(cfg)\n",
    "    audio_dataset = SeperationDataset(cfg, train=True)\n",
    "    eval_dataset = SeperationDataset(cfg, train=False)\n",
    "compression_model.eval()\n",
    "\n",
    "model = AudioProcessing(cfg, lm)\n",
    "test_dataset = TestDataset(cfg)\n",
    "\n",
    "audio_dataloader = DataLoader(audio_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=8)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=cfg.eval_batch_size, shuffle=False, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "# gradients for lm\n",
    "optimizer_parameters = [param for param in model.lm.parameters() if param.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    optimizer_parameters, \n",
    "    lr=cfg.learning_rate,\n",
    "    betas=(cfg.adam_beta1, cfg.adam_beta2),\n",
    "    weight_decay=cfg.adam_weight_decay,\n",
    "    eps=cfg.adam_epsilon,\n",
    ")\n",
    "\n",
    "num_update_steps_per_epoch = math.ceil(len(audio_dataloader) / cfg.gradient_accumulation_steps)\n",
    "if cfg.max_train_steps is None:\n",
    "  cfg.max_train_steps = cfg.num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "      name=cfg.lr_scheduler_type,\n",
    "      optimizer=optimizer,\n",
    "      num_warmup_steps=cfg.num_warmup_steps * cfg.gradient_accumulation_steps,\n",
    "      num_training_steps=cfg.max_train_steps * cfg.gradient_accumulation_steps,\n",
    "  )\n",
    "\n",
    "with accelerator.main_process_first():\n",
    "    if cfg.resume_from_checkpoint is not None:\n",
    "        accelerator.print(f\"Resumed from local checkpoint: {cfg.resume_from_checkpoint}\")\n",
    "        # model.load_state_dict(torch.load(cfg.resume_from_checkpoint, map_location=accelerator.device))\n",
    "        #accelerator.load_state(cfg.resume_from_checkpoint)\n",
    "\n",
    "\n",
    "audio_dataloader, eval_dataloader, model, compression_model, optimizer, lr_scheduler = accelerator.prepare(\n",
    "    audio_dataloader, eval_dataloader, model, compression_model, optimizer, lr_scheduler\n",
    ")\n",
    "\n",
    "starting_epoch, completed_steps, best_loss, save_epoch = 0, 0, np.inf, 0\n",
    "progress_bar = tqdm(range(cfg.max_train_steps), disable=not accelerator.is_local_main_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4bc3d96-a782-4486-9484-8b8a35d05523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 48000])\n",
      "(\"Remove 'A short burst of a whoosh sound with a high-pitched whistle at the end. This is the sound of an arrow being shot from a bow. This sound is often used in movies and video games as the sound of an arrow being shot from a bow. This sound is often used in movies and video games'\", \"Remove 'A low rumbling sound with a deep and resonant quality, and a pulsating effect, suitable for creating tension or anticipation in a movie or video game soundtrack.'\", \"Remove 'The sound of a laser gun being fired is heard. The laser gun emits a laser beam of light. The sound of the laser gun being fired is heard. The laser gun emits a laser beam of light. The sound of the laser gun being fired is heard. The laser gun emits a'\", \"Remove 'A small splash occurs, followed by bubbles rising to the surface of the water.'\", \"Remove 'A metallic object clangs against another metallic object.'\", \"Remove 'Someone is knocking on a wooden door. The knocks vary in speed, volume, and intensity, creating a sense of urgency and hesitation. The sound of the knocks becomes muffled towards the end.'\", \"Remove 'A loud whooshing sound with a high pitched whistling sound in the background.'\", \"Remove 'A loud, high-pitched beep is followed by a loud, low-pitched beep. This is followed by a low-pitched beep. This is followed by a loud, high-pitched beep. This is followed by a low-pitched beep. This is followed by'\", \"Remove 'The audio features a metallic object being shuffled through. The audio is in mono. The audio is low fidelity. There is no background noise. The audio has a metallic sound to it. The audio can be used as a sound effect for a sci-fi movie. The audio can also be used as'\", \"Remove 'The slamming sound of a door closing.'\", \"Remove 'A coin is flipped through the air and lands on a hard surface.'\", \"Remove 'The sound of a paper bag being crumpled.'\", \"Remove 'A hard object is dropped and shattered on a hard surface.'\", \"Remove 'The sound of someone blowing bubbles through a straw.'\", \"Remove 'The loud blow of a train horn.'\", \"Remove 'A gun is fired, and the bullet flies through the air. The sound of the gun firing is followed by the sound of the bullet as it flies through the air. The sound of the gun firing is loud and echoing, while the sound of the bullet as it flies through the air is soft and muffled. The'\")\n",
      "torch.Size([16, 1, 48000])\n",
      "tensor([ 17405,  85142, 101136,  11760,  97843,  73853, 106781, 110073,  31517,\n",
      "        122304,  48451,  13171,  77145,  54096,  49862,  19286],\n",
      "       device='cuda:0')\n",
      "padding_mask :  tensor([[[ True,  True,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False]],\n",
      "\n",
      "        [[ True,  True,  True,  ...,  True,  True,  True],\n",
      "         [ True,  True,  True,  ...,  True,  True,  True],\n",
      "         [ True,  True,  True,  ...,  True,  True,  True],\n",
      "         [ True,  True,  True,  ...,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  ...,  True,  True,  True],\n",
      "         [ True,  True,  True,  ...,  True,  True,  True],\n",
      "         [ True,  True,  True,  ...,  True,  True,  True],\n",
      "         [ True,  True,  True,  ...,  True,  True,  True]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ True,  True,  True,  ...,  True,  True,  True],\n",
      "         [ True,  True,  True,  ...,  True,  True,  True],\n",
      "         [ True,  True,  True,  ...,  True,  True,  True],\n",
      "         [ True,  True,  True,  ...,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  ...,  True,  True,  True],\n",
      "         [ True,  True,  True,  ...,  True,  True,  True],\n",
      "         [ True,  True,  True,  ...,  True,  True,  True],\n",
      "         [ True,  True,  True,  ...,  True,  True,  True]],\n",
      "\n",
      "        [[ True,  True,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False],\n",
      "         [ True,  True,  True,  ..., False, False, False]]], device='cuda:0')\n",
      "torch.Size([16, 4, 300])\n",
      "torch.Size([16, 4, 300]) \n",
      "\n",
      "------\n",
      "\n",
      "\n",
      "torch.Size([16, 4, 300])\n",
      "tensor([[[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]],\n",
      "\n",
      "        [[False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True]],\n",
      "\n",
      "        [[False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True]],\n",
      "\n",
      "        [[False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True],\n",
      "         [False, False, False,  ...,  True,  True,  True]],\n",
      "\n",
      "        [[False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False],\n",
      "         [False, False, False,  ..., False, False, False]]], device='cuda:0')\n",
      "\n",
      "\n",
      "loss :  tensor(1.4223, device='cuda:0', grad_fn=<DivBackward0>) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unwrapped_vae = accelerator.unwrap_model(compression_model)\n",
    "\n",
    "synthesized_audio, prompt, ground_truth, length = next(iter(audio_dataloader))\n",
    "\n",
    "print(synthesized_audio.shape)\n",
    "print(prompt)\n",
    "print(ground_truth.shape)\n",
    "print(length)\n",
    "\n",
    "synthesized_audio_tokens = process_audio_tokenizer(synthesized_audio, unwrapped_vae)\n",
    "synthesized_audio_tokens, synthesized_padding_mask = post_process_audio_tokenizer(synthesized_audio_tokens, length, unwrapped_vae, lm, cfg)\n",
    "\n",
    "ground_truth_tokens = process_audio_tokenizer(ground_truth, unwrapped_vae)\n",
    "ground_truth_tokens, ground_truth_padding_mask = post_process_audio_tokenizer(ground_truth_tokens, length, unwrapped_vae, lm, cfg)\n",
    "\n",
    "\n",
    "print(\"padding_mask : \", ground_truth_padding_mask)\n",
    "\n",
    "cona = torch.concatenate((synthesized_audio_tokens, ground_truth_tokens), dim=2)\n",
    "cona_mask = torch.concatenate((synthesized_padding_mask, ground_truth_padding_mask), dim=2)\n",
    "print(cona_mask.shape)\n",
    "print(cona.shape, \"\\n\\n------\\n\\n\")\n",
    "cona_mask[:, :, :150] = 0\n",
    "print(cona_mask.shape)\n",
    "print(cona_mask)\n",
    "\n",
    "# # for batch_idx, (synthesized_wav, prompts, ground_truth, lengths) in enumerate(audio_dataloader):\n",
    "# #     print(batch_idx)\n",
    "# #     if batch_idx == 4:\n",
    "# #         break\n",
    "\n",
    "attributes = [\n",
    "    ConditioningAttributes(text={'description': str(description)})\n",
    "    for description in prompt\n",
    "]\n",
    "\n",
    "loss, model_output = model(cona, cona_mask, attributes)\n",
    "\n",
    "print(\"\\n\\nloss : \", loss, \"\\n\\n\")\n",
    "\n",
    "del unwrapped_vae\n",
    "del loss\n",
    "del model_output\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad48c1f-60b9-42d2-b54d-953c274fa669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b48e6b1-3df6-4626-989a-e58ccb7f0df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------EPOCH0-------------------------\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "for epoch in range(starting_epoch, cfg.num_train_epochs):\n",
    "    accelerator.print(f\"-------------------EPOCH{epoch}-------------------------\" )\n",
    "    total_loss, total_val_loss = 0, 0\n",
    "    temp_loss = 0\n",
    "    model.train()\n",
    "    for batch_idx, (synthesized_wav, prompts, ground_truth, lengths) in enumerate(audio_dataloader):\n",
    "        # Consider batch\n",
    "        with accelerator.accumulate(model):\n",
    "            # print(1)\n",
    "            with torch.no_grad():\n",
    "                unwrapped_vae = accelerator.unwrap_model(compression_model)\n",
    "                synthesized_audio_tokens = process_audio_tokenizer(synthesized_wav, unwrapped_vae)\n",
    "                synthesized_audio_tokens, synthesized_padding_mask = post_process_audio_tokenizer(synthesized_audio_tokens, lengths, unwrapped_vae, lm, cfg)\n",
    "                # print(2)\n",
    "                \n",
    "                audio_tokens = process_audio_tokenizer(ground_truth, unwrapped_vae)\n",
    "                audio_tokens, padding_mask = post_process_audio_tokenizer(audio_tokens, lengths, unwrapped_vae, lm, cfg)\n",
    "                # print(3)\n",
    "                \n",
    "                attributes = [\n",
    "                    ConditioningAttributes(text={'description': str(description)})\n",
    "                    for description in prompts\n",
    "                ]\n",
    "            # print(3)\n",
    "\n",
    "            mask_token_seperation = torch.tensor([[[lm.special_token_id],[lm.special_token_id],[lm.special_token_id],[lm.special_token_id]]]*cfg.batch_size).to(accelerator.device)\n",
    "            mask_padding_token_seperation = torch.zeros(mask_token_seperation.shape, dtype=bool).to(accelerator.device)\n",
    "\n",
    "            made_up_tokens = torch.concatenate((synthesized_audio_tokens, mask_token_seperation, audio_tokens), dim=2)\n",
    "            made_padding_tokens = torch.concatenate((synthesized_padding_mask, mask_padding_token_seperation, padding_mask), dim=2)\n",
    "            # print(4)\n",
    "            made_padding_tokens[:, :, :150]=0\n",
    "            \n",
    "            loss, model_output = model(made_up_tokens, made_padding_tokens, attributes)\n",
    "            # print(5)\n",
    "            ppl =  torch.exp(loss)\n",
    "            total_loss += loss.detach().cpu().float()\n",
    "            accelerator.backward(loss)     \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                completed_steps += 1\n",
    "            \n",
    "            if batch_idx % 20==19:\n",
    "                losses.append(total_loss/batch_idx)\n",
    "            \n",
    "            if batch_idx % 100==99:\n",
    "                print(total_loss/batch_idx)\n",
    "                plt.plot(losses)\n",
    "                plt.show()\n",
    "    # save checkpoint every epoch\n",
    "    torch.save(model.state_dict(), f\"./sep_models/model_epoch_{epoch}_.pth\")\n",
    "\n",
    "    print(\"start evaluation\")\n",
    "    model.eval()\n",
    "    for batch_idx, (synthesized_wav, prompts, ground_truth, lengths) in enumerate(eval_dataloader):\n",
    "        # Consider batch\n",
    "        with accelerator.accumulate(model):\n",
    "            # print(1eval)\n",
    "            with torch.no_grad():\n",
    "                unwrapped_vae = accelerator.unwrap_model(compression_model)\n",
    "                synthesized_audio_tokens = process_audio_tokenizer(synthesized_wav, unwrapped_vae)\n",
    "                synthesized_audio_tokens, synthesized_padding_mask = post_process_audio_tokenizer(synthesized_audio_tokens, lengths, unwrapped_vae, lm, cfg)\n",
    "                # print(2eval)\n",
    "                \n",
    "                audio_tokens = process_audio_tokenizer(ground_truth, unwrapped_vae)\n",
    "                audio_tokens, padding_mask = post_process_audio_tokenizer(audio_tokens, lengths, unwrapped_vae, lm, cfg)\n",
    "                # print(3eval)\n",
    "                \n",
    "                attributes = [\n",
    "                    ConditioningAttributes(text={'description': str(description)})\n",
    "                    for description in prompts\n",
    "                ]\n",
    "            mask_token_seperation = torch.tensor([[[lm.special_token_id],[lm.special_token_id],[lm.special_token_id],[lm.special_token_id]]]*cfg.eval_batch_size).to(accelerator.device)\n",
    "            mask_padding_token_seperation = torch.zeros(mask_token_seperation.shape, dtype=bool).to(accelerator.device)\n",
    "\n",
    "            made_up_tokens = torch.concatenate((synthesized_audio_tokens, mask_token_seperation, audio_tokens), dim=2)\n",
    "            made_padding_tokens = torch.concatenate((synthesized_padding_mask, mask_padding_token_seperation, padding_mask), dim=2)\n",
    "            # print(4eval)\n",
    "            made_padding_tokens[:, :, :150]=0\n",
    "            \n",
    "            loss, model_output = model(made_up_tokens, made_padding_tokens, attributes)\n",
    "            total_val_loss += loss.cpu().detach()\n",
    "    \n",
    "    del model_output\n",
    "    del loss\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"total_val_loss : \", total_val_loss)\n",
    "\n",
    "    # reocrd to wandb\n",
    "    if accelerator.is_main_process:         \n",
    "        result = {}\n",
    "        result[\"epoch\"] = save_epoch + 1,\n",
    "        result[\"step\"] = completed_steps\n",
    "        result[\"train_loss\"] = round(total_loss.item()/cfg.save_steps, 4)\n",
    "        result[\"valid_loss\"] = round(total_val_loss.item()/len(eval_dataloader), 4)\n",
    "        \n",
    "        wandb.log(result)\n",
    "        result_string = \"Epoch: {}, Loss Train: {}, Valid: {}\\n\".format(save_epoch + 1, result[\"train_loss\"], result[\"valid_loss\"])\n",
    "        print(result_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a804d92-2b6f-4393-bf20-9800afc3c996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 모델을 가지고 inference하기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e231feb-facc-44f0-b209-3c0b25431a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7cec5f-a7da-4edc-908f-2e837ef1150a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da67362-62ad-408c-9c3c-952f85a889d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2753092-6ebf-498c-a250-18a9144fde11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
