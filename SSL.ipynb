{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3463963-c328-4006-92e1-d8615cdbea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcfd5338-711a-49fc-9cdd-7b0c4213973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#from audiotools import AudioSignal\n",
    "'''from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, cfg, train=True):\n",
    "        self.train = train\n",
    "        \n",
    "        self.target_sample_rate = cfg.sample_rate\n",
    "        self.duration = cfg.duration\n",
    "        self.device = cfg.device\n",
    "\n",
    "        if self.train:\n",
    "            self.audio_paths = cfg.train_data_path\n",
    "        else:\n",
    "            self.audio_paths = cfg.eval_data_path\n",
    "\n",
    "        self.df = pd.read_csv(self.audio_paths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        data = self.df.iloc[idx] #self.audio_files_list[idx]\n",
    "        \n",
    "        import random\n",
    "        audio_path = data['audio_path']\n",
    "        total_duration = data['duration']\n",
    "        description = \"<RANDOM>\"\n",
    "        \n",
    "        # Set duration\n",
    "        duration = self.duration if total_duration >= 3 else total_duration  # Duration is 3 seconds or total_duration if less than 3\n",
    "        \n",
    "        # Set offset based on conditions\n",
    "        if total_duration < self.duration or self.train == False:\n",
    "            offset = 0.0 \n",
    "        else:\n",
    "            max_offset = total_duration - duration  # Calculate the maximum possible offset\n",
    "            offset = random.uniform(0, max_offset)  # Choose a random offset within the possible range\n",
    "        \n",
    "        # Load audio signal file\n",
    "        wav = AudioSignal(audio_path, offset=offset, duration=duration)\n",
    "        length = wav.signal_length\n",
    "\n",
    "        # Encode audio signal as one long file\n",
    "        wav.to_mono()\n",
    "        wav.resample(self.target_sample_rate)\n",
    "\n",
    "        if wav.duration < self.duration:\n",
    "          pad_len = int(self.duration * self.target_sample_rate) - wav.signal_length\n",
    "          wav.zero_pad(0, pad_len)\n",
    "        elif wav.duration > self.duration:\n",
    "          wav.truncate_samples(self.duration * self.target_sample_rate)\n",
    "\n",
    "        return wav.audio_data.squeeze(1), description, length\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, cfg):\n",
    "\n",
    "        if cfg.prompts is None:\n",
    "            test_df = pd.read_csv(cfg.eval_data_path)\n",
    "            self.prompts = [\"<RANDOM>\"] * 8\n",
    "        else:\n",
    "            self.prompts = cfg.prompts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.prompts[idx] '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47d2113d-9be5-4c34-9c89-cfdf09285a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    import os\n",
    "    import json\n",
    "    import math\n",
    "    import sys\n",
    "    import argparse\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.nn import functional as F\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    \n",
    "    import wandb\n",
    "    \n",
    "    from accelerate import Accelerator\n",
    "    from transformers import get_scheduler\n",
    "    from audiocraft.modules.conditioners import JointEmbedCondition, SegmentWithAttributes, WavCondition, ConditioningAttributes\n",
    "    from config import Config\n",
    "    from audiomodel import AudioProcessing\n",
    "    from audiodataset_ssl import AudioDataset, TestDataset\n",
    "\n",
    "    def make_dir(path):\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "    \n",
    "    def wandb_init(cfg):\n",
    "        wandb.init(\n",
    "                # set the wandb project where this run will be logged\n",
    "                project=cfg.wandb_project_name,\n",
    "                \n",
    "                # track hyperparameters and run metadata\n",
    "                config={\n",
    "                \"learning_rate\": cfg.learning_rate,\n",
    "                \"epochs\": cfg.num_train_epochs,\n",
    "                \"batch_size\": cfg.batch_size,\n",
    "                }\n",
    "        )\n",
    "        \n",
    "    def save_checkpoint(cfg, model, result, best_loss, category, epoch=0):\n",
    "        save_checkpoint = False\n",
    "        with open(\"{}/summary.jsonl\".format(cfg.output_dir), \"a\") as f:\n",
    "            f.write(json.dumps(result) + \"\\n\\n\")\n",
    "            \n",
    "        if result[\"valid_loss\"] < best_loss:\n",
    "          best_loss = result[\"valid_loss\"]\n",
    "          save_checkpoint = True\n",
    "          \n",
    "        # 모델 상태 저장\n",
    "        if save_checkpoint and cfg.checkpointing_steps == \"best\":\n",
    "            torch.save(model.state_dict(), os.path.join(cfg.output_dir, f\"{category}.pth\"))\n",
    "    \n",
    "        #torch.save(model.state_dict(), os.path.join(cfg.output_dir, \"last.pth\"))\n",
    "        #torch.save(model.state_dict(), os.path.join(cfg.output_dir, f\"epoch_{epoch}.pth\"))\n",
    "    \n",
    "        return best_loss\n",
    "    \n",
    "    def build_model(cfg):\n",
    "            from audiocraft.models.loaders import load_compression_model, load_lm_model\n",
    "            \"\"\"Instantiate models and optimizer.\"\"\"     \n",
    "            compression_model = load_compression_model('facebook/audiogen-medium', device=cfg.device)\n",
    "            lm = load_lm_model('facebook/audiogen-medium', device=cfg.device)\n",
    "            return compression_model, lm\n",
    "    \n",
    "    def process_audio_tokenizer(wav, compression_model):\n",
    "            with torch.no_grad():\n",
    "                audio_tokens, scale = compression_model.encode(wav)\n",
    "            return audio_tokens\n",
    "    \n",
    "    def post_process_audio_tokenizer(audio_tokens, audio_lengths=None, compression_model=None, lm=None, cfg=None):\n",
    "        padding_mask = torch.ones_like(audio_tokens, dtype=torch.bool, device=audio_tokens.device)\n",
    "        audio_tokens = audio_tokens.clone()\n",
    "        padding_mask = padding_mask.clone()\n",
    "        token_sample_rate = compression_model.frame_rate\n",
    "        B, K, T_s = audio_tokens.shape\n",
    "        \n",
    "        for i in range(B):\n",
    "            valid_tokens = math.floor(audio_lengths[i] / cfg.sample_rate * token_sample_rate)\n",
    "            audio_tokens[i, :, valid_tokens:] = lm.special_token_id\n",
    "            padding_mask[i, :, valid_tokens:] = 0\n",
    "    \n",
    "        return audio_tokens, padding_mask\n",
    "\n",
    "    base_path = \"./csv_files\"\n",
    "    train_data_path = f\"{base_path}/train_dataset.csv\"\n",
    "    eval_data_path = f\"{base_path}/eval_dataset.csv\"\n",
    "    cfg = Config()\n",
    "    cfg.update(train_data_path=train_data_path, eval_data_path=eval_data_path)\n",
    "    accelerator = Accelerator(gradient_accumulation_steps=cfg.gradient_accumulation_steps)\n",
    "    cfg.update(device=accelerator.device)\n",
    "    make_dir(cfg.output_dir)\n",
    "    make_dir(cfg.generated_dir)\n",
    "    if accelerator.is_main_process: \n",
    "        wandb_init(cfg)\n",
    "\n",
    "    #with accelerator.main_process_first():  \n",
    "    compression_model, lm = build_model(cfg)\n",
    "    audio_dataset = AudioDataset(cfg, train=True) \n",
    "    eval_dataset = AudioDataset(cfg, train=False)\n",
    "    print(3)\n",
    "    compression_model.eval()\n",
    "    print(4)\n",
    "    model = AudioProcessing(cfg, lm)\n",
    "    print(5)\n",
    "    test_dataset = TestDataset(cfg)\n",
    "\n",
    "    audio_dataloader = DataLoader(audio_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=12)\n",
    "    print(6)\n",
    "    eval_dataloader = DataLoader(eval_dataset, batch_size=cfg.eval_batch_size, shuffle=False, num_workers=4)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "    optimizer_parameters = [param for param in model.lm.parameters() if param.requires_grad]\n",
    "    print(7)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        optimizer_parameters, lr=cfg.learning_rate,\n",
    "        betas=(cfg.adam_beta1, cfg.adam_beta2),\n",
    "        weight_decay=cfg.adam_weight_decay,\n",
    "        eps=cfg.adam_epsilon,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    num_update_steps_per_epoch = math.ceil(len(audio_dataloader) / cfg.gradient_accumulation_steps)\n",
    "    if cfg.max_train_steps is None:\n",
    "      cfg.max_train_steps = cfg.num_train_epochs * num_update_steps_per_epoch\n",
    "    \n",
    "    lr_scheduler = get_scheduler(\n",
    "          name=cfg.lr_scheduler_type,\n",
    "          optimizer=optimizer,\n",
    "          num_warmup_steps=cfg.num_warmup_steps * cfg.gradient_accumulation_steps,\n",
    "          num_training_steps=cfg.max_train_steps * cfg.gradient_accumulation_steps,\n",
    "      )\n",
    "    \n",
    "    #with accelerator.main_process_first():\n",
    "    #    print(f\"This will be printed by process {accelerator.process_index}\")\n",
    "        \n",
    "    \"\"\"with accelerator.main_process_first():\n",
    "      if cfg.resume_from_checkpoint:\n",
    "            if cfg.resume_from_checkpoint is not None or cfg.resume_from_checkpoint != \"\":\n",
    "                accelerator.print(f\"Resumed from local checkpoint: {cfg.resume_from_checkpoint}\")\n",
    "                accelerator.load_state(cfg.resume_from_checkpoint)\n",
    "                # path = os.path.basename(args.resume_from_checkpoint)\n",
    "                accelerator.print(f\"Resumed from local checkpoint: {cfg.resume_from_checkpoint}\")\"\"\"\n",
    "\n",
    "    print(8)\n",
    "    audio_dataloader, eval_dataloader, model, compression_model, optimizer, lr_scheduler = accelerator.prepare(\n",
    "        audio_dataloader, eval_dataloader, model, compression_model, optimizer, lr_scheduler\n",
    "    )\n",
    "    print(9)\n",
    "\n",
    "    starting_epoch, completed_steps, best_loss, save_epoch = 0, 0, np.inf, 0\n",
    "    progress_bar = tqdm(range(cfg.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "    \n",
    "    for epoch in range(starting_epoch, cfg.num_train_epochs):\n",
    "        accelerator.print(f\"-------------------EPOCH{epoch}-------------------------\" )\n",
    "        total_loss, total_val_loss = 0, 0\n",
    "        model.train()\n",
    "        for batch_idx, (wav, descriptions, lengths) in enumerate(audio_dataloader):\n",
    "            with accelerator.accumulate(model):\n",
    "                with torch.no_grad():\n",
    "                    unwrapped_vae = accelerator.unwrap_model(compression_model)\n",
    "                    audio_tokens = process_audio_tokenizer(wav, unwrapped_vae)\n",
    "                    audio_tokens, padding_mask = post_process_audio_tokenizer(audio_tokens, lengths, unwrapped_vae, lm, cfg) \n",
    "                    attributes = [\n",
    "                        ConditioningAttributes(text={'description': description})\n",
    "                        for description in descriptions]\n",
    "                loss = model(audio_tokens, padding_mask, attributes)\n",
    "                ppl =  torch.exp(loss)\n",
    "                total_loss += loss.detach().float()\n",
    "                accelerator.backward(loss)     \n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                if accelerator.sync_gradients:\n",
    "                    progress_bar.update(1)\n",
    "                    completed_steps += 1\n",
    "                    \n",
    "            #if batch_idx == cfg.save_steps:\n",
    "            #    break\n",
    "    \n",
    "\n",
    "            #if completed_steps & cfg.save_steps == 0:\n",
    "        \n",
    "        model.eval()\n",
    "        for batch_idx, (wav, descriptions, lengths) in enumerate(eval_dataloader):\n",
    "            with accelerator.accumulate(model):\n",
    "                with torch.no_grad():\n",
    "                    unwrapped_vae = accelerator.unwrap_model(compression_model)\n",
    "                    audio_tokens = process_audio_tokenizer(wav, unwrapped_vae)\n",
    "                    audio_tokens, padding_mask = post_process_audio_tokenizer(audio_tokens, lengths, unwrapped_vae, lm, cfg) \n",
    "                    attributes = [\n",
    "                        ConditioningAttributes(text={'description': description})\n",
    "                        for description in descriptions]\n",
    "                    loss = model(audio_tokens, padding_mask, attributes)\n",
    "                    total_val_loss += loss  \n",
    "    \n",
    "        if accelerator.is_main_process:         \n",
    "            result = {}\n",
    "            result[\"epoch\"] = save_epoch + 1,\n",
    "            result[\"step\"] = completed_steps\n",
    "            result[\"train_loss\"] = round(total_loss.item()/cfg.save_steps, 4)\n",
    "            result[\"valid_loss\"] = round(total_val_loss.item()/len(eval_dataloader), 4)\n",
    "            \n",
    "            wandb.log(result)\n",
    "            result_string = \"Epoch: {}, Loss Train: {}, Valid: {}\\n\".format(save_epoch + 1, result[\"train_loss\"], result[\"valid_loss\"])    \n",
    "            accelerator.print(result_string) \n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_vae = accelerator.unwrap_model(compression_model)\n",
    "            best_loss = save_checkpoint(cfg, unwrapped_model, result, best_loss, save_epoch)\n",
    "            for test_step, batch in enumerate(test_dataloader):\n",
    "                _, gen_audio = unwrapped_model.inference(batch, unwrapped_vae)\n",
    "                audio_filename = f\"epoch_{save_epoch}_{test_step}.wav\"\n",
    "                unwrapped_model.save_audio(gen_audio, audio_filename, cfg)\n",
    "            save_epoch += 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95298726-9bae-418f-b36f-bf628daaccad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.1.0+cu121 with CUDA 1201 (you have 2.1.0+cu118)\n",
      "    Python  3.10.13 (you have 3.10.12)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.1.0+cu121 with CUDA 1201 (you have 2.1.0+cu118)\n",
      "    Python  3.10.13 (you have 3.10.12)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.1.0+cu121 with CUDA 1201 (you have 2.1.0+cu118)\n",
      "    Python  3.10.13 (you have 3.10.12)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.1.0+cu121 with CUDA 1201 (you have 2.1.0+cu118)\n",
      "    Python  3.10.13 (you have 3.10.12)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moptimizerai\u001b[0m (\u001b[33moptimizer_ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/audiogen-finetune/wandb/run-20240107_072741-74qvvo26</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1/runs/74qvvo26' target=\"_blank\">treasured-night-190</a></strong> to <a href='https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1' target=\"_blank\">https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1/runs/74qvvo26' target=\"_blank\">https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1/runs/74qvvo26</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "notebook_launcher(main, num_processes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "effc9479-6a88-4828-83df-87f3b787e0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moptimizerai\u001b[0m (\u001b[33moptimizer_ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/audiogen-finetune/wandb/run-20240106_140653-sushtzkb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1/runs/sushtzkb' target=\"_blank\">apricot-sound-168</a></strong> to <a href='https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1' target=\"_blank\">https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1/runs/sushtzkb' target=\"_blank\">https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1/runs/sushtzkb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "base_path = \"./csv_files\"\n",
    "train_data_path = f\"{base_path}/train_dataset.csv\"\n",
    "eval_data_path = f\"{base_path}/eval_dataset.csv\"\n",
    "cfg = Config()\n",
    "cfg.update(train_data_path=train_data_path, eval_data_path=eval_data_path)\n",
    "accelerator = Accelerator(gradient_accumulation_steps=cfg.gradient_accumulation_steps)\n",
    "cfg.update(device=accelerator.device)\n",
    "make_dir(cfg.output_dir)\n",
    "make_dir(cfg.generated_dir)\n",
    "if accelerator.is_main_process: \n",
    "    wandb_init(cfg)\n",
    "\n",
    "with accelerator.main_process_first():  \n",
    "    compression_model, lm = build_model(cfg)\n",
    "    audio_dataset = AudioDataset(cfg, train=True) \n",
    "    eval_dataset = AudioDataset(cfg, train=False)\n",
    "compression_model.eval()\n",
    "model = AudioProcessing(cfg, lm)\n",
    "test_dataset = TestDataset(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f19cf96a-f348-477b-a6fb-e450ecfcc1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dataset = AudioDataset(cfg, train=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "008dc95e-7932-4f65-ac61-d4e704295798",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dataloader = DataLoader(audio_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=12)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=cfg.eval_batch_size, shuffle=False, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becbcc6b-7b4c-480e-8ac4-3ac37d5fb9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
