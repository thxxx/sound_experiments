{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bbe7497-eb30-4101-ba66-f39b40a88907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.1.0+cu121 with CUDA 1201 (you have 2.1.0+cu118)\n",
      "    Python  3.10.13 (you have 3.10.12)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import sys\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import wandb\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from transformers import get_scheduler\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "from beats.BEATs import BEATsConfig, BEATs\n",
    "\n",
    "from config import Config\n",
    "from captioning_config import CaptionConfig\n",
    "from audiomodel import AudioProcessing\n",
    "from audiocraft.modules.conditioners import JointEmbedCondition, SegmentWithAttributes, WavCondition, ConditioningAttributes\n",
    "\n",
    "def make_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def wandb_init(cfg):\n",
    "    wandb.init(\n",
    "            # set the wandb project where this run will be logged\n",
    "            project=cfg.wandb_project_name,\n",
    "            \n",
    "            # track hyperparameters and run metadata\n",
    "            config={\n",
    "            \"learning_rate\": cfg.learning_rate,\n",
    "            \"epochs\": cfg.num_train_epochs,\n",
    "            \"batch_size\": cfg.batch_size,\n",
    "            }\n",
    "    )\n",
    "    \n",
    "def save_checkpoint(cfg, model, result, best_loss, epoch=0):\n",
    "    save_checkpoint = False\n",
    "    with open(\"{}/summary.jsonl\".format(cfg.output_dir), \"a\") as f:\n",
    "        f.write(json.dumps(result) + \"\\n\\n\")\n",
    "        \n",
    "    if result[\"train_loss\"] < best_loss:\n",
    "      best_loss = result[\"train_loss\"]\n",
    "      save_checkpoint = True\n",
    "      \n",
    "    # 모델 상태 저장\n",
    "    if save_checkpoint and cfg.checkpointing_steps == \"best\":\n",
    "        torch.save(model.state_dict(), os.path.join(cfg.output_dir, \"best.pth\"))\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(cfg.output_dir, \"last.pth\"))\n",
    "    torch.save(model.state_dict(), os.path.join(cfg.output_dir, f\"epoch_{epoch}.pth\"))\n",
    "\n",
    "    return best_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73243168-ef53-4d4b-a6e8-a94559cf53ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beats\n",
    "def load_beats(beats_ckpt, device):\n",
    "    beats_checkpoint = torch.load(beats_ckpt, map_location='cpu')\n",
    "    beats_cfg = BEATsConfig(beats_checkpoint['cfg'])\n",
    "    beats = BEATs(beats_cfg)\n",
    "    beats.load_state_dict(beats_checkpoint['model'])\n",
    "    for name, param in beats.named_parameters():\n",
    "        param.requires_grad = False\n",
    "    return beats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88bd4987-cc75-459c-9134-c14686986efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(cfg.lm_model_name)\n",
    "        self.ln_audio = nn.LayerNorm(cfg.audio_embedding_size)\n",
    "        self.lm_proj_audio = nn.Linear(cfg.audio_embedding_size, cfg.text_embedding_size)\n",
    "\n",
    "    def forward(self, audio_embeds, input_ids, labels, tokenizer, device):\n",
    "        # 오디오 임베딩 레이어 정규화 및 크기 조정\n",
    "        audio_embeds = self.ln_audio(audio_embeds)\n",
    "        audio_embeds = self.lm_proj_audio(audio_embeds)\n",
    "\n",
    "        # 텍스트 임베딩\n",
    "        embed_tokens = self.model.transformer.wte\n",
    "        inputs_embeds = embed_tokens(input_ids)\n",
    "\n",
    "        # BOS 토큰 임베딩 생성 및 반복\n",
    "        bsz = input_ids.size(0)\n",
    "        bos_embeds = embed_tokens(torch.ones([1], dtype=torch.long, device=device) * tokenizer.bos_token_id)\n",
    "        bos_embeds = bos_embeds.repeat(bsz, 1, 1)\n",
    "\n",
    "        # 오디오, BOS 및 텍스트 임베딩 결합\n",
    "        inputs_embeds = torch.cat([bos_embeds, audio_embeds, inputs_embeds], dim=1)\n",
    "\n",
    "        # 모델 실행 및 손실 계산\n",
    "        output = self.model(inputs_embeds=inputs_embeds, labels=labels)\n",
    "        loss = output.loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def generate(self, audio_embeds, input_ids, tokenizer, max_length=50, num_return_sequences=1):\n",
    "\n",
    "        # 텍스트 생성\n",
    "        audio_embeds = self.ln_audio(audio_embeds)\n",
    "        audio_embeds = self.lm_proj_audio(audio_embeds)\n",
    "\n",
    "        # 텍스트 임베딩\n",
    "        embed_tokens = self.model.transformer.wte\n",
    "        inputs_embeds = embed_tokens(input_ids)\n",
    "\n",
    "        # BOS 토큰 임베딩 생성 및 반복\n",
    "        bsz = input_ids.size(0)\n",
    "        bos_embeds = embed_tokens(torch.ones([1], dtype=torch.long, device=inputs_embeds.device) * tokenizer.bos_token_id)\n",
    "        bos_embeds = bos_embeds.repeat(bsz, 1, 1)\n",
    "\n",
    "        # 오디오, BOS 및 텍스트 임베딩 결합\n",
    "        inputs_embeds = torch.cat([bos_embeds, audio_embeds, inputs_embeds], dim=1)\n",
    "        atts = torch.ones(inputs_embeds.size()[:-1], dtype=torch.long).to(inputs_embeds.device)\n",
    "        \n",
    "        output = self.model.generate(inputs_embeds=inputs_embeds, attention_mask=atts, max_length=50, num_return_sequences=1,pad_token_id=tokenizer.eos_token_id)\n",
    "        \n",
    "        # 생성된 텍스트 디코딩\n",
    "        generated_texts = []\n",
    "        for i in range(len(output)):\n",
    "            generated_text = tokenizer.decode(output[i], skip_special_tokens=True)\n",
    "            generated_texts.append(generated_text)\n",
    "        \n",
    "        return generated_texts, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f04a343e-e67b-4d3f-91c4-d1e07b1a491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_embedding(wav, beats, audio_token_length, device):\n",
    "    # 오디오 패딩 마스크 생성\n",
    "    audio_padding_mask = torch.zeros(wav.shape, device=device).bool()\n",
    "\n",
    "    # 오디오 특징 추출\n",
    "    audio_embeds, _ = beats.extract_features(wav, padding_mask=audio_padding_mask, feature_only=True)\n",
    "\n",
    "    # 현재 길이 확인\n",
    "    current_length = audio_embeds.size(1)\n",
    "\n",
    "    if current_length > audio_token_length:\n",
    "        # 오디오 임베딩 자르기\n",
    "        audio_embeds = audio_embeds.narrow(1, 0, audio_token_length)\n",
    "    elif current_length < audio_token_length:\n",
    "        # 필요한 패딩 길이 계산 및 적용\n",
    "        padding_length = audio_token_length - current_length\n",
    "        audio_embeds = F.pad(audio_embeds, (0, 0, 0, padding_length))\n",
    "\n",
    "    return audio_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "954d2e20-16d5-404a-99f9-2813a23abe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, cfg, tokenizer: GPT2Tokenizer, train=True):\n",
    "        if train:\n",
    "            self.data_path = cfg.train_data_path\n",
    "        else:\n",
    "            self.data_path = cfg.eval_data_path\n",
    "        self.dataframe = pd.read_csv(self.data_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.pad_token = tokenizer.eos_token\n",
    "        self.audio_token_length = cfg.audio_token_length\n",
    "        self.text_max_length = cfg.text_max_length\n",
    "        self.max_length = self.audio_token_length + self.text_max_length + 1   # 전체 길이 설정\n",
    "        self.sample_rate = cfg.sample_rate\n",
    "        self.duration = cfg.duration\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # DataFrame에서 데이터 로드\n",
    "        data = self.dataframe.iloc[idx]\n",
    "        wav_path = data['audio_path']\n",
    "        \n",
    "        caption = \"<TEXT>\"\n",
    "\n",
    "        # 오디오 파일 정보 읽기\n",
    "        info = sf.info(wav_path)\n",
    "        lengths = info.duration * info.samplerate\n",
    "        \n",
    "        # 오디오 파일이 3초 이상인 경우\n",
    "        if info.duration > 3:\n",
    "            # 첫 3초만 읽기\n",
    "            wav, sr = sf.read(wav_path, frames=info.samplerate * 3)\n",
    "        else:\n",
    "            # 전체 파일 읽기\n",
    "            wav, sr = sf.read(wav_path)\n",
    "            \n",
    "        if len(wav.shape) == 2:\n",
    "            wav = wav[:, 0]\n",
    "\n",
    "        # 샘플링 레이트 조정\n",
    "        if sr != self.sample_rate:\n",
    "            wav = librosa.resample(wav, orig_sr=sr, target_sr=self.sample_rate, res_type=\"fft\")\n",
    "\n",
    "        #from IPython.display import Audio\n",
    "        #display(Audio(wav, rate=16000))\n",
    "        \n",
    "        # 오디오 길이 조정 (3초로)\n",
    "        target_length = self.duration * self.sample_rate  # 3초에 해당하는 샘플 수\n",
    "        if len(wav) > target_length:\n",
    "            wav = wav[:target_length]  # 3초를 초과하는 경우 자르기\n",
    "        elif len(wav) < target_length:\n",
    "            padding = target_length - len(wav)  # 필요한 패딩 계산\n",
    "            wav = np.pad(wav, (0, padding), 'constant')  # 패딩 적용\n",
    "\n",
    "        # 토큰화\n",
    "        batch_encoding = self.tokenizer(caption, return_tensors='pt')\n",
    "        input_ids = batch_encoding['input_ids'].squeeze(0)\n",
    "\n",
    "        return wav, input_ids, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04b688a0-b9d8-49f4-8aeb-f8393c443473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(cfg):\n",
    "        from audiocraft.models.loaders import load_compression_model, load_lm_model\n",
    "        \"\"\"Instantiate models and optimizer.\"\"\"     \n",
    "        compression_model = load_compression_model('facebook/audiogen-medium', device=cfg.device)\n",
    "        lm = load_lm_model('facebook/audiogen-medium', device=cfg.device)\n",
    "        return compression_model, lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "136ecf4f-0a20-4c54-b085-8a08ab263244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_tokenizer(wav, compression_model):\n",
    "        with torch.no_grad():\n",
    "            audio_tokens, scale = compression_model.encode(wav)\n",
    "        return audio_tokens\n",
    "\n",
    "def post_process_audio_tokenizer(audio_tokens, audio_lengths=None, compression_model=None, lm=None, cfg=None):\n",
    "    padding_mask = torch.ones_like(audio_tokens, dtype=torch.bool, device=audio_tokens.device)\n",
    "    audio_tokens = audio_tokens.clone()\n",
    "    padding_mask = padding_mask.clone()\n",
    "    token_sample_rate = compression_model.frame_rate\n",
    "    B, K, T_s = audio_tokens.shape\n",
    "    \n",
    "    for i in range(B):\n",
    "        valid_tokens = math.floor(audio_lengths[i] / cfg.sample_rate * token_sample_rate)\n",
    "        audio_tokens[i, :, valid_tokens:] = lm.special_token_id\n",
    "        padding_mask[i, :, valid_tokens:] = 0\n",
    "\n",
    "    return audio_tokens, padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e57ba89f-a582-4ffe-b742-04a5bf8b6221",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, cfg):\n",
    "\n",
    "        if cfg.prompts is None:\n",
    "            test_df = pd.read_csv(cfg.test_data_path)\n",
    "            self.prompts = [test_df.iloc[0]['caption'], test_df.iloc[1]['caption'], test_df.iloc[2]['caption'], test_df.iloc[3]['caption'], test_df.iloc[4]['caption'], test_df.iloc[5]['caption'], test_df.iloc[6]['caption'], test_df.iloc[7]['caption'] ]\n",
    "        else:\n",
    "            self.prompts = cfg.prompts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.prompts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aad3683d-6b78-4003-b1a1-f42a1bcfe086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a02f8de-9b30-4297-9aaf-190e4c43748c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 wav, input_ids, lengths = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">next</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">iter</span>(audio_dataloader))                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>audio_embeds = process_audio_embedding(wav.to(device), beats_model, caption_cfg.audio_to     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>descriptions, output = caption_model.generate(audio_embeds.to(device), input_ids.to(devi     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'audio_dataloader'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 wav, input_ids, lengths = \u001b[96mnext\u001b[0m(\u001b[96miter\u001b[0m(audio_dataloader))                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0maudio_embeds = process_audio_embedding(wav.to(device), beats_model, caption_cfg.audio_to     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0mdescriptions, output = caption_model.generate(audio_embeds.to(device), input_ids.to(devi     \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'audio_dataloader'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wav, input_ids, lengths = next(iter(audio_dataloader))\n",
    "audio_embeds = process_audio_embedding(wav.to(device), beats_model, caption_cfg.audio_token_length, device)\n",
    "\n",
    "descriptions, output = caption_model.generate(audio_embeds.to(device), input_ids.to(device), tokenizer)\n",
    "for d in descriptions:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0821a85-c54a-4665-a157-6e0d9154a2b6",
   "metadata": {},
   "source": [
    "### run train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "498cbe47-c121-48b3-87bb-089cfc516735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    cfg = Config()\n",
    "    caption_cfg = CaptionConfig()\n",
    "\n",
    "    accelerator = Accelerator(gradient_accumulation_steps=cfg.gradient_accumulation_steps)\n",
    "    device = accelerator.device\n",
    "    cfg.update(device=accelerator.device)\n",
    "    make_dir(cfg.output_dir)\n",
    "    make_dir(cfg.generated_dir)\n",
    "    #if accelerator.is_main_process: \n",
    "    #    wandb_init(cfg)\n",
    "\n",
    "    with accelerator.main_process_first():  \n",
    "        compression_model, lm = build_model(cfg)\n",
    "        model = AudioProcessing(cfg, lm)\n",
    "    \n",
    "        beats_model = load_beats(caption_cfg.beats_ckpt, device).to(device)\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(caption_cfg.lm_model_name)\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "        caption_model = CaptionModel(caption_cfg).to(device)\n",
    "        caption_model.load_state_dict(torch.load(\"caption_weight/6.pth\"))\n",
    "\n",
    "        audio_dataset = CaptionDataset(caption_cfg, tokenizer, train=True) \n",
    "        eval_dataset = CaptionDataset(caption_cfg, tokenizer, train=False)\n",
    "    test_dataset = TestDataset(caption_cfg)\n",
    "    \n",
    "    audio_dataloader = DataLoader(audio_dataset, batch_size=caption_cfg.batch_size, shuffle=True, num_workers=8)\n",
    "    eval_dataloader = DataLoader(eval_dataset, batch_size=caption_cfg.eval_batch_size, shuffle=False, num_workers=4)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "    \n",
    "    optimizer_parameters = [param for param in model.lm.parameters() if param.requires_grad]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        optimizer_parameters, lr=cfg.learning_rate,\n",
    "        betas=(cfg.adam_beta1, cfg.adam_beta2),\n",
    "        weight_decay=cfg.adam_weight_decay,\n",
    "        eps=cfg.adam_epsilon,\n",
    "    )\n",
    "\n",
    "    num_update_steps_per_epoch = math.ceil(len(audio_dataloader) / cfg.gradient_accumulation_steps)\n",
    "    if cfg.max_train_steps is None:\n",
    "      cfg.max_train_steps = cfg.num_train_epochs * num_update_steps_per_epoch\n",
    "    \n",
    "    lr_scheduler = get_scheduler(\n",
    "          name=cfg.lr_scheduler_type,\n",
    "          optimizer=optimizer,\n",
    "          num_warmup_steps=cfg.num_warmup_steps * cfg.gradient_accumulation_steps,\n",
    "          num_training_steps=cfg.max_train_steps * cfg.gradient_accumulation_steps,\n",
    "      )\n",
    "\n",
    "\n",
    "    audio_dataloader, eval_dataloader, model, compression_model, caption_model, beats_model, optimizer, lr_scheduler = accelerator.prepare(\n",
    "    audio_dataloader, eval_dataloader, model, compression_model, caption_model, beats_model, optimizer, lr_scheduler\n",
    ")\n",
    "    compression_model.eval()\n",
    "    beats_model.eval()\n",
    "    caption_model.eval()\n",
    "\n",
    "    starting_epoch, completed_steps, best_loss = 0, 0, np.inf\n",
    "    progress_bar = tqdm(range(cfg.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "    \n",
    "    for epoch in range(starting_epoch, cfg.num_train_epochs):\n",
    "        print(f\"-------------------EPOCH{epoch}-------------------------\" )\n",
    "        total_loss, total_val_loss = 0, 0\n",
    "        model.train()\n",
    "        for batch_idx, (wav, input_ids, lengths) in enumerate(audio_dataloader):\n",
    "             with accelerator.accumulate(model):\n",
    "                with torch.no_grad():\n",
    "                    unwrapped_vae = accelerator.unwrap_model(compression_model)\n",
    "                    audio_tokens = process_audio_tokenizer(wav.unsqueeze(1).to(torch.float32), unwrapped_vae)\n",
    "                    audio_tokens, padding_mask = post_process_audio_tokenizer(audio_tokens, lengths, unwrapped_vae, lm, cfg) \n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    unwrapped_beats = accelerator.unwrap_model(beats_model)\n",
    "                    unwrapped_gpt = accelerator.unwrap_model(caption_model)\n",
    "                    audio_embeds = process_audio_embedding(wav.to(device), unwrapped_beats, caption_cfg.audio_token_length, device)\n",
    "    \n",
    "                    descriptions, output = unwrapped_gpt.generate(audio_embeds.to(device), input_ids.to(device), tokenizer)\n",
    "                    #for d in descriptions:\n",
    "                    #    print(d)\n",
    "                    attributes = [\n",
    "                        ConditioningAttributes(text={'description': str(description)})\n",
    "                        for description in descriptions]\n",
    "              \n",
    "                loss = model(audio_tokens, padding_mask, attributes)\n",
    "                #print(loss)\n",
    "                ppl =  torch.exp(loss)\n",
    "                total_loss += loss.detach().float()\n",
    "                accelerator.backward(loss)     \n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if accelerator.sync_gradients:\n",
    "                    progress_bar.update(1)\n",
    "                    completed_steps += 1\n",
    "        model.eval()\n",
    "        for batch_idx, (wav, input_ids, lengths) in enumerate(eval_dataloader):\n",
    "            with accelerator.accumulate(model):\n",
    "                with torch.no_grad():\n",
    "                  \n",
    "                    unwrapped_vae = accelerator.unwrap_model(compression_model)\n",
    "                    audio_tokens = process_audio_tokenizer(wav.unsqueeze(1).to(torch.float32), unwrapped_vae)\n",
    "                    audio_tokens, padding_mask = post_process_audio_tokenizer(audio_tokens, lengths, unwrapped_vae, lm, cfg) \n",
    "                \n",
    "                    unwrapped_beats = accelerator.unwrap_model(beats_model)\n",
    "                    unwrapped_gpt = accelerator.unwrap_model(caption_model)\n",
    "                    audio_embeds = process_audio_embedding(wav.to(device), unwrapped_beats, caption_cfg.audio_token_length, device)\n",
    "    \n",
    "                    descriptions, output = unwrapped_gpt.generate(audio_embeds.to(device), input_ids.to(device), tokenizer)\n",
    "                    #for d in descriptions:\n",
    "                    #    print(d)\n",
    "                    attributes = [\n",
    "                        ConditioningAttributes(text={'description': str(description)})\n",
    "                        for description in descriptions]\n",
    "                    loss = model(audio_tokens, padding_mask, attributes)\n",
    "                    total_val_loss += loss \n",
    "                    \n",
    "        if accelerator.is_main_process:         \n",
    "            result = {}\n",
    "            result[\"epoch\"] = save_epoch + 1,\n",
    "            result[\"step\"] = completed_steps\n",
    "            result[\"train_loss\"] = round(total_loss.item()/cfg.save_steps, 4)\n",
    "            result[\"valid_loss\"] = round(total_val_loss.item()/len(eval_dataloader), 4)\n",
    "            \n",
    "            wandb.log(result)\n",
    "            result_string = \"Epoch: {}, Loss Train: {}, Valid: {}\\n\".format(save_epoch + 1, result[\"train_loss\"], result[\"valid_loss\"])    \n",
    "            accelerator.print(result_string) \n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_vae = accelerator.unwrap_model(compression_model)\n",
    "            best_loss = save_checkpoint(cfg, unwrapped_model, result, best_loss, save_epoch)\n",
    "            for test_step, batch in enumerate(test_dataloader):\n",
    "                gen_token, gen_audio = unwrapped_model.inference(batch, unwrapped_vae)\n",
    "                audio_filename = f\"epoch_{save_epoch}_{test_step}.wav\"\n",
    "                unwrapped_model.save_audio(gen_audio, audio_filename, cfg)\n",
    "            save_epoch += 1 \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510c8896-d209-4925-8acd-588e45fe0578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "\n",
      "  0%|          | 0/14893000 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------EPOCH0-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 1/14893000 [00:07<30143:36:38,  7.29s/it]\u001b[A\n",
      "  0%|          | 2/14893000 [00:11<21975:37:12,  5.31s/it]\u001b[A\n",
      "  0%|          | 3/14893000 [00:15<19416:09:57,  4.69s/it]\u001b[A\n",
      "  0%|          | 4/14893000 [00:19<18229:25:53,  4.41s/it]\u001b[A\n",
      "  0%|          | 5/14893000 [00:22<17288:14:16,  4.18s/it]\u001b[A\n",
      "  0%|          | 6/14893000 [00:26<16950:58:43,  4.10s/it]\u001b[A\n",
      "  0%|          | 7/14893000 [00:30<16643:03:51,  4.02s/it]\u001b[A\n",
      "  0%|          | 8/14893000 [00:34<16558:05:44,  4.00s/it]\u001b[A\n",
      "  0%|          | 9/14893000 [00:38<16448:22:03,  3.98s/it]\u001b[A\n",
      "  0%|          | 10/14893000 [00:42<16383:33:38,  3.96s/it]\u001b[A\n",
      "  0%|          | 11/14893000 [00:46<16338:35:22,  3.95s/it]\u001b[A\n",
      "  0%|          | 12/14893000 [00:50<16323:30:47,  3.95s/it]\u001b[A\n",
      "  0%|          | 13/14893000 [00:54<16315:28:48,  3.94s/it]\u001b[A\n",
      "  0%|          | 14/14893000 [00:58<16269:39:58,  3.93s/it]\u001b[A\n",
      "  0%|          | 15/14893000 [01:02<16331:42:04,  3.95s/it]\u001b[A\n",
      "  0%|          | 16/14893000 [01:06<16288:19:17,  3.94s/it]\u001b[A\n",
      "  0%|          | 17/14893000 [01:10<16249:26:09,  3.93s/it]\u001b[A\n",
      "  0%|          | 18/14893000 [01:13<16226:02:45,  3.92s/it]\u001b[A\n",
      "  0%|          | 19/14893000 [01:17<16253:03:38,  3.93s/it]\u001b[A\n",
      "  0%|          | 20/14893000 [01:21<16254:39:48,  3.93s/it]\u001b[A\n",
      "  0%|          | 21/14893000 [01:25<16230:05:21,  3.92s/it]\u001b[A\n",
      "  0%|          | 22/14893000 [01:29<16336:35:32,  3.95s/it]\u001b[A\n",
      "  0%|          | 23/14893000 [01:33<16329:14:07,  3.95s/it]\u001b[A\n",
      "  0%|          | 24/14893000 [01:37<16309:04:07,  3.94s/it]\u001b[A\n",
      "  0%|          | 25/14893000 [01:41<16264:37:31,  3.93s/it]\u001b[A\n",
      "  0%|          | 26/14893000 [01:45<16240:27:14,  3.93s/it]\u001b[A\n",
      "  0%|          | 27/14893000 [01:49<16219:03:19,  3.92s/it]\u001b[A\n",
      "  0%|          | 28/14893000 [01:53<16263:04:39,  3.93s/it]\u001b[A\n",
      "  0%|          | 29/14893000 [01:57<16297:08:12,  3.94s/it]\u001b[A\n",
      "  0%|          | 30/14893000 [02:01<16314:19:19,  3.94s/it]\u001b[A\n",
      "  0%|          | 31/14893000 [02:05<16359:26:04,  3.95s/it]\u001b[A\n",
      "  0%|          | 32/14893000 [02:09<16321:54:51,  3.95s/it]\u001b[A\n",
      "  0%|          | 33/14893000 [02:13<16301:58:57,  3.94s/it]\u001b[A\n",
      "  0%|          | 34/14893000 [02:16<16285:38:08,  3.94s/it]\u001b[A\n",
      "  0%|          | 35/14893000 [02:20<16263:01:22,  3.93s/it]\u001b[A\n",
      "  0%|          | 36/14893000 [02:24<16287:38:26,  3.94s/it]\u001b[A\n",
      "  0%|          | 37/14893000 [02:28<16306:30:00,  3.94s/it]\u001b[A\n",
      "  0%|          | 38/14893000 [02:32<16621:04:57,  4.02s/it]\u001b[A\n",
      "  0%|          | 39/14893000 [02:36<16543:11:27,  4.00s/it]\u001b[A\n",
      "  0%|          | 40/14893000 [02:40<16247:53:48,  3.93s/it]\u001b[A\n",
      "  0%|          | 41/14893000 [02:44<16265:46:49,  3.93s/it]\u001b[A\n",
      "  0%|          | 42/14893000 [02:48<16082:23:54,  3.89s/it]\u001b[A\n",
      "  0%|          | 43/14893000 [02:52<16119:22:45,  3.90s/it]\u001b[A\n",
      "  0%|          | 44/14893000 [02:56<16149:20:52,  3.90s/it]\u001b[A\n",
      "  0%|          | 45/14893000 [03:00<16176:43:58,  3.91s/it]\u001b[A\n",
      "  0%|          | 46/14893000 [03:04<16232:47:49,  3.92s/it]\u001b[A\n",
      "  0%|          | 47/14893000 [03:08<16255:14:46,  3.93s/it]\u001b[A\n",
      "  0%|          | 48/14893000 [03:11<16047:26:45,  3.88s/it]\u001b[A\n",
      "  0%|          | 49/14893000 [03:15<16083:50:06,  3.89s/it]\u001b[A\n",
      "  0%|          | 50/14893000 [03:19<16129:49:49,  3.90s/it]\u001b[A\n",
      "  0%|          | 51/14893000 [03:23<16155:47:37,  3.91s/it]\u001b[A\n",
      "  0%|          | 52/14893000 [03:27<16155:44:51,  3.91s/it]\u001b[A\n",
      "  0%|          | 53/14893000 [03:31<16340:20:09,  3.95s/it]\u001b[A\n",
      "  0%|          | 54/14893000 [03:35<16310:44:12,  3.94s/it]\u001b[A\n",
      "  0%|          | 55/14893000 [03:39<16285:18:12,  3.94s/it]\u001b[A\n",
      "  0%|          | 56/14893000 [03:43<16252:51:28,  3.93s/it]\u001b[A\n",
      "  0%|          | 57/14893000 [03:47<16225:46:36,  3.92s/it]\u001b[A\n",
      "  0%|          | 58/14893000 [03:51<16218:10:18,  3.92s/it]\u001b[A\n",
      "  0%|          | 59/14893000 [03:55<16210:45:23,  3.92s/it]\u001b[A\n",
      "  0%|          | 60/14893000 [03:58<16228:56:49,  3.92s/it]\u001b[A\n",
      "  0%|          | 61/14893000 [04:02<16228:24:33,  3.92s/it]\u001b[A\n",
      "  0%|          | 62/14893000 [04:06<16289:22:24,  3.94s/it]\u001b[A\n",
      "  0%|          | 63/14893000 [04:10<16264:01:30,  3.93s/it]\u001b[A\n",
      "  0%|          | 64/14893000 [04:14<16242:27:27,  3.93s/it]\u001b[A\n",
      "  0%|          | 65/14893000 [04:18<16234:30:34,  3.92s/it]\u001b[A\n",
      "  0%|          | 66/14893000 [04:22<16225:10:29,  3.92s/it]\u001b[A\n",
      "  0%|          | 67/14893000 [04:26<16237:13:56,  3.92s/it]\u001b[A\n",
      "  0%|          | 68/14893000 [04:30<16379:37:01,  3.96s/it]\u001b[A\n",
      "  0%|          | 69/14893000 [04:34<16320:18:47,  3.95s/it]\u001b[A\n",
      "  0%|          | 70/14893000 [04:38<16260:10:26,  3.93s/it]\u001b[A\n",
      "  0%|          | 71/14893000 [04:42<16261:39:37,  3.93s/it]\u001b[A\n",
      "  0%|          | 72/14893000 [04:46<16248:40:59,  3.93s/it]\u001b[A\n",
      "  0%|          | 73/14893000 [04:49<16050:52:10,  3.88s/it]\u001b[A\n",
      "  0%|          | 74/14893000 [04:53<16118:36:34,  3.90s/it]\u001b[A\n",
      "  0%|          | 75/14893000 [04:57<16142:11:14,  3.90s/it]\u001b[A\n",
      "  0%|          | 76/14893000 [05:01<16155:43:15,  3.91s/it]\u001b[A\n",
      "  0%|          | 77/14893000 [05:05<16175:19:57,  3.91s/it]\u001b[A\n",
      "  0%|          | 78/14893000 [05:09<16185:12:54,  3.91s/it]\u001b[A\n",
      "  0%|          | 79/14893000 [05:13<16201:32:56,  3.92s/it]\u001b[A\n",
      "  0%|          | 80/14893000 [05:17<16259:06:52,  3.93s/it]\u001b[A\n",
      "  0%|          | 81/14893000 [05:21<16408:09:01,  3.97s/it]\u001b[A\n",
      "  0%|          | 82/14893000 [05:25<16358:11:09,  3.95s/it]\u001b[A\n",
      "  0%|          | 83/14893000 [05:29<16362:10:26,  3.96s/it]\u001b[A\n",
      "  0%|          | 84/14893000 [05:33<16406:14:53,  3.97s/it]\u001b[A\n",
      "  0%|          | 85/14893000 [05:37<16518:07:18,  3.99s/it]\u001b[A\n",
      "  0%|          | 86/14893000 [05:41<16601:07:11,  4.01s/it]\u001b[A\n",
      "  0%|          | 87/14893000 [05:45<16635:44:29,  4.02s/it]\u001b[A\n",
      "  0%|          | 88/14893000 [05:49<16353:23:14,  3.95s/it]\u001b[A\n",
      "  0%|          | 89/14893000 [05:53<16323:08:33,  3.95s/it]\u001b[A\n",
      "  0%|          | 90/14893000 [05:57<16300:45:25,  3.94s/it]\u001b[A\n",
      "  0%|          | 91/14893000 [06:01<16253:58:33,  3.93s/it]\u001b[A\n",
      "  0%|          | 92/14893000 [06:05<16262:08:58,  3.93s/it]\u001b[A\n",
      "  0%|          | 93/14893000 [06:08<16036:16:28,  3.88s/it]\u001b[A\n",
      "  0%|          | 94/14893000 [06:12<16092:53:27,  3.89s/it]\u001b[A\n",
      "  0%|          | 95/14893000 [06:16<16250:34:35,  3.93s/it]\u001b[A\n",
      "  0%|          | 96/14893000 [06:20<16232:05:13,  3.92s/it]\u001b[A\n",
      "  0%|          | 97/14893000 [06:24<16256:58:49,  3.93s/it]\u001b[A\n",
      "  0%|          | 98/14893000 [06:28<16277:51:03,  3.93s/it]\u001b[A\n",
      "  0%|          | 99/14893000 [06:32<16465:03:32,  3.98s/it]\u001b[A\n",
      "  0%|          | 100/14893000 [06:36<16441:35:19,  3.97s/it]\u001b[A\n",
      "  0%|          | 101/14893000 [06:40<16402:03:15,  3.96s/it]\u001b[A\n",
      "  0%|          | 102/14893000 [06:44<16406:35:27,  3.97s/it]\u001b[A\n",
      "  0%|          | 103/14893000 [06:48<16348:16:33,  3.95s/it]\u001b[A\n",
      "  0%|          | 104/14893000 [06:52<16315:05:33,  3.94s/it]\u001b[A\n",
      "  0%|          | 105/14893000 [06:56<16464:46:18,  3.98s/it]\u001b[A\n",
      "  0%|          | 106/14893000 [07:00<16398:49:07,  3.96s/it]\u001b[A\n",
      "  0%|          | 107/14893000 [07:04<16362:59:35,  3.96s/it]\u001b[A\n",
      "  0%|          | 108/14893000 [07:08<16316:37:53,  3.94s/it]\u001b[A\n",
      "  0%|          | 109/14893000 [07:11<16085:24:11,  3.89s/it]\u001b[A\n",
      "  0%|          | 110/14893000 [07:15<15913:54:12,  3.85s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "args = ()\n",
    "notebook_launcher(main, args, num_processes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6c6fdf-68ae-41e5-bf0d-da959e263ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb1a693-7227-49c8-af2c-8b99e1bf3bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcea2610-72c9-44b1-863f-4f6e2cb2f818",
   "metadata": {},
   "source": [
    "### captioning unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0fb4c2-58e5-445e-852f-007cfeebeba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    cfg = Config()\n",
    "    \n",
    "    accelerator = Accelerator(gradient_accumulation_steps=cfg.gradient_accumulation_steps)\n",
    "    make_dir(cfg.output_dir)\n",
    "    make_dir(cfg.generated_dir)\n",
    "    wandb_init(cfg)\n",
    "    \n",
    "    #compression_model, lm = build_model(cfg)\n",
    "    model = AudioProcessing(cfg)\n",
    "    \n",
    "    audio_dataset = AudioDataset(cfg, train=True) \n",
    "    eval_dataset = AudioDataset(cfg, train=False)\n",
    "    test_dataset = TestDataset(cfg)\n",
    "\n",
    "    audio_dataloader = DataLoader(audio_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=12)\n",
    "    eval_dataloader = DataLoader(eval_dataset, batch_size=cfg.eval_batch_size, shuffle=False, num_workers=4)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=1)\n",
    "\n",
    "    optimizer_parameters = [param for param in model.lm.parameters() if param.requires_grad]\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        optimizer_parameters, lr=cfg.learning_rate,\n",
    "        betas=(cfg.adam_beta1, cfg.adam_beta2),\n",
    "        weight_decay=cfg.adam_weight_decay,\n",
    "        eps=cfg.adam_epsilon,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    num_update_steps_per_epoch = math.ceil(len(audio_dataloader) / cfg.gradient_accumulation_steps)\n",
    "    if cfg.max_train_steps is None:\n",
    "      cfg.max_train_steps = cfg.num_train_epochs * num_update_steps_per_epoch\n",
    "    \n",
    "    lr_scheduler = get_scheduler(\n",
    "          name=cfg.lr_scheduler_type,\n",
    "          optimizer=optimizer,\n",
    "          num_warmup_steps=cfg.num_warmup_steps * cfg.gradient_accumulation_steps,\n",
    "          num_training_steps=cfg.max_train_steps * cfg.gradient_accumulation_steps,\n",
    "      )\n",
    "\n",
    "\n",
    "    audio_dataloader, eval_dataloader, model, optimizer, lr_scheduler = accelerator.prepare(\n",
    "        audio_dataloader, eval_dataloader, model, optimizer, lr_scheduler\n",
    "    )\n",
    "\n",
    "    starting_epoch, completed_steps, best_loss = 0, 0, np.inf\n",
    "    progress_bar = tqdm(range(cfg.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "    \n",
    "    for epoch in range(starting_epoch, cfg.num_train_epochs):\n",
    "        print(f\"-------------------EPOCH{epoch}-------------------------\" )\n",
    "        total_loss, total_val_loss = 0, 0\n",
    "        model.train()\n",
    "        for batch_idx, (wav, descriptions, lengths) in enumerate(audio_dataloader):\n",
    "          with accelerator.accumulate(model):\n",
    "              loss = model(wav, descriptions, lengths)\n",
    "              ppl =  torch.exp(loss)\n",
    "              total_loss += loss.detach().float()\n",
    "              accelerator.backward(loss)     \n",
    "              optimizer.step()\n",
    "              lr_scheduler.step()\n",
    "              optimizer.zero_grad()\n",
    "              \n",
    "          if accelerator.sync_gradients:\n",
    "              progress_bar.update(1)\n",
    "              completed_steps += 1\n",
    "            \n",
    "        model.eval()\n",
    "        for batch_idx, (wav, descriptions, lengths) in enumerate(eval_dataloader):\n",
    "              loss = model(wav, descriptions, lengths)\n",
    "              total_val_loss += loss  \n",
    "    \n",
    "        if accelerator.is_main_process:         \n",
    "            result = {}\n",
    "            result[\"epoch\"] = epoch + 1,\n",
    "            result[\"step\"] = completed_steps\n",
    "            result[\"train_loss\"] = round(total_loss.item()/len(audio_dataloader), 4)\n",
    "            result[\"valid_loss\"] = round(total_val_loss.item()/len(eval_dataloader), 4)\n",
    "            \n",
    "            wandb.log(result)\n",
    "            result_string = \"Epoch: {}, Loss Train: {}, Valid: {}\\n\".format(epoch + 1, result[\"train_loss\"], result[\"valid_loss\"])    \n",
    "            accelerator.print(result_string) \n",
    "            best_loss = save_checkpoint(cfg, model, result, best_loss, epoch)\n",
    "            \n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            for test_step, batch in enumerate(test_dataloader):\n",
    "                gen_audio = unwrapped_model.inference(batch)\n",
    "                audio_filename = f\"epoch_{epoch}_{test_step}.wav\"\n",
    "                unwrapped_model.save_audio(gen_audio, audio_filename, cfg)\n",
    "             \n",
    "    #wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b5a51e4-87db-475e-be27-5550ac28e8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:wfdvdk2e) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.014 MB of 0.014 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>▁█</td></tr><tr><td>train_loss</td><td>█▁</td></tr><tr><td>valid_loss</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>92</td></tr><tr><td>train_loss</td><td>2.151</td></tr><tr><td>valid_loss</td><td>2.4137</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">driven-dragon-52</strong> at: <a href='https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1/runs/wfdvdk2e' target=\"_blank\">https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1/runs/wfdvdk2e</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231214_081454-wfdvdk2e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:wfdvdk2e). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c8a20207c844f481968d9b27df390a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112272594538, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20231214_081933-pkdbo0ih</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1/runs/pkdbo0ih' target=\"_blank\">rural-firebrand-53</a></strong> to <a href='https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1' target=\"_blank\">https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1/runs/pkdbo0ih' target=\"_blank\">https://wandb.ai/optimizer_ai/audiogen-finetune-init-test1/runs/pkdbo0ih</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "\n",
      "  0%|          | 0/46000 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------EPOCH0-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 1/46000 [00:03<40:52:21,  3.20s/it]\u001b[A\n",
      "  0%|          | 2/46000 [00:03<20:06:14,  1.57s/it]\u001b[A\n",
      "  0%|          | 3/46000 [00:04<13:44:09,  1.08s/it]\u001b[A\n",
      "  0%|          | 4/46000 [00:04<10:43:04,  1.19it/s]\u001b[A\n",
      "  0%|          | 5/46000 [00:05<9:00:29,  1.42it/s] \u001b[A\n",
      "  0%|          | 6/46000 [00:05<8:01:04,  1.59it/s]\u001b[A\n",
      "  0%|          | 7/46000 [00:06<7:22:41,  1.73it/s]\u001b[A\n",
      "  0%|          | 8/46000 [00:06<6:57:42,  1.84it/s]\u001b[A\n",
      "  0%|          | 9/46000 [00:06<6:41:39,  1.91it/s]\u001b[A\n",
      "  0%|          | 10/46000 [00:07<6:31:49,  1.96it/s]\u001b[A\n",
      "  0%|          | 11/46000 [00:07<6:23:56,  2.00it/s]\u001b[A\n",
      "  0%|          | 12/46000 [00:08<6:19:03,  2.02it/s]\u001b[A\n",
      "  0%|          | 13/46000 [00:08<6:16:05,  2.04it/s]\u001b[A\n",
      "  0%|          | 14/46000 [00:09<6:13:20,  2.05it/s]\u001b[A\n",
      "  0%|          | 15/46000 [00:09<6:12:47,  2.06it/s]\u001b[A\n",
      "  0%|          | 16/46000 [00:10<6:09:37,  2.07it/s]\u001b[A\n",
      "  0%|          | 17/46000 [00:10<6:10:40,  2.07it/s]\u001b[A\n",
      "  0%|          | 18/46000 [00:11<6:09:34,  2.07it/s]\u001b[A\n",
      "  0%|          | 19/46000 [00:11<6:08:36,  2.08it/s]\u001b[A\n",
      "  0%|          | 20/46000 [00:12<6:07:15,  2.09it/s]\u001b[A\n",
      "  0%|          | 21/46000 [00:12<6:07:10,  2.09it/s]\u001b[A\n",
      "  0%|          | 22/46000 [00:13<6:05:50,  2.09it/s]\u001b[A\n",
      "  0%|          | 23/46000 [00:13<6:06:08,  2.09it/s]\u001b[A\n",
      "  0%|          | 24/46000 [00:14<6:07:11,  2.09it/s]\u001b[A\n",
      "  0%|          | 25/46000 [00:14<6:06:24,  2.09it/s]\u001b[A\n",
      "  0%|          | 26/46000 [00:15<6:06:42,  2.09it/s]\u001b[A\n",
      "  0%|          | 27/46000 [00:15<6:05:53,  2.09it/s]\u001b[A\n",
      "  0%|          | 28/46000 [00:16<6:06:08,  2.09it/s]\u001b[A\n",
      "  0%|          | 29/46000 [00:16<6:07:14,  2.09it/s]\u001b[A\n",
      "  0%|          | 30/46000 [00:17<6:07:23,  2.09it/s]\u001b[A\n",
      "  0%|          | 31/46000 [00:17<6:06:47,  2.09it/s]\u001b[A\n",
      "  0%|          | 32/46000 [00:17<6:07:00,  2.09it/s]\u001b[A\n",
      "  0%|          | 33/46000 [00:18<6:06:08,  2.09it/s]\u001b[A\n",
      "  0%|          | 34/46000 [00:18<6:06:19,  2.09it/s]\u001b[A\n",
      "  0%|          | 35/46000 [00:19<6:05:32,  2.10it/s]\u001b[A\n",
      "  0%|          | 36/46000 [00:19<6:06:46,  2.09it/s]\u001b[A\n",
      "  0%|          | 37/46000 [00:20<6:06:11,  2.09it/s]\u001b[A\n",
      "  0%|          | 38/46000 [00:20<6:07:21,  2.09it/s]\u001b[A\n",
      "  0%|          | 39/46000 [00:21<6:06:47,  2.09it/s]\u001b[A\n",
      "  0%|          | 40/46000 [00:21<6:04:57,  2.10it/s]\u001b[A\n",
      "  0%|          | 41/46000 [00:22<6:05:16,  2.10it/s]\u001b[A\n",
      "  0%|          | 42/46000 [00:22<6:05:21,  2.10it/s]\u001b[A\n",
      "  0%|          | 43/46000 [00:23<6:05:21,  2.10it/s]\u001b[A\n",
      "  0%|          | 44/46000 [00:23<6:04:09,  2.10it/s]\u001b[A\n",
      "  0%|          | 45/46000 [00:24<6:06:02,  2.09it/s]\u001b[A\n",
      "  0%|          | 46/46000 [00:24<6:10:26,  2.07it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss Train: 2.457, Valid: 2.4067\n",
      "\n",
      "-------------------EPOCH1-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 47/46000 [01:00<140:45:57, 11.03s/it]\u001b[A\n",
      "  0%|          | 48/46000 [01:00<100:20:00,  7.86s/it]\u001b[A\n",
      "  0%|          | 49/46000 [01:01<72:03:39,  5.65s/it] \u001b[A\n",
      "  0%|          | 50/46000 [01:01<52:17:27,  4.10s/it]\u001b[A\n",
      "  0%|          | 51/46000 [01:02<38:27:35,  3.01s/it]\u001b[A\n",
      "  0%|          | 52/46000 [01:02<28:46:16,  2.25s/it]\u001b[A\n",
      "  0%|          | 53/46000 [01:03<21:59:09,  1.72s/it]\u001b[A\n",
      "  0%|          | 54/46000 [01:03<17:13:31,  1.35s/it]\u001b[A\n",
      "  0%|          | 55/46000 [01:04<13:53:30,  1.09s/it]\u001b[A\n",
      "  0%|          | 56/46000 [01:04<11:34:12,  1.10it/s]\u001b[A\n",
      "  0%|          | 57/46000 [01:05<9:56:37,  1.28it/s] \u001b[A\n",
      "  0%|          | 58/46000 [01:05<8:49:51,  1.45it/s]\u001b[A\n",
      "  0%|          | 59/46000 [01:06<8:01:54,  1.59it/s]\u001b[A\n",
      "  0%|          | 60/46000 [01:06<7:27:37,  1.71it/s]\u001b[A\n",
      "  0%|          | 61/46000 [01:07<7:03:48,  1.81it/s]\u001b[A\n",
      "  0%|          | 62/46000 [01:07<6:47:33,  1.88it/s]\u001b[A\n",
      "  0%|          | 63/46000 [01:08<6:35:16,  1.94it/s]\u001b[A\n",
      "  0%|          | 64/46000 [01:08<6:28:24,  1.97it/s]\u001b[A\n",
      "  0%|          | 65/46000 [01:08<6:22:05,  2.00it/s]\u001b[A\n",
      "  0%|          | 66/46000 [01:09<6:17:55,  2.03it/s]\u001b[A\n",
      "  0%|          | 67/46000 [01:09<6:13:49,  2.05it/s]\u001b[A\n",
      "  0%|          | 68/46000 [01:10<6:14:20,  2.04it/s]\u001b[A\n",
      "  0%|          | 69/46000 [01:10<6:11:11,  2.06it/s]\u001b[A\n",
      "  0%|          | 70/46000 [01:11<6:10:57,  2.06it/s]\u001b[A\n",
      "  0%|          | 71/46000 [01:11<6:09:44,  2.07it/s]\u001b[A\n",
      "  0%|          | 72/46000 [01:12<6:10:03,  2.07it/s]\u001b[A\n",
      "  0%|          | 73/46000 [01:12<6:08:00,  2.08it/s]\u001b[A\n",
      "  0%|          | 74/46000 [01:13<6:06:44,  2.09it/s]\u001b[A\n",
      "  0%|          | 75/46000 [01:13<6:07:42,  2.08it/s]\u001b[A\n",
      "  0%|          | 76/46000 [01:14<6:08:14,  2.08it/s]\u001b[A\n",
      "  0%|          | 77/46000 [01:14<6:08:25,  2.08it/s]\u001b[A\n",
      "  0%|          | 78/46000 [01:15<6:07:38,  2.08it/s]\u001b[A\n",
      "  0%|          | 79/46000 [01:15<6:08:26,  2.08it/s]\u001b[A\n",
      "  0%|          | 80/46000 [01:16<6:08:47,  2.08it/s]\u001b[A\n",
      "  0%|          | 81/46000 [01:16<6:08:38,  2.08it/s]\u001b[A\n",
      "  0%|          | 82/46000 [01:17<6:08:01,  2.08it/s]\u001b[A\n",
      "  0%|          | 83/46000 [01:17<6:06:59,  2.09it/s]\u001b[A\n",
      "  0%|          | 84/46000 [01:18<6:07:29,  2.08it/s]\u001b[A\n",
      "  0%|          | 85/46000 [01:18<6:08:59,  2.07it/s]\u001b[A\n",
      "  0%|          | 86/46000 [01:19<6:08:12,  2.08it/s]\u001b[A\n",
      "  0%|          | 87/46000 [01:19<6:08:41,  2.08it/s]\u001b[A\n",
      "  0%|          | 88/46000 [01:20<6:06:58,  2.09it/s]\u001b[A\n",
      "  0%|          | 89/46000 [01:20<6:07:56,  2.08it/s]\u001b[A\n",
      "  0%|          | 90/46000 [01:21<6:08:33,  2.08it/s]\u001b[A\n",
      "  0%|          | 91/46000 [01:21<6:09:23,  2.07it/s]\u001b[A\n",
      "  0%|          | 92/46000 [01:21<6:00:00,  2.13it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss Train: 2.151, Valid: 2.4137\n",
      "\n",
      "-------------------EPOCH2-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 93/46000 [01:57<139:14:47, 10.92s/it]\u001b[A\n",
      "  0%|          | 94/46000 [01:57<99:18:36,  7.79s/it] \u001b[A\n",
      "  0%|          | 95/46000 [01:58<71:21:33,  5.60s/it]\u001b[A\n",
      "  0%|          | 96/46000 [01:58<51:45:49,  4.06s/it]\u001b[A\n",
      "  0%|          | 97/46000 [01:59<38:04:44,  2.99s/it]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#main()\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m notebook_launcher\n\u001b[0;32m----> 3\u001b[0m \u001b[43mnotebook_launcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/launchers.py:207\u001b[0m, in \u001b[0;36mnotebook_launcher\u001b[0;34m(function, args, num_processes, mixed_precision, use_port, master_addr, node_rank, num_nodes)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLaunching training on CPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 207\u001b[0m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 112\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (wav, descriptions, lengths) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(audio_dataloader):\n\u001b[1;32m    111\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m accelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m--> 112\u001b[0m       loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescriptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m       ppl \u001b[38;5;241m=\u001b[39m  torch\u001b[38;5;241m.\u001b[39mexp(loss)\n\u001b[1;32m    114\u001b[0m       total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/audiomodel.py:32\u001b[0m, in \u001b[0;36mAudioProcessing.forward\u001b[0;34m(self, wav, descriptions, lengths)\u001b[0m\n\u001b[1;32m     26\u001b[0m audio_tokens, padding_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_process_audio_tokenizer(audio_tokens, audio_lengths\u001b[38;5;241m=\u001b[39mlengths)\n\u001b[1;32m     28\u001b[0m attributes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     29\u001b[0m     ConditioningAttributes(text\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m: description})\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m description \u001b[38;5;129;01min\u001b[39;00m descriptions]\n\u001b[0;32m---> 32\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondition_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     33\u001b[0m logits \u001b[38;5;241m=\u001b[39m model_output\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     35\u001b[0m mask \u001b[38;5;241m=\u001b[39m padding_mask \u001b[38;5;241m&\u001b[39m model_output\u001b[38;5;241m.\u001b[39mmask\n",
      "File \u001b[0;32m/workspace/audiocraft/models/lm.py:297\u001b[0m, in \u001b[0;36mLMModel.compute_predictions\u001b[0;34m(self, codes, conditions, condition_tensors)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# apply model on pattern sequence\u001b[39;00m\n\u001b[1;32m    296\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fsdp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fsdp\n\u001b[0;32m--> 297\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_codes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondition_tensors\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, K, S, card]\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# map back the logits on pattern sequence to logits on original codes: [B, K, S, card] -> [B, K, T, card]\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# and provide the corresponding mask over invalid positions of tokens\u001b[39;00m\n\u001b[1;32m    300\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# [B, card, K, S]\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/audiocraft/models/lm.py:253\u001b[0m, in \u001b[0;36mLMModel.forward\u001b[0;34m(self, sequence, conditions, condition_tensors)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conditions, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt pass both conditions and condition_tensors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    251\u001b[0m input_, cross_attention_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuser(input_, condition_tensors)\n\u001b[0;32m--> 253\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcross_attention_src\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_norm:\n\u001b[1;32m    255\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_norm(out)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/audiocraft/modules/transformer.py:698\u001b[0m, in \u001b[0;36mStreamingTransformer.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    695\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_scale \u001b[38;5;241m*\u001b[39m pos_emb\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 698\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_streaming:\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming_state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moffsets\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m offsets \u001b[38;5;241m+\u001b[39m T\n",
      "File \u001b[0;32m/workspace/audiocraft/modules/transformer.py:655\u001b[0m, in \u001b[0;36mStreamingTransformer._apply_layer\u001b[0;34m(self, layer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointing\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_checkpoint(layer, \u001b[38;5;241m*\u001b[39margs, use_reentrant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/audiocraft/modules/transformer.py:550\u001b[0m, in \u001b[0;36mStreamingTransformerLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, cross_attention_src)\u001b[0m\n\u001b[1;32m    547\u001b[0m x \u001b[38;5;241m=\u001b[39m src\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_first:\n\u001b[1;32m    549\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_scale_1(\n\u001b[0;32m--> 550\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cross_attention_src \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    552\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_scale_cross(\n\u001b[1;32m    553\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cross_attention_block(\n\u001b[1;32m    554\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_cross(x), cross_attention_src))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:715\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[1;32m    714\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 715\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/audiocraft/modules/transformer.py:372\u001b[0m, in \u001b[0;36mStreamingMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m    370\u001b[0m         bound_layout \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb t p h d\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m     packed \u001b[38;5;241m=\u001b[39m rearrange(projected, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb t (p h d) -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbound_layout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, h\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads)\n\u001b[0;32m--> 372\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    374\u001b[0m     embed_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/xformers/ops/unbind.py:117\u001b[0m, in \u001b[0;36munbind\u001b[0;34m(x, dim)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munbind\u001b[39m(x: torch\u001b[38;5;241m.\u001b[39mTensor, dim: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[1;32m    112\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    Does exactly the same as :attr:`torch.unbind` for the forward.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    In backward, avoids a :attr:`torch.cat` if the gradients\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m    are already multiple views of the same storage\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_Unbind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py:536\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_are_functorch_transforms_active\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    537\u001b[0m         \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m         args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m    539\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#main()\n",
    "from accelerate import notebook_launcher\n",
    "notebook_launcher(main, num_processes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f65cfa-85aa-4bc7-b7a7-770a50dd7df7",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "325e8d99-5bcc-4503-b82c-00fb9c1f1204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "import tqdm\n",
    "import json\n",
    "import typing as tp\n",
    "import pandas as pd\n",
    "import glob2\n",
    "import math\n",
    "import omegaconf\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.sample_rate = 16000\n",
    "        self.is_training = True\n",
    "        self.duration = 1\n",
    "        self.total_updates = 10000\n",
    "        self.eval_steps = 4\n",
    "        self.device = 'cuda'  # 'cuda' 또는 'cpu'\n",
    "        self.batch_size = 24\n",
    "        self.eval_batch_size = 4\n",
    "        self.train_data_path = \"/workspace/train_dataset.csv\"\n",
    "        self.eval_data_path = \"/workspace/eval_dataset.csv\"\n",
    "        self.output_dir = \"./output_dir\"\n",
    "        self.checkpointing_steps = \"best\"\n",
    "        self.save_every = 10\n",
    "        self.with_tracking = False\n",
    "        self.text_encoder_name = None  # 나중에 설정\n",
    "        self.snr_gamma = 5.0\n",
    "        self.freeze_text_encoder = True\n",
    "        self.uncondition = False\n",
    "        self.learning_rate = 3e-5\n",
    "        self.adam_beta1 = 0.9\n",
    "        self.adam_beta2 = 0.999\n",
    "        self.adam_weight_decay = 1e-2\n",
    "        self.adam_epsilon = 1e-08\n",
    "        self.gradient_accumulation_steps = 1\n",
    "        self.num_train_epochs = 1000\n",
    "        self.num_warmup_steps = 0\n",
    "        self.max_train_steps = None\n",
    "        self.lr_scheduler_type = \"linear\"\n",
    "        self.resume_from_checkpoint = None #\"/workspace/output_dir_batch48/last/\" #None\n",
    "        self.wandb_project_name = \"audiogen-finetune-init-test1\"\n",
    "        self.wandb_id = None #\"earnest-pond-52\"\n",
    "        self.resume_epoch = 0 #127\n",
    "        self.dtype = \"float32\"\n",
    "        \n",
    "        self.update_audiocraft_config()\n",
    "        \n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            # 기존 속성에 값 할당하거나 새 속성 생성\n",
    "            if not hasattr(self, key):\n",
    "                print(key)\n",
    "            setattr(self, key, value)\n",
    "            \n",
    "    def update_audiocraft_config(self):\n",
    "        self.solver = None\n",
    "        self.fsdp = None\n",
    "        self.profiler = None\n",
    "        self.deadlock = None\n",
    "        self.dataset = None\n",
    "        self.checkpoint = None\n",
    "        self.generate = None\n",
    "        self.evaluate = None\n",
    "        self.optim = None\n",
    "        self.schedule = None\n",
    "        self.default = None\n",
    "        self.defaults = None\n",
    "        self.autocast = None\n",
    "        self.autocast_dtype = None\n",
    "\n",
    "        self.compression_model_checkpoint = None\n",
    "        self.channels = None\n",
    "        self.logging = None\n",
    "        self.lm_model = None\n",
    "        self.codebooks_pattern = None\n",
    "        self.transformer_lm = None\n",
    "        self.classifier_free_guidance = None\n",
    "        self.attribute_dropout = None\n",
    "        self.fuser = None\n",
    "        self.conditioners = None\n",
    "        self.datasource = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "463fbdc2-a1ab-405d-b417-a9dfe15418fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_paths, device, target_sample_rate=44100, duration=3):\n",
    "        import pandas as pd\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_files_list (list): List of paths to audio files.\n",
    "            target_sample_rate (int): The sample rate to which audio should be resampled.\n",
    "            frame_length (int): The frame length for slicing or padding audio.\n",
    "        \"\"\"\n",
    "        self.audio_paths = audio_paths\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.duration = duration\n",
    "        self.device = device\n",
    "\n",
    "        self.df = pd.read_csv(self.audio_paths)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        data = self.df.iloc[idx] #self.audio_files_list[idx]\n",
    "        audio_path = data['sliced_audio_path']\n",
    "        description = data['description']\n",
    "\n",
    "        # Load audio signal file\n",
    "        from audiotools import AudioSignal\n",
    "        wav = AudioSignal(audio_path)\n",
    "        length = wav.signal_length\n",
    "\n",
    "        # Encode audio signal as one long file\n",
    "        wav.to_mono()\n",
    "        wav.resample(self.target_sample_rate)\n",
    "\n",
    "        if wav.duration < self.duration:\n",
    "          pad_len = int(self.duration * self.target_sample_rate) - wav.signal_length\n",
    "          wav.zero_pad(0, pad_len)\n",
    "        elif wav.duration > self.duration:\n",
    "          wav.truncate_samples(self.duration * self.target_sample_rate)\n",
    "\n",
    "\n",
    "        return wav.audio_data.squeeze(1), description, length\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, prompts):\n",
    "\n",
    "        self.prompts = prompts\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.prompts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c36c688-9e32-408e-b60f-01811aa9fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.solvers import base, builders\n",
    "from audiocraft.solvers.compression import CompressionSolver\n",
    "from audiocraft import metrics as eval_metrics\n",
    "from audiocraft import models\n",
    "from audiocraft.data.audio_utils import normalize_audio\n",
    "from audiocraft.modules.conditioners import JointEmbedCondition, SegmentWithAttributes, WavCondition, ConditioningAttributes\n",
    "from audiocraft.utils.utils import get_dataset_from_loader, is_jsonable, warn_once\n",
    "from audiocraft.models.loaders import load_compression_model, load_lm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2516970a-a1ee-4324-81cc-17167ede6e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioProcessing(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()  # 부모 클래스 초기화 호출\n",
    "        self.cfg = cfg\n",
    "        self.compression_model, self.lm  = self.build_model(self.cfg)\n",
    "        self.to_float32()\n",
    "        self.freeze_layers()\n",
    "\n",
    "    def forward(self, wav, descriptions, lengths):\n",
    "        from audiocraft.modules.conditioners import JointEmbedCondition, SegmentWithAttributes, WavCondition, ConditioningAttributes\n",
    "        audio_tokens = self.process_audio_tokenizer(wav.to(self.cfg.device))\n",
    "        audio_tokens, padding_mask = self.post_process_audio_tokenizer(audio_tokens, audio_lengths=lengths)\n",
    "        \n",
    "        attributes = [\n",
    "            ConditioningAttributes(text={'description': description})\n",
    "            for description in descriptions]\n",
    "    \n",
    "        model_output = self.lm.compute_predictions(audio_tokens, conditions=attributes, condition_tensors=None)  # type: ignore\n",
    "        logits = model_output.logits\n",
    "    \n",
    "        mask = padding_mask & model_output.mask\n",
    "        ce, ce_per_codebook = self.compute_cross_entropy(logits, audio_tokens, mask)\n",
    "        \n",
    "        return ce\n",
    "\n",
    "    def build_model(self, cfg):\n",
    "        from audiocraft.models.loaders import load_compression_model, load_lm_model\n",
    "        \"\"\"Instantiate models and optimizer.\"\"\"\n",
    "        \n",
    "        compression_model = load_compression_model('facebook/audiogen-medium', device=cfg.device)\n",
    "        lm = load_lm_model('facebook/audiogen-medium', device=cfg.device)\n",
    "    \n",
    "        return compression_model, lm\n",
    "\n",
    "    def process_audio_tokenizer(self, wav):\n",
    "        with torch.no_grad():\n",
    "            audio_tokens, scale = self.compression_model.encode(wav)\n",
    "        return audio_tokens\n",
    "\n",
    "    def post_process_audio_tokenizer(self, audio_tokens, audio_lengths=None):\n",
    "        padding_mask = torch.ones_like(audio_tokens, dtype=torch.bool, device=audio_tokens.device)\n",
    "\n",
    "        audio_tokens = audio_tokens.clone()\n",
    "        padding_mask = padding_mask.clone()\n",
    "        token_sample_rate = self.compression_model.frame_rate\n",
    "        B, K, T_s = audio_tokens.shape\n",
    "        for i in range(B):\n",
    "            valid_tokens = math.floor(audio_lengths[i] / self.cfg.sample_rate * token_sample_rate)\n",
    "            audio_tokens[i, :, valid_tokens:] = self.lm.special_token_id\n",
    "            padding_mask[i, :, valid_tokens:] = 0\n",
    "\n",
    "        return audio_tokens, padding_mask\n",
    "\n",
    "    def compute_cross_entropy(self, logits: torch.Tensor, targets: torch.Tensor, mask: torch.Tensor) -> tp.Tuple[torch.Tensor, tp.List[torch.Tensor]]:\n",
    "\n",
    "        B, K, T = targets.shape\n",
    "        assert logits.shape[:-1] == targets.shape\n",
    "        assert mask.shape == targets.shape\n",
    "        ce = torch.zeros([], device=targets.device)\n",
    "        ce_per_codebook: tp.List[torch.Tensor] = []\n",
    "        for k in range(K):\n",
    "            logits_k = logits[:, k, ...].contiguous().view(-1, logits.size(-1))  # [B x T, card]\n",
    "            targets_k = targets[:, k, ...].contiguous().view(-1)  # [B x T]\n",
    "            mask_k = mask[:, k, ...].contiguous().view(-1)  # [B x T]\n",
    "            ce_targets = targets_k[mask_k]\n",
    "            ce_logits = logits_k[mask_k]\n",
    "            q_ce = F.cross_entropy(ce_logits, ce_targets)\n",
    "            ce += q_ce\n",
    "            ce_per_codebook.append(q_ce.detach())\n",
    "        # average cross entropy across codebooks\n",
    "        ce = ce / K\n",
    "        return ce, ce_per_codebook\n",
    "\n",
    "    def audio_generate(self, condition_tensors, gen_duration=5):\n",
    "        with torch.no_grad():\n",
    "            total_gen_len = math.ceil(gen_duration * self.compression_model.frame_rate)\n",
    "            gen_tokens = self.lm.generate(\n",
    "                None, condition_tensors, max_gen_len=total_gen_len,\n",
    "                num_samples=1)\n",
    "            gen_audio = self.compression_model.decode(gen_tokens, None)\n",
    "\n",
    "        return gen_tokens, gen_audio\n",
    "\n",
    "    def inference(self, descriptions):\n",
    "        #with torch.no_grad():\n",
    "        from audiocraft.modules.conditioners import JointEmbedCondition, SegmentWithAttributes, WavCondition, ConditioningAttributes\n",
    "        attributes = [\n",
    "        ConditioningAttributes(text={'description': description})\n",
    "        for description in descriptions]\n",
    "        _, gen_audio = self.audio_generate(attributes, gen_duration=self.cfg.duration)\n",
    "        \n",
    "        return gen_audio\n",
    "\n",
    "    def to_float32(self):\n",
    "        # 모든 가중치를 FP32로 변환\n",
    "        for param in self.lm.parameters():\n",
    "            param.data = param.data.to(dtype=torch.float32)\n",
    "\n",
    "    def freeze_layers(self, train_layers=12):\n",
    "        for param in self.lm.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        if train_layers > 0 :\n",
    "            num_layers = len(self.lm.transformer.layers)\n",
    "            \n",
    "            for i in range(num_layers - train_layers, num_layers):\n",
    "                for param in self.lm.transformer.layers[i].parameters():\n",
    "                    param.requires_grad = True\n",
    "                    \n",
    "            for name, param in self.lm.named_parameters():\n",
    "                if 'out_norm' in name or 'linears' in name:\n",
    "                    param.requires_grad = True\n",
    "                \n",
    "            \n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "102033ad-06dd-418f-8a63-5dcb73587ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract discrete codes from EnCodec\n",
    "def process_audio_tokenizer(wav):\n",
    "  with torch.no_grad():\n",
    "      audio_tokens, scale = compression_model.encode(wav)\n",
    "  return audio_tokens\n",
    "\n",
    "def post_process_audio_tokenizer(audio_tokens, audio_lengths=None, cfg=None):\n",
    "  padding_mask = torch.ones_like(audio_tokens, dtype=torch.bool, device=audio_tokens.device)\n",
    "  # replace encodec tokens from padded audio with special_token_id\n",
    "\n",
    "  audio_tokens = audio_tokens.clone()\n",
    "  padding_mask = padding_mask.clone()\n",
    "  token_sample_rate = compression_model.frame_rate\n",
    "  B, K, T_s = audio_tokens.shape\n",
    "  for i in range(B):\n",
    "      # take the last token generated from actual audio frames (non-padded audio)\n",
    "      #math.floor(float(n_frames[i]) / sr[i] * token_sample_rate)\n",
    "      valid_tokens = math.floor(audio_lengths[i] / cfg.sample_rate * token_sample_rate)\n",
    "      audio_tokens[i, :, valid_tokens:] = lm.special_token_id\n",
    "      padding_mask[i, :, valid_tokens:] = 0\n",
    "\n",
    "  return audio_tokens, padding_mask\n",
    "\n",
    "def _compute_cross_entropy(\n",
    "      logits: torch.Tensor, targets: torch.Tensor, mask: torch.Tensor\n",
    "    ) -> tp.Tuple[torch.Tensor, tp.List[torch.Tensor]]:\n",
    "        \"\"\"Compute cross entropy between multi-codebook targets and model's logits.\n",
    "        The cross entropy is computed per codebook to provide codebook-level cross entropy.\n",
    "        Valid timesteps for each of the codebook are pulled from the mask, where invalid\n",
    "        timesteps are set to 0.\n",
    "\n",
    "        Args:\n",
    "            logits (torch.Tensor): Model's logits of shape [B, K, T, card].\n",
    "            targets (torch.Tensor): Target codes, of shape [B, K, T].\n",
    "            mask (torch.Tensor): Mask for valid target codes, of shape [B, K, T].\n",
    "        Returns:\n",
    "            ce (torch.Tensor): Cross entropy averaged over the codebooks\n",
    "            ce_per_codebook (list of torch.Tensor): Cross entropy per codebook (detached).\n",
    "        \"\"\"\n",
    "        B, K, T = targets.shape\n",
    "        assert logits.shape[:-1] == targets.shape\n",
    "        assert mask.shape == targets.shape\n",
    "        ce = torch.zeros([], device=targets.device)\n",
    "        ce_per_codebook: tp.List[torch.Tensor] = []\n",
    "        for k in range(K):\n",
    "            logits_k = logits[:, k, ...].contiguous().view(-1, logits.size(-1))  # [B x T, card]\n",
    "            targets_k = targets[:, k, ...].contiguous().view(-1)  # [B x T]\n",
    "            mask_k = mask[:, k, ...].contiguous().view(-1)  # [B x T]\n",
    "            ce_targets = targets_k[mask_k]\n",
    "            ce_logits = logits_k[mask_k]\n",
    "            q_ce = F.cross_entropy(ce_logits, ce_targets)\n",
    "            ce += q_ce\n",
    "            ce_per_codebook.append(q_ce.detach())\n",
    "        # average cross entropy across codebooks\n",
    "        ce = ce / K\n",
    "        return ce, ce_per_codebook\n",
    "\n",
    "def audio_generate(condition_tensors, gen_duration=5):\n",
    "    with torch.no_grad():\n",
    "      total_gen_len = math.ceil(gen_duration * compression_model.frame_rate)\n",
    "      gen_tokens = lm.generate(\n",
    "          None, condition_tensors, max_gen_len=total_gen_len,\n",
    "          num_samples=1)\n",
    "      gen_audio = compression_model.decode(gen_tokens, None)\n",
    "\n",
    "    return gen_tokens, gen_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b264f61-dfaa-40a1-9510-328b9f9049c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav, text, length = next(iter(audio_dataloader))\n",
    "audio_tokens = process_audio_tokenizer(wav.to(cfg.device))\n",
    "print(\"Wav shape: \", wav.shape)\n",
    "print(\"Token shape: \", audio_tokens.shape)\n",
    "post_process_audio_tokenizer(audio_tokens, audio_lengths=length)\n",
    "\n",
    "import torch\n",
    "\n",
    "# 가정: model이라는 이름의 PyTorch 모델이 이미 정의되어 있음\n",
    "for name, param in lm.named_parameters():\n",
    "    print(f\"Layer {name} has data type {param.dtype}\")\n",
    "    #break  # 모든 레이어를 표시하지 않고 첫 레이어에서 루프 중단\n",
    "\n",
    "def check_requires_grad(model: torch.nn.Module):\n",
    "    for name, module in model.named_children():\n",
    "        for param_name, param in module.named_parameters():\n",
    "            print(f\"{name}.{param_name}: requires_grad = {param.requires_grad}\")\n",
    "\n",
    "# DAC 모델의 인스턴스를 생성한 후에 아래와 같이 사용할 수 있습니다:\n",
    "# dac_instance = DAC(...)\n",
    "check_requires_grad(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5283af14-dd57-406b-8834-58fb21dcc208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2b4a7e-1ca5-4749-84a3-c5543e669d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
